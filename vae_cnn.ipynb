{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_practical.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wpBsQZEsHyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-XezsG4HuHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# All  Necessary Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S2BT78K_HjFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sf90laAWIE-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "h_dim = 400\n",
        "z_dim = 20\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQ_qqM9wIIbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                     train=True,\n",
        "                                     transform=transforms.ToTensor(),\n",
        "                                     download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGmUXV4dINSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lwo_CT_FISKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, image_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return F.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8ROV0XlIXEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QCCWJ3prIbw0",
        "colab_type": "code",
        "outputId": "8d76c7a0-89c9-4c83-9f1d-ac82afc0cbe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33607
        }
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "t = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        x = x.to(device).view(-1, image_size)\n",
        "        x_reconst, mu, log_var = model(x)\n",
        "        \n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconstruction Loss: {:.4f}, KL Divergence: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
        "\n",
        "elapsed = time.time() - t\n",
        "print(\"Training Takes {} seconds\".format(elapsed))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[1/20], Step [10/938], Reconstruction Loss: 18407.2812, KL Divergence: 1500.0900\n",
            "Epoch[1/20], Step [20/938], Reconstruction Loss: 15081.2598, KL Divergence: 607.1226\n",
            "Epoch[1/20], Step [30/938], Reconstruction Loss: 13397.5508, KL Divergence: 693.4001\n",
            "Epoch[1/20], Step [40/938], Reconstruction Loss: 12872.6602, KL Divergence: 303.7212\n",
            "Epoch[1/20], Step [50/938], Reconstruction Loss: 14244.0000, KL Divergence: 340.8522\n",
            "Epoch[1/20], Step [60/938], Reconstruction Loss: 12740.5645, KL Divergence: 454.7859\n",
            "Epoch[1/20], Step [70/938], Reconstruction Loss: 12440.0254, KL Divergence: 389.9010\n",
            "Epoch[1/20], Step [80/938], Reconstruction Loss: 12106.4453, KL Divergence: 614.0410\n",
            "Epoch[1/20], Step [90/938], Reconstruction Loss: 11423.2930, KL Divergence: 584.7109\n",
            "Epoch[1/20], Step [100/938], Reconstruction Loss: 12452.1943, KL Divergence: 553.8268\n",
            "Epoch[1/20], Step [110/938], Reconstruction Loss: 11614.8740, KL Divergence: 692.5308\n",
            "Epoch[1/20], Step [120/938], Reconstruction Loss: 10313.9229, KL Divergence: 746.7720\n",
            "Epoch[1/20], Step [130/938], Reconstruction Loss: 10024.8193, KL Divergence: 794.9501\n",
            "Epoch[1/20], Step [140/938], Reconstruction Loss: 10565.7852, KL Divergence: 833.5703\n",
            "Epoch[1/20], Step [150/938], Reconstruction Loss: 10287.2686, KL Divergence: 775.1121\n",
            "Epoch[1/20], Step [160/938], Reconstruction Loss: 9867.0928, KL Divergence: 902.6984\n",
            "Epoch[1/20], Step [170/938], Reconstruction Loss: 9060.2764, KL Divergence: 942.6378\n",
            "Epoch[1/20], Step [180/938], Reconstruction Loss: 8956.2949, KL Divergence: 880.3208\n",
            "Epoch[1/20], Step [190/938], Reconstruction Loss: 9329.7246, KL Divergence: 1026.9436\n",
            "Epoch[1/20], Step [200/938], Reconstruction Loss: 9070.2070, KL Divergence: 976.6708\n",
            "Epoch[1/20], Step [210/938], Reconstruction Loss: 8137.1880, KL Divergence: 982.0186\n",
            "Epoch[1/20], Step [220/938], Reconstruction Loss: 8888.5283, KL Divergence: 992.5565\n",
            "Epoch[1/20], Step [230/938], Reconstruction Loss: 8829.9805, KL Divergence: 1008.2658\n",
            "Epoch[1/20], Step [240/938], Reconstruction Loss: 9358.7051, KL Divergence: 986.7409\n",
            "Epoch[1/20], Step [250/938], Reconstruction Loss: 9065.6484, KL Divergence: 1037.1310\n",
            "Epoch[1/20], Step [260/938], Reconstruction Loss: 8710.1260, KL Divergence: 989.9503\n",
            "Epoch[1/20], Step [270/938], Reconstruction Loss: 8353.1162, KL Divergence: 1099.3820\n",
            "Epoch[1/20], Step [280/938], Reconstruction Loss: 8655.8799, KL Divergence: 1065.9254\n",
            "Epoch[1/20], Step [290/938], Reconstruction Loss: 8166.3687, KL Divergence: 1021.6863\n",
            "Epoch[1/20], Step [300/938], Reconstruction Loss: 8069.3726, KL Divergence: 978.7256\n",
            "Epoch[1/20], Step [310/938], Reconstruction Loss: 8482.4287, KL Divergence: 1152.9446\n",
            "Epoch[1/20], Step [320/938], Reconstruction Loss: 7976.0522, KL Divergence: 1095.1056\n",
            "Epoch[1/20], Step [330/938], Reconstruction Loss: 7487.9795, KL Divergence: 1200.5487\n",
            "Epoch[1/20], Step [340/938], Reconstruction Loss: 8325.9404, KL Divergence: 1110.3187\n",
            "Epoch[1/20], Step [350/938], Reconstruction Loss: 8576.4443, KL Divergence: 1187.4578\n",
            "Epoch[1/20], Step [360/938], Reconstruction Loss: 7680.2754, KL Divergence: 1214.8953\n",
            "Epoch[1/20], Step [370/938], Reconstruction Loss: 7391.0264, KL Divergence: 1222.4928\n",
            "Epoch[1/20], Step [380/938], Reconstruction Loss: 7576.0942, KL Divergence: 1161.4664\n",
            "Epoch[1/20], Step [390/938], Reconstruction Loss: 7597.5146, KL Divergence: 1314.1296\n",
            "Epoch[1/20], Step [400/938], Reconstruction Loss: 6973.1255, KL Divergence: 1280.1727\n",
            "Epoch[1/20], Step [410/938], Reconstruction Loss: 7019.2026, KL Divergence: 1348.5635\n",
            "Epoch[1/20], Step [420/938], Reconstruction Loss: 7613.4844, KL Divergence: 1221.4882\n",
            "Epoch[1/20], Step [430/938], Reconstruction Loss: 7304.8198, KL Divergence: 1347.1591\n",
            "Epoch[1/20], Step [440/938], Reconstruction Loss: 7996.5254, KL Divergence: 1311.3239\n",
            "Epoch[1/20], Step [450/938], Reconstruction Loss: 6924.3579, KL Divergence: 1311.1644\n",
            "Epoch[1/20], Step [460/938], Reconstruction Loss: 7015.3179, KL Divergence: 1329.0328\n",
            "Epoch[1/20], Step [470/938], Reconstruction Loss: 6833.6172, KL Divergence: 1277.3184\n",
            "Epoch[1/20], Step [480/938], Reconstruction Loss: 7091.6841, KL Divergence: 1363.0793\n",
            "Epoch[1/20], Step [490/938], Reconstruction Loss: 6908.5474, KL Divergence: 1287.5658\n",
            "Epoch[1/20], Step [500/938], Reconstruction Loss: 7048.1646, KL Divergence: 1242.2439\n",
            "Epoch[1/20], Step [510/938], Reconstruction Loss: 7044.1860, KL Divergence: 1408.0056\n",
            "Epoch[1/20], Step [520/938], Reconstruction Loss: 6916.7817, KL Divergence: 1338.3271\n",
            "Epoch[1/20], Step [530/938], Reconstruction Loss: 7468.3833, KL Divergence: 1322.9250\n",
            "Epoch[1/20], Step [540/938], Reconstruction Loss: 6932.6260, KL Divergence: 1338.9939\n",
            "Epoch[1/20], Step [550/938], Reconstruction Loss: 7078.6509, KL Divergence: 1317.0210\n",
            "Epoch[1/20], Step [560/938], Reconstruction Loss: 7302.4312, KL Divergence: 1458.0081\n",
            "Epoch[1/20], Step [570/938], Reconstruction Loss: 7147.2207, KL Divergence: 1298.7434\n",
            "Epoch[1/20], Step [580/938], Reconstruction Loss: 6948.9434, KL Divergence: 1325.3503\n",
            "Epoch[1/20], Step [590/938], Reconstruction Loss: 6858.7441, KL Divergence: 1352.2262\n",
            "Epoch[1/20], Step [600/938], Reconstruction Loss: 6892.5420, KL Divergence: 1310.8467\n",
            "Epoch[1/20], Step [610/938], Reconstruction Loss: 7002.4185, KL Divergence: 1290.2081\n",
            "Epoch[1/20], Step [620/938], Reconstruction Loss: 6294.2437, KL Divergence: 1328.8822\n",
            "Epoch[1/20], Step [630/938], Reconstruction Loss: 6457.6167, KL Divergence: 1325.6345\n",
            "Epoch[1/20], Step [640/938], Reconstruction Loss: 7025.5205, KL Divergence: 1366.9150\n",
            "Epoch[1/20], Step [650/938], Reconstruction Loss: 6413.6846, KL Divergence: 1346.4971\n",
            "Epoch[1/20], Step [660/938], Reconstruction Loss: 6682.7222, KL Divergence: 1368.9146\n",
            "Epoch[1/20], Step [670/938], Reconstruction Loss: 6325.3887, KL Divergence: 1395.6633\n",
            "Epoch[1/20], Step [680/938], Reconstruction Loss: 6853.4824, KL Divergence: 1347.3552\n",
            "Epoch[1/20], Step [690/938], Reconstruction Loss: 6648.4600, KL Divergence: 1356.7411\n",
            "Epoch[1/20], Step [700/938], Reconstruction Loss: 6262.6548, KL Divergence: 1398.6248\n",
            "Epoch[1/20], Step [710/938], Reconstruction Loss: 6207.1875, KL Divergence: 1485.0924\n",
            "Epoch[1/20], Step [720/938], Reconstruction Loss: 6624.6348, KL Divergence: 1411.5105\n",
            "Epoch[1/20], Step [730/938], Reconstruction Loss: 6235.3354, KL Divergence: 1425.1714\n",
            "Epoch[1/20], Step [740/938], Reconstruction Loss: 6327.7598, KL Divergence: 1351.8748\n",
            "Epoch[1/20], Step [750/938], Reconstruction Loss: 6505.9297, KL Divergence: 1422.2103\n",
            "Epoch[1/20], Step [760/938], Reconstruction Loss: 6230.5986, KL Divergence: 1479.9792\n",
            "Epoch[1/20], Step [770/938], Reconstruction Loss: 6444.9033, KL Divergence: 1369.3657\n",
            "Epoch[1/20], Step [780/938], Reconstruction Loss: 6451.7573, KL Divergence: 1491.6095\n",
            "Epoch[1/20], Step [790/938], Reconstruction Loss: 6451.7124, KL Divergence: 1413.0927\n",
            "Epoch[1/20], Step [800/938], Reconstruction Loss: 6105.9111, KL Divergence: 1402.8051\n",
            "Epoch[1/20], Step [810/938], Reconstruction Loss: 6560.4287, KL Divergence: 1420.6230\n",
            "Epoch[1/20], Step [820/938], Reconstruction Loss: 6549.1294, KL Divergence: 1389.8049\n",
            "Epoch[1/20], Step [830/938], Reconstruction Loss: 6171.8755, KL Divergence: 1465.0659\n",
            "Epoch[1/20], Step [840/938], Reconstruction Loss: 6108.7598, KL Divergence: 1402.9240\n",
            "Epoch[1/20], Step [850/938], Reconstruction Loss: 6143.2012, KL Divergence: 1434.5797\n",
            "Epoch[1/20], Step [860/938], Reconstruction Loss: 6492.1880, KL Divergence: 1391.5270\n",
            "Epoch[1/20], Step [870/938], Reconstruction Loss: 6070.4385, KL Divergence: 1472.6443\n",
            "Epoch[1/20], Step [880/938], Reconstruction Loss: 6364.8530, KL Divergence: 1465.6346\n",
            "Epoch[1/20], Step [890/938], Reconstruction Loss: 6087.4873, KL Divergence: 1520.3668\n",
            "Epoch[1/20], Step [900/938], Reconstruction Loss: 6004.6899, KL Divergence: 1591.1083\n",
            "Epoch[1/20], Step [910/938], Reconstruction Loss: 6510.0776, KL Divergence: 1415.7261\n",
            "Epoch[1/20], Step [920/938], Reconstruction Loss: 6283.5684, KL Divergence: 1481.1376\n",
            "Epoch[1/20], Step [930/938], Reconstruction Loss: 5644.6421, KL Divergence: 1497.6658\n",
            "Epoch[2/20], Step [10/938], Reconstruction Loss: 6125.8750, KL Divergence: 1420.3123\n",
            "Epoch[2/20], Step [20/938], Reconstruction Loss: 6260.7144, KL Divergence: 1549.9495\n",
            "Epoch[2/20], Step [30/938], Reconstruction Loss: 6351.4561, KL Divergence: 1485.1415\n",
            "Epoch[2/20], Step [40/938], Reconstruction Loss: 6269.8198, KL Divergence: 1514.6343\n",
            "Epoch[2/20], Step [50/938], Reconstruction Loss: 5995.0229, KL Divergence: 1457.7463\n",
            "Epoch[2/20], Step [60/938], Reconstruction Loss: 6270.3438, KL Divergence: 1509.7278\n",
            "Epoch[2/20], Step [70/938], Reconstruction Loss: 6126.3369, KL Divergence: 1421.2391\n",
            "Epoch[2/20], Step [80/938], Reconstruction Loss: 6039.3540, KL Divergence: 1535.7079\n",
            "Epoch[2/20], Step [90/938], Reconstruction Loss: 6020.4849, KL Divergence: 1493.9395\n",
            "Epoch[2/20], Step [100/938], Reconstruction Loss: 6200.5366, KL Divergence: 1484.1573\n",
            "Epoch[2/20], Step [110/938], Reconstruction Loss: 6220.7598, KL Divergence: 1509.9899\n",
            "Epoch[2/20], Step [120/938], Reconstruction Loss: 5524.9805, KL Divergence: 1555.1340\n",
            "Epoch[2/20], Step [130/938], Reconstruction Loss: 6106.1631, KL Divergence: 1475.7603\n",
            "Epoch[2/20], Step [140/938], Reconstruction Loss: 6208.4482, KL Divergence: 1474.6349\n",
            "Epoch[2/20], Step [150/938], Reconstruction Loss: 5863.1479, KL Divergence: 1544.7521\n",
            "Epoch[2/20], Step [160/938], Reconstruction Loss: 6263.4116, KL Divergence: 1450.7577\n",
            "Epoch[2/20], Step [170/938], Reconstruction Loss: 6198.5679, KL Divergence: 1555.3623\n",
            "Epoch[2/20], Step [180/938], Reconstruction Loss: 6029.6538, KL Divergence: 1494.4940\n",
            "Epoch[2/20], Step [190/938], Reconstruction Loss: 5969.2393, KL Divergence: 1531.6617\n",
            "Epoch[2/20], Step [200/938], Reconstruction Loss: 5863.2393, KL Divergence: 1527.0957\n",
            "Epoch[2/20], Step [210/938], Reconstruction Loss: 6490.0967, KL Divergence: 1486.6995\n",
            "Epoch[2/20], Step [220/938], Reconstruction Loss: 6019.9185, KL Divergence: 1550.5146\n",
            "Epoch[2/20], Step [230/938], Reconstruction Loss: 6004.7207, KL Divergence: 1534.9269\n",
            "Epoch[2/20], Step [240/938], Reconstruction Loss: 5812.3931, KL Divergence: 1520.8438\n",
            "Epoch[2/20], Step [250/938], Reconstruction Loss: 5969.3257, KL Divergence: 1456.3015\n",
            "Epoch[2/20], Step [260/938], Reconstruction Loss: 6172.2974, KL Divergence: 1501.2065\n",
            "Epoch[2/20], Step [270/938], Reconstruction Loss: 6429.2983, KL Divergence: 1523.4010\n",
            "Epoch[2/20], Step [280/938], Reconstruction Loss: 5793.8721, KL Divergence: 1509.7446\n",
            "Epoch[2/20], Step [290/938], Reconstruction Loss: 5831.1738, KL Divergence: 1512.8262\n",
            "Epoch[2/20], Step [300/938], Reconstruction Loss: 5897.0049, KL Divergence: 1568.1461\n",
            "Epoch[2/20], Step [310/938], Reconstruction Loss: 5860.4980, KL Divergence: 1483.0175\n",
            "Epoch[2/20], Step [320/938], Reconstruction Loss: 5694.0449, KL Divergence: 1549.9799\n",
            "Epoch[2/20], Step [330/938], Reconstruction Loss: 5337.2632, KL Divergence: 1474.7925\n",
            "Epoch[2/20], Step [340/938], Reconstruction Loss: 6251.8545, KL Divergence: 1582.5261\n",
            "Epoch[2/20], Step [350/938], Reconstruction Loss: 5596.1860, KL Divergence: 1488.9719\n",
            "Epoch[2/20], Step [360/938], Reconstruction Loss: 6073.8677, KL Divergence: 1528.1505\n",
            "Epoch[2/20], Step [370/938], Reconstruction Loss: 5951.7017, KL Divergence: 1581.5120\n",
            "Epoch[2/20], Step [380/938], Reconstruction Loss: 5969.9219, KL Divergence: 1529.9017\n",
            "Epoch[2/20], Step [390/938], Reconstruction Loss: 5773.9805, KL Divergence: 1468.8807\n",
            "Epoch[2/20], Step [400/938], Reconstruction Loss: 5752.5425, KL Divergence: 1512.3601\n",
            "Epoch[2/20], Step [410/938], Reconstruction Loss: 6061.6880, KL Divergence: 1546.6669\n",
            "Epoch[2/20], Step [420/938], Reconstruction Loss: 5915.7930, KL Divergence: 1561.1993\n",
            "Epoch[2/20], Step [430/938], Reconstruction Loss: 6279.5781, KL Divergence: 1613.4993\n",
            "Epoch[2/20], Step [440/938], Reconstruction Loss: 5427.7349, KL Divergence: 1468.3702\n",
            "Epoch[2/20], Step [450/938], Reconstruction Loss: 5498.8979, KL Divergence: 1495.0411\n",
            "Epoch[2/20], Step [460/938], Reconstruction Loss: 5605.3125, KL Divergence: 1479.5504\n",
            "Epoch[2/20], Step [470/938], Reconstruction Loss: 5528.6411, KL Divergence: 1500.2354\n",
            "Epoch[2/20], Step [480/938], Reconstruction Loss: 5761.4648, KL Divergence: 1573.5206\n",
            "Epoch[2/20], Step [490/938], Reconstruction Loss: 5591.7705, KL Divergence: 1455.9233\n",
            "Epoch[2/20], Step [500/938], Reconstruction Loss: 5939.8491, KL Divergence: 1519.1342\n",
            "Epoch[2/20], Step [510/938], Reconstruction Loss: 5812.2925, KL Divergence: 1615.1208\n",
            "Epoch[2/20], Step [520/938], Reconstruction Loss: 6073.7295, KL Divergence: 1525.8922\n",
            "Epoch[2/20], Step [530/938], Reconstruction Loss: 5992.2856, KL Divergence: 1477.9001\n",
            "Epoch[2/20], Step [540/938], Reconstruction Loss: 5555.4438, KL Divergence: 1538.3005\n",
            "Epoch[2/20], Step [550/938], Reconstruction Loss: 6031.6426, KL Divergence: 1542.6724\n",
            "Epoch[2/20], Step [560/938], Reconstruction Loss: 5641.5078, KL Divergence: 1603.8549\n",
            "Epoch[2/20], Step [570/938], Reconstruction Loss: 5615.5601, KL Divergence: 1513.4183\n",
            "Epoch[2/20], Step [580/938], Reconstruction Loss: 5900.6216, KL Divergence: 1571.8739\n",
            "Epoch[2/20], Step [590/938], Reconstruction Loss: 5710.4424, KL Divergence: 1561.7021\n",
            "Epoch[2/20], Step [600/938], Reconstruction Loss: 5762.4399, KL Divergence: 1563.5094\n",
            "Epoch[2/20], Step [610/938], Reconstruction Loss: 5890.6313, KL Divergence: 1573.7004\n",
            "Epoch[2/20], Step [620/938], Reconstruction Loss: 5614.0044, KL Divergence: 1584.4113\n",
            "Epoch[2/20], Step [630/938], Reconstruction Loss: 5708.5688, KL Divergence: 1506.4703\n",
            "Epoch[2/20], Step [640/938], Reconstruction Loss: 5822.1406, KL Divergence: 1567.7812\n",
            "Epoch[2/20], Step [650/938], Reconstruction Loss: 5684.5557, KL Divergence: 1511.6459\n",
            "Epoch[2/20], Step [660/938], Reconstruction Loss: 5447.6206, KL Divergence: 1543.1354\n",
            "Epoch[2/20], Step [670/938], Reconstruction Loss: 6138.7188, KL Divergence: 1550.7682\n",
            "Epoch[2/20], Step [680/938], Reconstruction Loss: 5244.8916, KL Divergence: 1543.7137\n",
            "Epoch[2/20], Step [690/938], Reconstruction Loss: 6034.6572, KL Divergence: 1567.5175\n",
            "Epoch[2/20], Step [700/938], Reconstruction Loss: 5979.8853, KL Divergence: 1562.7373\n",
            "Epoch[2/20], Step [710/938], Reconstruction Loss: 5449.0117, KL Divergence: 1422.9701\n",
            "Epoch[2/20], Step [720/938], Reconstruction Loss: 5847.2744, KL Divergence: 1552.9452\n",
            "Epoch[2/20], Step [730/938], Reconstruction Loss: 5790.1436, KL Divergence: 1590.8562\n",
            "Epoch[2/20], Step [740/938], Reconstruction Loss: 5775.1104, KL Divergence: 1652.7573\n",
            "Epoch[2/20], Step [750/938], Reconstruction Loss: 5819.9429, KL Divergence: 1647.4542\n",
            "Epoch[2/20], Step [760/938], Reconstruction Loss: 5384.6279, KL Divergence: 1513.1942\n",
            "Epoch[2/20], Step [770/938], Reconstruction Loss: 5748.6812, KL Divergence: 1563.0756\n",
            "Epoch[2/20], Step [780/938], Reconstruction Loss: 5577.0586, KL Divergence: 1504.5653\n",
            "Epoch[2/20], Step [790/938], Reconstruction Loss: 5582.5112, KL Divergence: 1552.7133\n",
            "Epoch[2/20], Step [800/938], Reconstruction Loss: 5781.6177, KL Divergence: 1569.1206\n",
            "Epoch[2/20], Step [810/938], Reconstruction Loss: 5798.6929, KL Divergence: 1577.5591\n",
            "Epoch[2/20], Step [820/938], Reconstruction Loss: 5630.9116, KL Divergence: 1529.5421\n",
            "Epoch[2/20], Step [830/938], Reconstruction Loss: 5912.9590, KL Divergence: 1633.5753\n",
            "Epoch[2/20], Step [840/938], Reconstruction Loss: 5614.1133, KL Divergence: 1579.9021\n",
            "Epoch[2/20], Step [850/938], Reconstruction Loss: 5827.1045, KL Divergence: 1627.1910\n",
            "Epoch[2/20], Step [860/938], Reconstruction Loss: 6071.4644, KL Divergence: 1558.8256\n",
            "Epoch[2/20], Step [870/938], Reconstruction Loss: 5948.5557, KL Divergence: 1626.6299\n",
            "Epoch[2/20], Step [880/938], Reconstruction Loss: 5611.2773, KL Divergence: 1598.0348\n",
            "Epoch[2/20], Step [890/938], Reconstruction Loss: 5530.4907, KL Divergence: 1573.4275\n",
            "Epoch[2/20], Step [900/938], Reconstruction Loss: 5538.1465, KL Divergence: 1565.3334\n",
            "Epoch[2/20], Step [910/938], Reconstruction Loss: 5614.9673, KL Divergence: 1480.0237\n",
            "Epoch[2/20], Step [920/938], Reconstruction Loss: 5463.3345, KL Divergence: 1605.2678\n",
            "Epoch[2/20], Step [930/938], Reconstruction Loss: 5683.4678, KL Divergence: 1609.1530\n",
            "Epoch[3/20], Step [10/938], Reconstruction Loss: 5625.2896, KL Divergence: 1574.5057\n",
            "Epoch[3/20], Step [20/938], Reconstruction Loss: 5954.6768, KL Divergence: 1578.6296\n",
            "Epoch[3/20], Step [30/938], Reconstruction Loss: 5465.5259, KL Divergence: 1560.0132\n",
            "Epoch[3/20], Step [40/938], Reconstruction Loss: 5607.9771, KL Divergence: 1521.2753\n",
            "Epoch[3/20], Step [50/938], Reconstruction Loss: 5578.5737, KL Divergence: 1589.9579\n",
            "Epoch[3/20], Step [60/938], Reconstruction Loss: 5807.1846, KL Divergence: 1645.7839\n",
            "Epoch[3/20], Step [70/938], Reconstruction Loss: 5431.7231, KL Divergence: 1586.5995\n",
            "Epoch[3/20], Step [80/938], Reconstruction Loss: 5665.4497, KL Divergence: 1552.8707\n",
            "Epoch[3/20], Step [90/938], Reconstruction Loss: 5633.7446, KL Divergence: 1596.6217\n",
            "Epoch[3/20], Step [100/938], Reconstruction Loss: 5617.4531, KL Divergence: 1601.4934\n",
            "Epoch[3/20], Step [110/938], Reconstruction Loss: 5294.2144, KL Divergence: 1571.6259\n",
            "Epoch[3/20], Step [120/938], Reconstruction Loss: 5295.8472, KL Divergence: 1509.2777\n",
            "Epoch[3/20], Step [130/938], Reconstruction Loss: 5773.2695, KL Divergence: 1569.5089\n",
            "Epoch[3/20], Step [140/938], Reconstruction Loss: 5494.3833, KL Divergence: 1575.8176\n",
            "Epoch[3/20], Step [150/938], Reconstruction Loss: 5454.3540, KL Divergence: 1585.0375\n",
            "Epoch[3/20], Step [160/938], Reconstruction Loss: 5451.3706, KL Divergence: 1495.7263\n",
            "Epoch[3/20], Step [170/938], Reconstruction Loss: 5822.1299, KL Divergence: 1624.2002\n",
            "Epoch[3/20], Step [180/938], Reconstruction Loss: 5376.9312, KL Divergence: 1602.6251\n",
            "Epoch[3/20], Step [190/938], Reconstruction Loss: 5459.9497, KL Divergence: 1543.1326\n",
            "Epoch[3/20], Step [200/938], Reconstruction Loss: 5514.8232, KL Divergence: 1594.5872\n",
            "Epoch[3/20], Step [210/938], Reconstruction Loss: 5642.5610, KL Divergence: 1596.3435\n",
            "Epoch[3/20], Step [220/938], Reconstruction Loss: 5678.0493, KL Divergence: 1593.6127\n",
            "Epoch[3/20], Step [230/938], Reconstruction Loss: 5828.5352, KL Divergence: 1636.9913\n",
            "Epoch[3/20], Step [240/938], Reconstruction Loss: 5342.7305, KL Divergence: 1538.8522\n",
            "Epoch[3/20], Step [250/938], Reconstruction Loss: 5172.3892, KL Divergence: 1527.7620\n",
            "Epoch[3/20], Step [260/938], Reconstruction Loss: 5736.6421, KL Divergence: 1547.0015\n",
            "Epoch[3/20], Step [270/938], Reconstruction Loss: 5713.9961, KL Divergence: 1653.2635\n",
            "Epoch[3/20], Step [280/938], Reconstruction Loss: 5846.0547, KL Divergence: 1631.6355\n",
            "Epoch[3/20], Step [290/938], Reconstruction Loss: 5906.6851, KL Divergence: 1644.0858\n",
            "Epoch[3/20], Step [300/938], Reconstruction Loss: 5375.6499, KL Divergence: 1548.9166\n",
            "Epoch[3/20], Step [310/938], Reconstruction Loss: 5913.4834, KL Divergence: 1632.7291\n",
            "Epoch[3/20], Step [320/938], Reconstruction Loss: 5801.6865, KL Divergence: 1679.5728\n",
            "Epoch[3/20], Step [330/938], Reconstruction Loss: 5542.6982, KL Divergence: 1664.8917\n",
            "Epoch[3/20], Step [340/938], Reconstruction Loss: 5925.8364, KL Divergence: 1681.1542\n",
            "Epoch[3/20], Step [350/938], Reconstruction Loss: 5356.0898, KL Divergence: 1633.2084\n",
            "Epoch[3/20], Step [360/938], Reconstruction Loss: 5243.8203, KL Divergence: 1521.6321\n",
            "Epoch[3/20], Step [370/938], Reconstruction Loss: 5559.1436, KL Divergence: 1613.0875\n",
            "Epoch[3/20], Step [380/938], Reconstruction Loss: 5940.6392, KL Divergence: 1617.0924\n",
            "Epoch[3/20], Step [390/938], Reconstruction Loss: 5683.9341, KL Divergence: 1604.9155\n",
            "Epoch[3/20], Step [400/938], Reconstruction Loss: 5424.4570, KL Divergence: 1541.9681\n",
            "Epoch[3/20], Step [410/938], Reconstruction Loss: 5533.0762, KL Divergence: 1595.6290\n",
            "Epoch[3/20], Step [420/938], Reconstruction Loss: 5360.8564, KL Divergence: 1579.9812\n",
            "Epoch[3/20], Step [430/938], Reconstruction Loss: 5767.2871, KL Divergence: 1593.7312\n",
            "Epoch[3/20], Step [440/938], Reconstruction Loss: 5509.4189, KL Divergence: 1546.3854\n",
            "Epoch[3/20], Step [450/938], Reconstruction Loss: 5259.5781, KL Divergence: 1520.6069\n",
            "Epoch[3/20], Step [460/938], Reconstruction Loss: 5755.2710, KL Divergence: 1559.2250\n",
            "Epoch[3/20], Step [470/938], Reconstruction Loss: 5630.0410, KL Divergence: 1652.9701\n",
            "Epoch[3/20], Step [480/938], Reconstruction Loss: 5689.0293, KL Divergence: 1618.5895\n",
            "Epoch[3/20], Step [490/938], Reconstruction Loss: 5534.0156, KL Divergence: 1559.5522\n",
            "Epoch[3/20], Step [500/938], Reconstruction Loss: 5550.0488, KL Divergence: 1545.4009\n",
            "Epoch[3/20], Step [510/938], Reconstruction Loss: 5361.0889, KL Divergence: 1577.2102\n",
            "Epoch[3/20], Step [520/938], Reconstruction Loss: 5499.9219, KL Divergence: 1601.0153\n",
            "Epoch[3/20], Step [530/938], Reconstruction Loss: 5711.2935, KL Divergence: 1654.5438\n",
            "Epoch[3/20], Step [540/938], Reconstruction Loss: 5788.5850, KL Divergence: 1628.7699\n",
            "Epoch[3/20], Step [550/938], Reconstruction Loss: 5629.3994, KL Divergence: 1647.3765\n",
            "Epoch[3/20], Step [560/938], Reconstruction Loss: 5410.2363, KL Divergence: 1576.3347\n",
            "Epoch[3/20], Step [570/938], Reconstruction Loss: 5667.7456, KL Divergence: 1627.7150\n",
            "Epoch[3/20], Step [580/938], Reconstruction Loss: 5524.7793, KL Divergence: 1613.8325\n",
            "Epoch[3/20], Step [590/938], Reconstruction Loss: 5305.0044, KL Divergence: 1592.2134\n",
            "Epoch[3/20], Step [600/938], Reconstruction Loss: 5465.3716, KL Divergence: 1545.3623\n",
            "Epoch[3/20], Step [610/938], Reconstruction Loss: 5552.3838, KL Divergence: 1575.4495\n",
            "Epoch[3/20], Step [620/938], Reconstruction Loss: 5390.3213, KL Divergence: 1609.0652\n",
            "Epoch[3/20], Step [630/938], Reconstruction Loss: 5566.2300, KL Divergence: 1574.5690\n",
            "Epoch[3/20], Step [640/938], Reconstruction Loss: 5353.6094, KL Divergence: 1631.5634\n",
            "Epoch[3/20], Step [650/938], Reconstruction Loss: 5244.2017, KL Divergence: 1535.9254\n",
            "Epoch[3/20], Step [660/938], Reconstruction Loss: 5602.0698, KL Divergence: 1605.4034\n",
            "Epoch[3/20], Step [670/938], Reconstruction Loss: 5395.9609, KL Divergence: 1626.7371\n",
            "Epoch[3/20], Step [680/938], Reconstruction Loss: 5454.1436, KL Divergence: 1640.2814\n",
            "Epoch[3/20], Step [690/938], Reconstruction Loss: 5503.1948, KL Divergence: 1572.5791\n",
            "Epoch[3/20], Step [700/938], Reconstruction Loss: 5321.8374, KL Divergence: 1583.7136\n",
            "Epoch[3/20], Step [710/938], Reconstruction Loss: 5321.4800, KL Divergence: 1545.4705\n",
            "Epoch[3/20], Step [720/938], Reconstruction Loss: 5732.5181, KL Divergence: 1606.7321\n",
            "Epoch[3/20], Step [730/938], Reconstruction Loss: 5535.0068, KL Divergence: 1530.1610\n",
            "Epoch[3/20], Step [740/938], Reconstruction Loss: 5492.9922, KL Divergence: 1605.2037\n",
            "Epoch[3/20], Step [750/938], Reconstruction Loss: 5674.3784, KL Divergence: 1577.4589\n",
            "Epoch[3/20], Step [760/938], Reconstruction Loss: 5314.0405, KL Divergence: 1560.4358\n",
            "Epoch[3/20], Step [770/938], Reconstruction Loss: 5704.9961, KL Divergence: 1567.5865\n",
            "Epoch[3/20], Step [780/938], Reconstruction Loss: 5586.2114, KL Divergence: 1651.9829\n",
            "Epoch[3/20], Step [790/938], Reconstruction Loss: 5635.9443, KL Divergence: 1646.0355\n",
            "Epoch[3/20], Step [800/938], Reconstruction Loss: 5641.7954, KL Divergence: 1624.5676\n",
            "Epoch[3/20], Step [810/938], Reconstruction Loss: 5141.4189, KL Divergence: 1506.3350\n",
            "Epoch[3/20], Step [820/938], Reconstruction Loss: 5263.8853, KL Divergence: 1626.7827\n",
            "Epoch[3/20], Step [830/938], Reconstruction Loss: 5299.9609, KL Divergence: 1515.7374\n",
            "Epoch[3/20], Step [840/938], Reconstruction Loss: 5690.6182, KL Divergence: 1592.1964\n",
            "Epoch[3/20], Step [850/938], Reconstruction Loss: 5629.3042, KL Divergence: 1591.7291\n",
            "Epoch[3/20], Step [860/938], Reconstruction Loss: 5268.6836, KL Divergence: 1508.7224\n",
            "Epoch[3/20], Step [870/938], Reconstruction Loss: 5813.7290, KL Divergence: 1643.1715\n",
            "Epoch[3/20], Step [880/938], Reconstruction Loss: 5199.5835, KL Divergence: 1592.8264\n",
            "Epoch[3/20], Step [890/938], Reconstruction Loss: 5697.4150, KL Divergence: 1516.1749\n",
            "Epoch[3/20], Step [900/938], Reconstruction Loss: 5710.8345, KL Divergence: 1640.2950\n",
            "Epoch[3/20], Step [910/938], Reconstruction Loss: 4972.3774, KL Divergence: 1510.2097\n",
            "Epoch[3/20], Step [920/938], Reconstruction Loss: 5575.4004, KL Divergence: 1572.2686\n",
            "Epoch[3/20], Step [930/938], Reconstruction Loss: 5464.5171, KL Divergence: 1637.3477\n",
            "Epoch[4/20], Step [10/938], Reconstruction Loss: 5622.1816, KL Divergence: 1578.2521\n",
            "Epoch[4/20], Step [20/938], Reconstruction Loss: 5368.8979, KL Divergence: 1632.0284\n",
            "Epoch[4/20], Step [30/938], Reconstruction Loss: 5857.3994, KL Divergence: 1578.3137\n",
            "Epoch[4/20], Step [40/938], Reconstruction Loss: 5558.2817, KL Divergence: 1636.3292\n",
            "Epoch[4/20], Step [50/938], Reconstruction Loss: 5767.6230, KL Divergence: 1691.0824\n",
            "Epoch[4/20], Step [60/938], Reconstruction Loss: 5361.5020, KL Divergence: 1588.7994\n",
            "Epoch[4/20], Step [70/938], Reconstruction Loss: 5362.7827, KL Divergence: 1616.2363\n",
            "Epoch[4/20], Step [80/938], Reconstruction Loss: 5324.4380, KL Divergence: 1604.8589\n",
            "Epoch[4/20], Step [90/938], Reconstruction Loss: 5866.0757, KL Divergence: 1593.3615\n",
            "Epoch[4/20], Step [100/938], Reconstruction Loss: 5200.2324, KL Divergence: 1563.9180\n",
            "Epoch[4/20], Step [110/938], Reconstruction Loss: 5163.7578, KL Divergence: 1551.5417\n",
            "Epoch[4/20], Step [120/938], Reconstruction Loss: 5675.3179, KL Divergence: 1545.0333\n",
            "Epoch[4/20], Step [130/938], Reconstruction Loss: 5590.2388, KL Divergence: 1603.5173\n",
            "Epoch[4/20], Step [140/938], Reconstruction Loss: 5068.2373, KL Divergence: 1603.9421\n",
            "Epoch[4/20], Step [150/938], Reconstruction Loss: 5386.6963, KL Divergence: 1640.8243\n",
            "Epoch[4/20], Step [160/938], Reconstruction Loss: 5777.5249, KL Divergence: 1588.0692\n",
            "Epoch[4/20], Step [170/938], Reconstruction Loss: 5659.6064, KL Divergence: 1569.2725\n",
            "Epoch[4/20], Step [180/938], Reconstruction Loss: 5416.2197, KL Divergence: 1548.1721\n",
            "Epoch[4/20], Step [190/938], Reconstruction Loss: 4956.5273, KL Divergence: 1580.0095\n",
            "Epoch[4/20], Step [200/938], Reconstruction Loss: 5525.7969, KL Divergence: 1637.8972\n",
            "Epoch[4/20], Step [210/938], Reconstruction Loss: 5572.9272, KL Divergence: 1600.2791\n",
            "Epoch[4/20], Step [220/938], Reconstruction Loss: 5278.5732, KL Divergence: 1646.1393\n",
            "Epoch[4/20], Step [230/938], Reconstruction Loss: 5361.0205, KL Divergence: 1580.1182\n",
            "Epoch[4/20], Step [240/938], Reconstruction Loss: 5671.3545, KL Divergence: 1678.8824\n",
            "Epoch[4/20], Step [250/938], Reconstruction Loss: 5717.3115, KL Divergence: 1646.0192\n",
            "Epoch[4/20], Step [260/938], Reconstruction Loss: 5593.0342, KL Divergence: 1652.0228\n",
            "Epoch[4/20], Step [270/938], Reconstruction Loss: 5473.3184, KL Divergence: 1588.2616\n",
            "Epoch[4/20], Step [280/938], Reconstruction Loss: 5444.0684, KL Divergence: 1622.5142\n",
            "Epoch[4/20], Step [290/938], Reconstruction Loss: 5418.6890, KL Divergence: 1596.8033\n",
            "Epoch[4/20], Step [300/938], Reconstruction Loss: 5092.5957, KL Divergence: 1613.3192\n",
            "Epoch[4/20], Step [310/938], Reconstruction Loss: 5438.1748, KL Divergence: 1604.5966\n",
            "Epoch[4/20], Step [320/938], Reconstruction Loss: 5113.5786, KL Divergence: 1678.6169\n",
            "Epoch[4/20], Step [330/938], Reconstruction Loss: 5624.0288, KL Divergence: 1600.6564\n",
            "Epoch[4/20], Step [340/938], Reconstruction Loss: 5278.8750, KL Divergence: 1673.7682\n",
            "Epoch[4/20], Step [350/938], Reconstruction Loss: 5224.8687, KL Divergence: 1594.4260\n",
            "Epoch[4/20], Step [360/938], Reconstruction Loss: 5535.5835, KL Divergence: 1660.2262\n",
            "Epoch[4/20], Step [370/938], Reconstruction Loss: 5352.4204, KL Divergence: 1564.4218\n",
            "Epoch[4/20], Step [380/938], Reconstruction Loss: 5810.8657, KL Divergence: 1668.9375\n",
            "Epoch[4/20], Step [390/938], Reconstruction Loss: 5477.7671, KL Divergence: 1562.5281\n",
            "Epoch[4/20], Step [400/938], Reconstruction Loss: 5401.5498, KL Divergence: 1648.3661\n",
            "Epoch[4/20], Step [410/938], Reconstruction Loss: 5556.4653, KL Divergence: 1639.0101\n",
            "Epoch[4/20], Step [420/938], Reconstruction Loss: 5470.7397, KL Divergence: 1608.1617\n",
            "Epoch[4/20], Step [430/938], Reconstruction Loss: 5708.7480, KL Divergence: 1635.9695\n",
            "Epoch[4/20], Step [440/938], Reconstruction Loss: 5387.3662, KL Divergence: 1621.1907\n",
            "Epoch[4/20], Step [450/938], Reconstruction Loss: 5854.5703, KL Divergence: 1615.6857\n",
            "Epoch[4/20], Step [460/938], Reconstruction Loss: 5822.2378, KL Divergence: 1640.9791\n",
            "Epoch[4/20], Step [470/938], Reconstruction Loss: 5340.6782, KL Divergence: 1539.3801\n",
            "Epoch[4/20], Step [480/938], Reconstruction Loss: 5444.7012, KL Divergence: 1594.8411\n",
            "Epoch[4/20], Step [490/938], Reconstruction Loss: 5352.4604, KL Divergence: 1548.9446\n",
            "Epoch[4/20], Step [500/938], Reconstruction Loss: 5562.9873, KL Divergence: 1636.5948\n",
            "Epoch[4/20], Step [510/938], Reconstruction Loss: 5557.9858, KL Divergence: 1548.4329\n",
            "Epoch[4/20], Step [520/938], Reconstruction Loss: 5289.3613, KL Divergence: 1584.0288\n",
            "Epoch[4/20], Step [530/938], Reconstruction Loss: 5368.2725, KL Divergence: 1584.5636\n",
            "Epoch[4/20], Step [540/938], Reconstruction Loss: 5163.3071, KL Divergence: 1592.9653\n",
            "Epoch[4/20], Step [550/938], Reconstruction Loss: 5520.9912, KL Divergence: 1622.2234\n",
            "Epoch[4/20], Step [560/938], Reconstruction Loss: 5519.6250, KL Divergence: 1622.7740\n",
            "Epoch[4/20], Step [570/938], Reconstruction Loss: 5443.7920, KL Divergence: 1637.0298\n",
            "Epoch[4/20], Step [580/938], Reconstruction Loss: 5311.1460, KL Divergence: 1555.2775\n",
            "Epoch[4/20], Step [590/938], Reconstruction Loss: 5665.7964, KL Divergence: 1620.6046\n",
            "Epoch[4/20], Step [600/938], Reconstruction Loss: 5601.6587, KL Divergence: 1552.9495\n",
            "Epoch[4/20], Step [610/938], Reconstruction Loss: 5411.5034, KL Divergence: 1501.3430\n",
            "Epoch[4/20], Step [620/938], Reconstruction Loss: 5451.4355, KL Divergence: 1617.4830\n",
            "Epoch[4/20], Step [630/938], Reconstruction Loss: 5194.5322, KL Divergence: 1543.7363\n",
            "Epoch[4/20], Step [640/938], Reconstruction Loss: 5153.5986, KL Divergence: 1576.5259\n",
            "Epoch[4/20], Step [650/938], Reconstruction Loss: 5115.0640, KL Divergence: 1538.2458\n",
            "Epoch[4/20], Step [660/938], Reconstruction Loss: 5251.1685, KL Divergence: 1598.7404\n",
            "Epoch[4/20], Step [670/938], Reconstruction Loss: 5199.7588, KL Divergence: 1529.7594\n",
            "Epoch[4/20], Step [680/938], Reconstruction Loss: 5123.7744, KL Divergence: 1645.5890\n",
            "Epoch[4/20], Step [690/938], Reconstruction Loss: 5703.2656, KL Divergence: 1585.2180\n",
            "Epoch[4/20], Step [700/938], Reconstruction Loss: 5364.1572, KL Divergence: 1592.1987\n",
            "Epoch[4/20], Step [710/938], Reconstruction Loss: 5359.4907, KL Divergence: 1630.0369\n",
            "Epoch[4/20], Step [720/938], Reconstruction Loss: 5384.2490, KL Divergence: 1583.2318\n",
            "Epoch[4/20], Step [730/938], Reconstruction Loss: 5673.7305, KL Divergence: 1628.6801\n",
            "Epoch[4/20], Step [740/938], Reconstruction Loss: 5368.2812, KL Divergence: 1637.9180\n",
            "Epoch[4/20], Step [750/938], Reconstruction Loss: 5342.3564, KL Divergence: 1691.7004\n",
            "Epoch[4/20], Step [760/938], Reconstruction Loss: 5215.2056, KL Divergence: 1587.0146\n",
            "Epoch[4/20], Step [770/938], Reconstruction Loss: 5262.7563, KL Divergence: 1593.9536\n",
            "Epoch[4/20], Step [780/938], Reconstruction Loss: 5531.1865, KL Divergence: 1625.1890\n",
            "Epoch[4/20], Step [790/938], Reconstruction Loss: 5490.9531, KL Divergence: 1668.3107\n",
            "Epoch[4/20], Step [800/938], Reconstruction Loss: 5419.6147, KL Divergence: 1583.9584\n",
            "Epoch[4/20], Step [810/938], Reconstruction Loss: 5677.9302, KL Divergence: 1667.0144\n",
            "Epoch[4/20], Step [820/938], Reconstruction Loss: 5455.6909, KL Divergence: 1620.0508\n",
            "Epoch[4/20], Step [830/938], Reconstruction Loss: 5315.1323, KL Divergence: 1613.1145\n",
            "Epoch[4/20], Step [840/938], Reconstruction Loss: 5167.6118, KL Divergence: 1545.9242\n",
            "Epoch[4/20], Step [850/938], Reconstruction Loss: 5297.4399, KL Divergence: 1667.1329\n",
            "Epoch[4/20], Step [860/938], Reconstruction Loss: 5357.6353, KL Divergence: 1497.6992\n",
            "Epoch[4/20], Step [870/938], Reconstruction Loss: 5515.4209, KL Divergence: 1707.9080\n",
            "Epoch[4/20], Step [880/938], Reconstruction Loss: 5663.1396, KL Divergence: 1657.0261\n",
            "Epoch[4/20], Step [890/938], Reconstruction Loss: 5747.3477, KL Divergence: 1676.7122\n",
            "Epoch[4/20], Step [900/938], Reconstruction Loss: 5355.0449, KL Divergence: 1649.9337\n",
            "Epoch[4/20], Step [910/938], Reconstruction Loss: 5334.6333, KL Divergence: 1581.2461\n",
            "Epoch[4/20], Step [920/938], Reconstruction Loss: 5169.9966, KL Divergence: 1563.0806\n",
            "Epoch[4/20], Step [930/938], Reconstruction Loss: 5141.1646, KL Divergence: 1624.9202\n",
            "Epoch[5/20], Step [10/938], Reconstruction Loss: 5337.4590, KL Divergence: 1572.1240\n",
            "Epoch[5/20], Step [20/938], Reconstruction Loss: 5185.5542, KL Divergence: 1627.8506\n",
            "Epoch[5/20], Step [30/938], Reconstruction Loss: 5256.5610, KL Divergence: 1635.1851\n",
            "Epoch[5/20], Step [40/938], Reconstruction Loss: 5025.2642, KL Divergence: 1594.4819\n",
            "Epoch[5/20], Step [50/938], Reconstruction Loss: 5404.8296, KL Divergence: 1628.6366\n",
            "Epoch[5/20], Step [60/938], Reconstruction Loss: 5275.2109, KL Divergence: 1621.6954\n",
            "Epoch[5/20], Step [70/938], Reconstruction Loss: 5418.8179, KL Divergence: 1548.4880\n",
            "Epoch[5/20], Step [80/938], Reconstruction Loss: 5158.9150, KL Divergence: 1602.0969\n",
            "Epoch[5/20], Step [90/938], Reconstruction Loss: 5214.7998, KL Divergence: 1571.0778\n",
            "Epoch[5/20], Step [100/938], Reconstruction Loss: 5600.8936, KL Divergence: 1713.8217\n",
            "Epoch[5/20], Step [110/938], Reconstruction Loss: 5810.7095, KL Divergence: 1674.4639\n",
            "Epoch[5/20], Step [120/938], Reconstruction Loss: 5704.6167, KL Divergence: 1641.0323\n",
            "Epoch[5/20], Step [130/938], Reconstruction Loss: 5034.6914, KL Divergence: 1559.0856\n",
            "Epoch[5/20], Step [140/938], Reconstruction Loss: 5306.7266, KL Divergence: 1572.9641\n",
            "Epoch[5/20], Step [150/938], Reconstruction Loss: 5100.5234, KL Divergence: 1617.5309\n",
            "Epoch[5/20], Step [160/938], Reconstruction Loss: 5439.4204, KL Divergence: 1669.0323\n",
            "Epoch[5/20], Step [170/938], Reconstruction Loss: 5180.0664, KL Divergence: 1556.3914\n",
            "Epoch[5/20], Step [180/938], Reconstruction Loss: 5142.9194, KL Divergence: 1708.7872\n",
            "Epoch[5/20], Step [190/938], Reconstruction Loss: 5601.6172, KL Divergence: 1621.7339\n",
            "Epoch[5/20], Step [200/938], Reconstruction Loss: 5507.5112, KL Divergence: 1650.6260\n",
            "Epoch[5/20], Step [210/938], Reconstruction Loss: 5444.3931, KL Divergence: 1591.3540\n",
            "Epoch[5/20], Step [220/938], Reconstruction Loss: 5447.6377, KL Divergence: 1597.9304\n",
            "Epoch[5/20], Step [230/938], Reconstruction Loss: 5271.6650, KL Divergence: 1559.4415\n",
            "Epoch[5/20], Step [240/938], Reconstruction Loss: 5187.6899, KL Divergence: 1564.2307\n",
            "Epoch[5/20], Step [250/938], Reconstruction Loss: 5391.5635, KL Divergence: 1624.4321\n",
            "Epoch[5/20], Step [260/938], Reconstruction Loss: 5167.5786, KL Divergence: 1667.0610\n",
            "Epoch[5/20], Step [270/938], Reconstruction Loss: 5199.5190, KL Divergence: 1600.8379\n",
            "Epoch[5/20], Step [280/938], Reconstruction Loss: 5093.5225, KL Divergence: 1607.9880\n",
            "Epoch[5/20], Step [290/938], Reconstruction Loss: 5394.9009, KL Divergence: 1658.7052\n",
            "Epoch[5/20], Step [300/938], Reconstruction Loss: 5280.5151, KL Divergence: 1670.8043\n",
            "Epoch[5/20], Step [310/938], Reconstruction Loss: 5176.4951, KL Divergence: 1603.3440\n",
            "Epoch[5/20], Step [320/938], Reconstruction Loss: 5244.8506, KL Divergence: 1503.0524\n",
            "Epoch[5/20], Step [330/938], Reconstruction Loss: 5012.3501, KL Divergence: 1666.2167\n",
            "Epoch[5/20], Step [340/938], Reconstruction Loss: 5206.1758, KL Divergence: 1574.2880\n",
            "Epoch[5/20], Step [350/938], Reconstruction Loss: 5571.2573, KL Divergence: 1639.3680\n",
            "Epoch[5/20], Step [360/938], Reconstruction Loss: 5371.0630, KL Divergence: 1625.1323\n",
            "Epoch[5/20], Step [370/938], Reconstruction Loss: 5092.3740, KL Divergence: 1535.5417\n",
            "Epoch[5/20], Step [380/938], Reconstruction Loss: 5630.2158, KL Divergence: 1710.7942\n",
            "Epoch[5/20], Step [390/938], Reconstruction Loss: 5658.5625, KL Divergence: 1674.3441\n",
            "Epoch[5/20], Step [400/938], Reconstruction Loss: 5250.8613, KL Divergence: 1614.5355\n",
            "Epoch[5/20], Step [410/938], Reconstruction Loss: 5572.3496, KL Divergence: 1561.4330\n",
            "Epoch[5/20], Step [420/938], Reconstruction Loss: 5237.4546, KL Divergence: 1646.6272\n",
            "Epoch[5/20], Step [430/938], Reconstruction Loss: 5755.6646, KL Divergence: 1588.8496\n",
            "Epoch[5/20], Step [440/938], Reconstruction Loss: 5624.7661, KL Divergence: 1654.3840\n",
            "Epoch[5/20], Step [450/938], Reconstruction Loss: 5046.2231, KL Divergence: 1562.1088\n",
            "Epoch[5/20], Step [460/938], Reconstruction Loss: 5581.5068, KL Divergence: 1749.3257\n",
            "Epoch[5/20], Step [470/938], Reconstruction Loss: 5032.9790, KL Divergence: 1578.3228\n",
            "Epoch[5/20], Step [480/938], Reconstruction Loss: 5451.6445, KL Divergence: 1654.5985\n",
            "Epoch[5/20], Step [490/938], Reconstruction Loss: 4932.0415, KL Divergence: 1553.4381\n",
            "Epoch[5/20], Step [500/938], Reconstruction Loss: 5672.7222, KL Divergence: 1609.0040\n",
            "Epoch[5/20], Step [510/938], Reconstruction Loss: 5218.4517, KL Divergence: 1634.6997\n",
            "Epoch[5/20], Step [520/938], Reconstruction Loss: 5291.1812, KL Divergence: 1634.8893\n",
            "Epoch[5/20], Step [530/938], Reconstruction Loss: 4930.8857, KL Divergence: 1542.7728\n",
            "Epoch[5/20], Step [540/938], Reconstruction Loss: 5273.2192, KL Divergence: 1590.0485\n",
            "Epoch[5/20], Step [550/938], Reconstruction Loss: 5489.3716, KL Divergence: 1628.0061\n",
            "Epoch[5/20], Step [560/938], Reconstruction Loss: 5300.7310, KL Divergence: 1579.4652\n",
            "Epoch[5/20], Step [570/938], Reconstruction Loss: 5589.4492, KL Divergence: 1674.0057\n",
            "Epoch[5/20], Step [580/938], Reconstruction Loss: 5213.7739, KL Divergence: 1570.7423\n",
            "Epoch[5/20], Step [590/938], Reconstruction Loss: 5166.1558, KL Divergence: 1617.5833\n",
            "Epoch[5/20], Step [600/938], Reconstruction Loss: 5337.6167, KL Divergence: 1636.3293\n",
            "Epoch[5/20], Step [610/938], Reconstruction Loss: 5113.3608, KL Divergence: 1646.7195\n",
            "Epoch[5/20], Step [620/938], Reconstruction Loss: 5644.5972, KL Divergence: 1583.9746\n",
            "Epoch[5/20], Step [630/938], Reconstruction Loss: 5150.7432, KL Divergence: 1632.2153\n",
            "Epoch[5/20], Step [640/938], Reconstruction Loss: 5237.9565, KL Divergence: 1573.9185\n",
            "Epoch[5/20], Step [650/938], Reconstruction Loss: 4949.6426, KL Divergence: 1558.0909\n",
            "Epoch[5/20], Step [660/938], Reconstruction Loss: 5116.3286, KL Divergence: 1572.4456\n",
            "Epoch[5/20], Step [670/938], Reconstruction Loss: 5109.8862, KL Divergence: 1612.8384\n",
            "Epoch[5/20], Step [680/938], Reconstruction Loss: 5337.5713, KL Divergence: 1574.8430\n",
            "Epoch[5/20], Step [690/938], Reconstruction Loss: 5504.1172, KL Divergence: 1583.3969\n",
            "Epoch[5/20], Step [700/938], Reconstruction Loss: 5261.7881, KL Divergence: 1589.3632\n",
            "Epoch[5/20], Step [710/938], Reconstruction Loss: 5224.1094, KL Divergence: 1614.8671\n",
            "Epoch[5/20], Step [720/938], Reconstruction Loss: 5703.0103, KL Divergence: 1665.2042\n",
            "Epoch[5/20], Step [730/938], Reconstruction Loss: 5067.2466, KL Divergence: 1588.1794\n",
            "Epoch[5/20], Step [740/938], Reconstruction Loss: 5300.5190, KL Divergence: 1640.9763\n",
            "Epoch[5/20], Step [750/938], Reconstruction Loss: 5335.3652, KL Divergence: 1653.6066\n",
            "Epoch[5/20], Step [760/938], Reconstruction Loss: 5115.3281, KL Divergence: 1556.7795\n",
            "Epoch[5/20], Step [770/938], Reconstruction Loss: 5092.5181, KL Divergence: 1588.1993\n",
            "Epoch[5/20], Step [780/938], Reconstruction Loss: 5177.6592, KL Divergence: 1627.2489\n",
            "Epoch[5/20], Step [790/938], Reconstruction Loss: 5491.7656, KL Divergence: 1651.1899\n",
            "Epoch[5/20], Step [800/938], Reconstruction Loss: 5449.2393, KL Divergence: 1635.2562\n",
            "Epoch[5/20], Step [810/938], Reconstruction Loss: 5352.9658, KL Divergence: 1582.6624\n",
            "Epoch[5/20], Step [820/938], Reconstruction Loss: 5098.7700, KL Divergence: 1556.9797\n",
            "Epoch[5/20], Step [830/938], Reconstruction Loss: 4980.3340, KL Divergence: 1668.0791\n",
            "Epoch[5/20], Step [840/938], Reconstruction Loss: 5265.7466, KL Divergence: 1567.4811\n",
            "Epoch[5/20], Step [850/938], Reconstruction Loss: 4974.8384, KL Divergence: 1658.0989\n",
            "Epoch[5/20], Step [860/938], Reconstruction Loss: 4844.5342, KL Divergence: 1539.3988\n",
            "Epoch[5/20], Step [870/938], Reconstruction Loss: 5053.8467, KL Divergence: 1553.8191\n",
            "Epoch[5/20], Step [880/938], Reconstruction Loss: 5373.3066, KL Divergence: 1660.3575\n",
            "Epoch[5/20], Step [890/938], Reconstruction Loss: 5026.1104, KL Divergence: 1569.6075\n",
            "Epoch[5/20], Step [900/938], Reconstruction Loss: 4822.5176, KL Divergence: 1532.3674\n",
            "Epoch[5/20], Step [910/938], Reconstruction Loss: 5299.6650, KL Divergence: 1586.7279\n",
            "Epoch[5/20], Step [920/938], Reconstruction Loss: 5424.5815, KL Divergence: 1637.6145\n",
            "Epoch[5/20], Step [930/938], Reconstruction Loss: 5424.2974, KL Divergence: 1689.4641\n",
            "Epoch[6/20], Step [10/938], Reconstruction Loss: 5043.9038, KL Divergence: 1613.9305\n",
            "Epoch[6/20], Step [20/938], Reconstruction Loss: 5321.0850, KL Divergence: 1637.6290\n",
            "Epoch[6/20], Step [30/938], Reconstruction Loss: 5086.8486, KL Divergence: 1594.7761\n",
            "Epoch[6/20], Step [40/938], Reconstruction Loss: 5153.7031, KL Divergence: 1588.1785\n",
            "Epoch[6/20], Step [50/938], Reconstruction Loss: 5125.4463, KL Divergence: 1589.0751\n",
            "Epoch[6/20], Step [60/938], Reconstruction Loss: 4944.9351, KL Divergence: 1572.0061\n",
            "Epoch[6/20], Step [70/938], Reconstruction Loss: 5329.9229, KL Divergence: 1654.5334\n",
            "Epoch[6/20], Step [80/938], Reconstruction Loss: 5313.5640, KL Divergence: 1649.2573\n",
            "Epoch[6/20], Step [90/938], Reconstruction Loss: 5152.2534, KL Divergence: 1627.6515\n",
            "Epoch[6/20], Step [100/938], Reconstruction Loss: 5319.0610, KL Divergence: 1627.5848\n",
            "Epoch[6/20], Step [110/938], Reconstruction Loss: 4949.4976, KL Divergence: 1552.8853\n",
            "Epoch[6/20], Step [120/938], Reconstruction Loss: 5509.5967, KL Divergence: 1666.3530\n",
            "Epoch[6/20], Step [130/938], Reconstruction Loss: 5514.6006, KL Divergence: 1576.2476\n",
            "Epoch[6/20], Step [140/938], Reconstruction Loss: 4929.2998, KL Divergence: 1581.7582\n",
            "Epoch[6/20], Step [150/938], Reconstruction Loss: 5458.9106, KL Divergence: 1705.0200\n",
            "Epoch[6/20], Step [160/938], Reconstruction Loss: 5104.2603, KL Divergence: 1705.3759\n",
            "Epoch[6/20], Step [170/938], Reconstruction Loss: 5367.8809, KL Divergence: 1546.4895\n",
            "Epoch[6/20], Step [180/938], Reconstruction Loss: 5288.1318, KL Divergence: 1623.9089\n",
            "Epoch[6/20], Step [190/938], Reconstruction Loss: 5121.6709, KL Divergence: 1655.2473\n",
            "Epoch[6/20], Step [200/938], Reconstruction Loss: 5558.0293, KL Divergence: 1615.4106\n",
            "Epoch[6/20], Step [210/938], Reconstruction Loss: 5046.4429, KL Divergence: 1583.1233\n",
            "Epoch[6/20], Step [220/938], Reconstruction Loss: 5340.1572, KL Divergence: 1644.5800\n",
            "Epoch[6/20], Step [230/938], Reconstruction Loss: 5371.8242, KL Divergence: 1606.8668\n",
            "Epoch[6/20], Step [240/938], Reconstruction Loss: 5323.6172, KL Divergence: 1620.1375\n",
            "Epoch[6/20], Step [250/938], Reconstruction Loss: 5138.0542, KL Divergence: 1583.7626\n",
            "Epoch[6/20], Step [260/938], Reconstruction Loss: 5478.3799, KL Divergence: 1665.6963\n",
            "Epoch[6/20], Step [270/938], Reconstruction Loss: 5274.9727, KL Divergence: 1567.7068\n",
            "Epoch[6/20], Step [280/938], Reconstruction Loss: 5313.4165, KL Divergence: 1651.3412\n",
            "Epoch[6/20], Step [290/938], Reconstruction Loss: 5326.3398, KL Divergence: 1625.2402\n",
            "Epoch[6/20], Step [300/938], Reconstruction Loss: 5512.9995, KL Divergence: 1599.9540\n",
            "Epoch[6/20], Step [310/938], Reconstruction Loss: 5443.6660, KL Divergence: 1612.4181\n",
            "Epoch[6/20], Step [320/938], Reconstruction Loss: 5003.5806, KL Divergence: 1524.1840\n",
            "Epoch[6/20], Step [330/938], Reconstruction Loss: 5258.0898, KL Divergence: 1629.1907\n",
            "Epoch[6/20], Step [340/938], Reconstruction Loss: 5278.4150, KL Divergence: 1671.7780\n",
            "Epoch[6/20], Step [350/938], Reconstruction Loss: 5178.4189, KL Divergence: 1542.7992\n",
            "Epoch[6/20], Step [360/938], Reconstruction Loss: 5310.8228, KL Divergence: 1666.0414\n",
            "Epoch[6/20], Step [370/938], Reconstruction Loss: 5016.7314, KL Divergence: 1585.9734\n",
            "Epoch[6/20], Step [380/938], Reconstruction Loss: 4847.4316, KL Divergence: 1553.8898\n",
            "Epoch[6/20], Step [390/938], Reconstruction Loss: 5125.3188, KL Divergence: 1611.9790\n",
            "Epoch[6/20], Step [400/938], Reconstruction Loss: 5116.1914, KL Divergence: 1569.4418\n",
            "Epoch[6/20], Step [410/938], Reconstruction Loss: 5495.7622, KL Divergence: 1595.2638\n",
            "Epoch[6/20], Step [420/938], Reconstruction Loss: 5501.7759, KL Divergence: 1630.4260\n",
            "Epoch[6/20], Step [430/938], Reconstruction Loss: 5349.0537, KL Divergence: 1626.4083\n",
            "Epoch[6/20], Step [440/938], Reconstruction Loss: 5537.8389, KL Divergence: 1612.3336\n",
            "Epoch[6/20], Step [450/938], Reconstruction Loss: 5438.9238, KL Divergence: 1567.0038\n",
            "Epoch[6/20], Step [460/938], Reconstruction Loss: 5128.8223, KL Divergence: 1688.4042\n",
            "Epoch[6/20], Step [470/938], Reconstruction Loss: 5042.0039, KL Divergence: 1561.2007\n",
            "Epoch[6/20], Step [480/938], Reconstruction Loss: 5075.0659, KL Divergence: 1623.1837\n",
            "Epoch[6/20], Step [490/938], Reconstruction Loss: 5112.1797, KL Divergence: 1605.9822\n",
            "Epoch[6/20], Step [500/938], Reconstruction Loss: 5433.6460, KL Divergence: 1578.6398\n",
            "Epoch[6/20], Step [510/938], Reconstruction Loss: 5217.0698, KL Divergence: 1588.4917\n",
            "Epoch[6/20], Step [520/938], Reconstruction Loss: 4984.6724, KL Divergence: 1583.2163\n",
            "Epoch[6/20], Step [530/938], Reconstruction Loss: 5678.2915, KL Divergence: 1671.7455\n",
            "Epoch[6/20], Step [540/938], Reconstruction Loss: 5274.4443, KL Divergence: 1632.6871\n",
            "Epoch[6/20], Step [550/938], Reconstruction Loss: 5239.9751, KL Divergence: 1565.1177\n",
            "Epoch[6/20], Step [560/938], Reconstruction Loss: 5267.2222, KL Divergence: 1663.3505\n",
            "Epoch[6/20], Step [570/938], Reconstruction Loss: 5587.6089, KL Divergence: 1646.6560\n",
            "Epoch[6/20], Step [580/938], Reconstruction Loss: 4812.2007, KL Divergence: 1576.6931\n",
            "Epoch[6/20], Step [590/938], Reconstruction Loss: 5089.5806, KL Divergence: 1600.9293\n",
            "Epoch[6/20], Step [600/938], Reconstruction Loss: 5269.8877, KL Divergence: 1648.1714\n",
            "Epoch[6/20], Step [610/938], Reconstruction Loss: 5094.8501, KL Divergence: 1616.1218\n",
            "Epoch[6/20], Step [620/938], Reconstruction Loss: 5520.5806, KL Divergence: 1686.0298\n",
            "Epoch[6/20], Step [630/938], Reconstruction Loss: 5113.0220, KL Divergence: 1614.0050\n",
            "Epoch[6/20], Step [640/938], Reconstruction Loss: 5385.7812, KL Divergence: 1647.8971\n",
            "Epoch[6/20], Step [650/938], Reconstruction Loss: 5741.8940, KL Divergence: 1649.4609\n",
            "Epoch[6/20], Step [660/938], Reconstruction Loss: 5111.3110, KL Divergence: 1666.8943\n",
            "Epoch[6/20], Step [670/938], Reconstruction Loss: 5240.8853, KL Divergence: 1607.7382\n",
            "Epoch[6/20], Step [680/938], Reconstruction Loss: 5348.6860, KL Divergence: 1591.9878\n",
            "Epoch[6/20], Step [690/938], Reconstruction Loss: 5092.5645, KL Divergence: 1613.7185\n",
            "Epoch[6/20], Step [700/938], Reconstruction Loss: 5548.6572, KL Divergence: 1609.4750\n",
            "Epoch[6/20], Step [710/938], Reconstruction Loss: 5375.3286, KL Divergence: 1608.4521\n",
            "Epoch[6/20], Step [720/938], Reconstruction Loss: 5110.7202, KL Divergence: 1633.4061\n",
            "Epoch[6/20], Step [730/938], Reconstruction Loss: 5264.4370, KL Divergence: 1588.9054\n",
            "Epoch[6/20], Step [740/938], Reconstruction Loss: 5070.4204, KL Divergence: 1632.0260\n",
            "Epoch[6/20], Step [750/938], Reconstruction Loss: 5109.0205, KL Divergence: 1653.3561\n",
            "Epoch[6/20], Step [760/938], Reconstruction Loss: 5187.8262, KL Divergence: 1594.4871\n",
            "Epoch[6/20], Step [770/938], Reconstruction Loss: 5056.4585, KL Divergence: 1630.3879\n",
            "Epoch[6/20], Step [780/938], Reconstruction Loss: 5311.8452, KL Divergence: 1553.3337\n",
            "Epoch[6/20], Step [790/938], Reconstruction Loss: 5366.7397, KL Divergence: 1616.3339\n",
            "Epoch[6/20], Step [800/938], Reconstruction Loss: 5194.2739, KL Divergence: 1595.9652\n",
            "Epoch[6/20], Step [810/938], Reconstruction Loss: 5221.7451, KL Divergence: 1605.8846\n",
            "Epoch[6/20], Step [820/938], Reconstruction Loss: 5574.4390, KL Divergence: 1770.0188\n",
            "Epoch[6/20], Step [830/938], Reconstruction Loss: 5031.0264, KL Divergence: 1528.3402\n",
            "Epoch[6/20], Step [840/938], Reconstruction Loss: 4979.5854, KL Divergence: 1594.0084\n",
            "Epoch[6/20], Step [850/938], Reconstruction Loss: 5281.4229, KL Divergence: 1618.2316\n",
            "Epoch[6/20], Step [860/938], Reconstruction Loss: 5435.9595, KL Divergence: 1714.0197\n",
            "Epoch[6/20], Step [870/938], Reconstruction Loss: 5101.1152, KL Divergence: 1569.4250\n",
            "Epoch[6/20], Step [880/938], Reconstruction Loss: 5248.3130, KL Divergence: 1616.4647\n",
            "Epoch[6/20], Step [890/938], Reconstruction Loss: 5160.2744, KL Divergence: 1581.4851\n",
            "Epoch[6/20], Step [900/938], Reconstruction Loss: 5486.4609, KL Divergence: 1674.2704\n",
            "Epoch[6/20], Step [910/938], Reconstruction Loss: 5351.4077, KL Divergence: 1618.4384\n",
            "Epoch[6/20], Step [920/938], Reconstruction Loss: 5418.1558, KL Divergence: 1679.0530\n",
            "Epoch[6/20], Step [930/938], Reconstruction Loss: 5309.6260, KL Divergence: 1567.3407\n",
            "Epoch[7/20], Step [10/938], Reconstruction Loss: 4942.9077, KL Divergence: 1631.1903\n",
            "Epoch[7/20], Step [20/938], Reconstruction Loss: 5378.4478, KL Divergence: 1675.1998\n",
            "Epoch[7/20], Step [30/938], Reconstruction Loss: 5165.4043, KL Divergence: 1683.6713\n",
            "Epoch[7/20], Step [40/938], Reconstruction Loss: 5599.9980, KL Divergence: 1609.9033\n",
            "Epoch[7/20], Step [50/938], Reconstruction Loss: 5040.6240, KL Divergence: 1599.2308\n",
            "Epoch[7/20], Step [60/938], Reconstruction Loss: 5260.1353, KL Divergence: 1667.7943\n",
            "Epoch[7/20], Step [70/938], Reconstruction Loss: 5069.3794, KL Divergence: 1650.1989\n",
            "Epoch[7/20], Step [80/938], Reconstruction Loss: 4992.6665, KL Divergence: 1577.8975\n",
            "Epoch[7/20], Step [90/938], Reconstruction Loss: 5208.4858, KL Divergence: 1684.9224\n",
            "Epoch[7/20], Step [100/938], Reconstruction Loss: 5124.9829, KL Divergence: 1628.6819\n",
            "Epoch[7/20], Step [110/938], Reconstruction Loss: 5545.7979, KL Divergence: 1660.4265\n",
            "Epoch[7/20], Step [120/938], Reconstruction Loss: 4992.5254, KL Divergence: 1652.7170\n",
            "Epoch[7/20], Step [130/938], Reconstruction Loss: 5453.0801, KL Divergence: 1637.9705\n",
            "Epoch[7/20], Step [140/938], Reconstruction Loss: 4832.3203, KL Divergence: 1613.7615\n",
            "Epoch[7/20], Step [150/938], Reconstruction Loss: 5188.4214, KL Divergence: 1711.8042\n",
            "Epoch[7/20], Step [160/938], Reconstruction Loss: 5081.3105, KL Divergence: 1598.1311\n",
            "Epoch[7/20], Step [170/938], Reconstruction Loss: 5370.0132, KL Divergence: 1691.2675\n",
            "Epoch[7/20], Step [180/938], Reconstruction Loss: 5311.9570, KL Divergence: 1625.2223\n",
            "Epoch[7/20], Step [190/938], Reconstruction Loss: 4838.0952, KL Divergence: 1555.0005\n",
            "Epoch[7/20], Step [200/938], Reconstruction Loss: 5404.2573, KL Divergence: 1694.3926\n",
            "Epoch[7/20], Step [210/938], Reconstruction Loss: 5350.8091, KL Divergence: 1604.3398\n",
            "Epoch[7/20], Step [220/938], Reconstruction Loss: 5114.3857, KL Divergence: 1698.2703\n",
            "Epoch[7/20], Step [230/938], Reconstruction Loss: 5435.3311, KL Divergence: 1632.8901\n",
            "Epoch[7/20], Step [240/938], Reconstruction Loss: 5351.9644, KL Divergence: 1655.2668\n",
            "Epoch[7/20], Step [250/938], Reconstruction Loss: 5402.4951, KL Divergence: 1691.2478\n",
            "Epoch[7/20], Step [260/938], Reconstruction Loss: 4935.9702, KL Divergence: 1589.1804\n",
            "Epoch[7/20], Step [270/938], Reconstruction Loss: 5430.5688, KL Divergence: 1641.6565\n",
            "Epoch[7/20], Step [280/938], Reconstruction Loss: 5168.3853, KL Divergence: 1605.2385\n",
            "Epoch[7/20], Step [290/938], Reconstruction Loss: 5388.1973, KL Divergence: 1625.9181\n",
            "Epoch[7/20], Step [300/938], Reconstruction Loss: 5304.4766, KL Divergence: 1633.0530\n",
            "Epoch[7/20], Step [310/938], Reconstruction Loss: 5262.9937, KL Divergence: 1576.0032\n",
            "Epoch[7/20], Step [320/938], Reconstruction Loss: 5312.4585, KL Divergence: 1661.0315\n",
            "Epoch[7/20], Step [330/938], Reconstruction Loss: 5311.8477, KL Divergence: 1545.3429\n",
            "Epoch[7/20], Step [340/938], Reconstruction Loss: 5498.2007, KL Divergence: 1649.6934\n",
            "Epoch[7/20], Step [350/938], Reconstruction Loss: 4631.0171, KL Divergence: 1601.2371\n",
            "Epoch[7/20], Step [360/938], Reconstruction Loss: 5088.5693, KL Divergence: 1623.2992\n",
            "Epoch[7/20], Step [370/938], Reconstruction Loss: 4894.8018, KL Divergence: 1568.9069\n",
            "Epoch[7/20], Step [380/938], Reconstruction Loss: 5364.5674, KL Divergence: 1576.3920\n",
            "Epoch[7/20], Step [390/938], Reconstruction Loss: 5189.6025, KL Divergence: 1591.0808\n",
            "Epoch[7/20], Step [400/938], Reconstruction Loss: 5101.6772, KL Divergence: 1620.0164\n",
            "Epoch[7/20], Step [410/938], Reconstruction Loss: 5583.7876, KL Divergence: 1612.5502\n",
            "Epoch[7/20], Step [420/938], Reconstruction Loss: 4985.3770, KL Divergence: 1613.8754\n",
            "Epoch[7/20], Step [430/938], Reconstruction Loss: 4844.0249, KL Divergence: 1553.3828\n",
            "Epoch[7/20], Step [440/938], Reconstruction Loss: 5194.7163, KL Divergence: 1643.7629\n",
            "Epoch[7/20], Step [450/938], Reconstruction Loss: 5245.0659, KL Divergence: 1648.4961\n",
            "Epoch[7/20], Step [460/938], Reconstruction Loss: 5571.3013, KL Divergence: 1619.2462\n",
            "Epoch[7/20], Step [470/938], Reconstruction Loss: 5239.7222, KL Divergence: 1598.6003\n",
            "Epoch[7/20], Step [480/938], Reconstruction Loss: 5151.3164, KL Divergence: 1681.7316\n",
            "Epoch[7/20], Step [490/938], Reconstruction Loss: 5071.9321, KL Divergence: 1547.8140\n",
            "Epoch[7/20], Step [500/938], Reconstruction Loss: 5255.3081, KL Divergence: 1656.0504\n",
            "Epoch[7/20], Step [510/938], Reconstruction Loss: 5420.9268, KL Divergence: 1688.4302\n",
            "Epoch[7/20], Step [520/938], Reconstruction Loss: 5232.2773, KL Divergence: 1584.5570\n",
            "Epoch[7/20], Step [530/938], Reconstruction Loss: 5469.6987, KL Divergence: 1667.9811\n",
            "Epoch[7/20], Step [540/938], Reconstruction Loss: 5235.2134, KL Divergence: 1622.2794\n",
            "Epoch[7/20], Step [550/938], Reconstruction Loss: 5245.2339, KL Divergence: 1509.7522\n",
            "Epoch[7/20], Step [560/938], Reconstruction Loss: 5402.2124, KL Divergence: 1696.4867\n",
            "Epoch[7/20], Step [570/938], Reconstruction Loss: 5345.7295, KL Divergence: 1580.0901\n",
            "Epoch[7/20], Step [580/938], Reconstruction Loss: 5207.4180, KL Divergence: 1621.6754\n",
            "Epoch[7/20], Step [590/938], Reconstruction Loss: 4989.0845, KL Divergence: 1618.1642\n",
            "Epoch[7/20], Step [600/938], Reconstruction Loss: 5279.0010, KL Divergence: 1624.5817\n",
            "Epoch[7/20], Step [610/938], Reconstruction Loss: 5177.9385, KL Divergence: 1661.6360\n",
            "Epoch[7/20], Step [620/938], Reconstruction Loss: 5281.5078, KL Divergence: 1545.3368\n",
            "Epoch[7/20], Step [630/938], Reconstruction Loss: 5505.6724, KL Divergence: 1633.2260\n",
            "Epoch[7/20], Step [640/938], Reconstruction Loss: 5470.0649, KL Divergence: 1659.9088\n",
            "Epoch[7/20], Step [650/938], Reconstruction Loss: 5323.2837, KL Divergence: 1654.7911\n",
            "Epoch[7/20], Step [660/938], Reconstruction Loss: 5419.3042, KL Divergence: 1698.1847\n",
            "Epoch[7/20], Step [670/938], Reconstruction Loss: 5146.1978, KL Divergence: 1605.5439\n",
            "Epoch[7/20], Step [680/938], Reconstruction Loss: 5241.0850, KL Divergence: 1678.8098\n",
            "Epoch[7/20], Step [690/938], Reconstruction Loss: 5454.6016, KL Divergence: 1679.1680\n",
            "Epoch[7/20], Step [700/938], Reconstruction Loss: 5077.6675, KL Divergence: 1561.3914\n",
            "Epoch[7/20], Step [710/938], Reconstruction Loss: 5386.7485, KL Divergence: 1623.8254\n",
            "Epoch[7/20], Step [720/938], Reconstruction Loss: 4940.9263, KL Divergence: 1572.2826\n",
            "Epoch[7/20], Step [730/938], Reconstruction Loss: 5151.8076, KL Divergence: 1645.7638\n",
            "Epoch[7/20], Step [740/938], Reconstruction Loss: 5391.9888, KL Divergence: 1682.0500\n",
            "Epoch[7/20], Step [750/938], Reconstruction Loss: 4962.9023, KL Divergence: 1568.5742\n",
            "Epoch[7/20], Step [760/938], Reconstruction Loss: 5063.7490, KL Divergence: 1617.3127\n",
            "Epoch[7/20], Step [770/938], Reconstruction Loss: 5319.4399, KL Divergence: 1605.9548\n",
            "Epoch[7/20], Step [780/938], Reconstruction Loss: 5259.3418, KL Divergence: 1674.5317\n",
            "Epoch[7/20], Step [790/938], Reconstruction Loss: 4918.8818, KL Divergence: 1575.8406\n",
            "Epoch[7/20], Step [800/938], Reconstruction Loss: 4912.4604, KL Divergence: 1584.4712\n",
            "Epoch[7/20], Step [810/938], Reconstruction Loss: 5183.4766, KL Divergence: 1535.3385\n",
            "Epoch[7/20], Step [820/938], Reconstruction Loss: 4987.3516, KL Divergence: 1618.3711\n",
            "Epoch[7/20], Step [830/938], Reconstruction Loss: 5236.5996, KL Divergence: 1591.4929\n",
            "Epoch[7/20], Step [840/938], Reconstruction Loss: 5455.2075, KL Divergence: 1666.1996\n",
            "Epoch[7/20], Step [850/938], Reconstruction Loss: 5174.9844, KL Divergence: 1626.2783\n",
            "Epoch[7/20], Step [860/938], Reconstruction Loss: 5245.7988, KL Divergence: 1602.0442\n",
            "Epoch[7/20], Step [870/938], Reconstruction Loss: 5282.6914, KL Divergence: 1714.2098\n",
            "Epoch[7/20], Step [880/938], Reconstruction Loss: 5486.0317, KL Divergence: 1610.1282\n",
            "Epoch[7/20], Step [890/938], Reconstruction Loss: 5117.5688, KL Divergence: 1666.0546\n",
            "Epoch[7/20], Step [900/938], Reconstruction Loss: 4987.0723, KL Divergence: 1556.7366\n",
            "Epoch[7/20], Step [910/938], Reconstruction Loss: 5385.9854, KL Divergence: 1624.3761\n",
            "Epoch[7/20], Step [920/938], Reconstruction Loss: 5544.0635, KL Divergence: 1683.4484\n",
            "Epoch[7/20], Step [930/938], Reconstruction Loss: 5405.1489, KL Divergence: 1660.8137\n",
            "Epoch[8/20], Step [10/938], Reconstruction Loss: 5045.0645, KL Divergence: 1625.1321\n",
            "Epoch[8/20], Step [20/938], Reconstruction Loss: 4845.1118, KL Divergence: 1557.5872\n",
            "Epoch[8/20], Step [30/938], Reconstruction Loss: 5198.2451, KL Divergence: 1621.3755\n",
            "Epoch[8/20], Step [40/938], Reconstruction Loss: 5126.0723, KL Divergence: 1600.5846\n",
            "Epoch[8/20], Step [50/938], Reconstruction Loss: 4967.9121, KL Divergence: 1613.1438\n",
            "Epoch[8/20], Step [60/938], Reconstruction Loss: 5547.0776, KL Divergence: 1703.9271\n",
            "Epoch[8/20], Step [70/938], Reconstruction Loss: 5235.4077, KL Divergence: 1658.5634\n",
            "Epoch[8/20], Step [80/938], Reconstruction Loss: 5207.7720, KL Divergence: 1607.9569\n",
            "Epoch[8/20], Step [90/938], Reconstruction Loss: 5434.3916, KL Divergence: 1682.1807\n",
            "Epoch[8/20], Step [100/938], Reconstruction Loss: 5348.4199, KL Divergence: 1648.9374\n",
            "Epoch[8/20], Step [110/938], Reconstruction Loss: 5266.9570, KL Divergence: 1605.3951\n",
            "Epoch[8/20], Step [120/938], Reconstruction Loss: 5115.8984, KL Divergence: 1684.3362\n",
            "Epoch[8/20], Step [130/938], Reconstruction Loss: 5461.0850, KL Divergence: 1674.7697\n",
            "Epoch[8/20], Step [140/938], Reconstruction Loss: 5059.0635, KL Divergence: 1664.9332\n",
            "Epoch[8/20], Step [150/938], Reconstruction Loss: 5084.8452, KL Divergence: 1603.4438\n",
            "Epoch[8/20], Step [160/938], Reconstruction Loss: 5164.9712, KL Divergence: 1689.1742\n",
            "Epoch[8/20], Step [170/938], Reconstruction Loss: 5001.5088, KL Divergence: 1521.6960\n",
            "Epoch[8/20], Step [180/938], Reconstruction Loss: 5211.6367, KL Divergence: 1611.8950\n",
            "Epoch[8/20], Step [190/938], Reconstruction Loss: 5146.2554, KL Divergence: 1649.9102\n",
            "Epoch[8/20], Step [200/938], Reconstruction Loss: 5211.8823, KL Divergence: 1662.0791\n",
            "Epoch[8/20], Step [210/938], Reconstruction Loss: 5244.5547, KL Divergence: 1652.6777\n",
            "Epoch[8/20], Step [220/938], Reconstruction Loss: 5195.8643, KL Divergence: 1665.3403\n",
            "Epoch[8/20], Step [230/938], Reconstruction Loss: 5024.3921, KL Divergence: 1584.8993\n",
            "Epoch[8/20], Step [240/938], Reconstruction Loss: 5385.9155, KL Divergence: 1608.4694\n",
            "Epoch[8/20], Step [250/938], Reconstruction Loss: 5246.9360, KL Divergence: 1605.9790\n",
            "Epoch[8/20], Step [260/938], Reconstruction Loss: 5238.3706, KL Divergence: 1574.7379\n",
            "Epoch[8/20], Step [270/938], Reconstruction Loss: 5186.2471, KL Divergence: 1626.7362\n",
            "Epoch[8/20], Step [280/938], Reconstruction Loss: 5319.0752, KL Divergence: 1632.3401\n",
            "Epoch[8/20], Step [290/938], Reconstruction Loss: 4964.6025, KL Divergence: 1565.3314\n",
            "Epoch[8/20], Step [300/938], Reconstruction Loss: 5283.8853, KL Divergence: 1641.2959\n",
            "Epoch[8/20], Step [310/938], Reconstruction Loss: 5169.8984, KL Divergence: 1640.9022\n",
            "Epoch[8/20], Step [320/938], Reconstruction Loss: 5214.9438, KL Divergence: 1623.7371\n",
            "Epoch[8/20], Step [330/938], Reconstruction Loss: 5236.0366, KL Divergence: 1625.4725\n",
            "Epoch[8/20], Step [340/938], Reconstruction Loss: 5163.5918, KL Divergence: 1644.7532\n",
            "Epoch[8/20], Step [350/938], Reconstruction Loss: 4912.4370, KL Divergence: 1623.9690\n",
            "Epoch[8/20], Step [360/938], Reconstruction Loss: 4883.7158, KL Divergence: 1642.7140\n",
            "Epoch[8/20], Step [370/938], Reconstruction Loss: 5138.8057, KL Divergence: 1654.6483\n",
            "Epoch[8/20], Step [380/938], Reconstruction Loss: 5271.8760, KL Divergence: 1641.1309\n",
            "Epoch[8/20], Step [390/938], Reconstruction Loss: 5498.8008, KL Divergence: 1643.2761\n",
            "Epoch[8/20], Step [400/938], Reconstruction Loss: 4974.1826, KL Divergence: 1650.3357\n",
            "Epoch[8/20], Step [410/938], Reconstruction Loss: 5031.4795, KL Divergence: 1659.2397\n",
            "Epoch[8/20], Step [420/938], Reconstruction Loss: 5066.7314, KL Divergence: 1627.5669\n",
            "Epoch[8/20], Step [430/938], Reconstruction Loss: 4815.2158, KL Divergence: 1569.5616\n",
            "Epoch[8/20], Step [440/938], Reconstruction Loss: 5020.2598, KL Divergence: 1633.6777\n",
            "Epoch[8/20], Step [450/938], Reconstruction Loss: 4876.7720, KL Divergence: 1542.5594\n",
            "Epoch[8/20], Step [460/938], Reconstruction Loss: 4810.3096, KL Divergence: 1608.7225\n",
            "Epoch[8/20], Step [470/938], Reconstruction Loss: 5028.7266, KL Divergence: 1593.5006\n",
            "Epoch[8/20], Step [480/938], Reconstruction Loss: 5017.9976, KL Divergence: 1607.5963\n",
            "Epoch[8/20], Step [490/938], Reconstruction Loss: 5389.8809, KL Divergence: 1636.4535\n",
            "Epoch[8/20], Step [500/938], Reconstruction Loss: 4973.7690, KL Divergence: 1604.8385\n",
            "Epoch[8/20], Step [510/938], Reconstruction Loss: 5269.9062, KL Divergence: 1632.8075\n",
            "Epoch[8/20], Step [520/938], Reconstruction Loss: 5283.1655, KL Divergence: 1609.7064\n",
            "Epoch[8/20], Step [530/938], Reconstruction Loss: 5174.3608, KL Divergence: 1627.0090\n",
            "Epoch[8/20], Step [540/938], Reconstruction Loss: 4966.6167, KL Divergence: 1640.3590\n",
            "Epoch[8/20], Step [550/938], Reconstruction Loss: 5252.5566, KL Divergence: 1599.6813\n",
            "Epoch[8/20], Step [560/938], Reconstruction Loss: 4459.1519, KL Divergence: 1574.8408\n",
            "Epoch[8/20], Step [570/938], Reconstruction Loss: 5245.7256, KL Divergence: 1598.7924\n",
            "Epoch[8/20], Step [580/938], Reconstruction Loss: 5448.8145, KL Divergence: 1703.7479\n",
            "Epoch[8/20], Step [590/938], Reconstruction Loss: 5165.1592, KL Divergence: 1600.9172\n",
            "Epoch[8/20], Step [600/938], Reconstruction Loss: 4984.7427, KL Divergence: 1627.8375\n",
            "Epoch[8/20], Step [610/938], Reconstruction Loss: 5474.9502, KL Divergence: 1620.4904\n",
            "Epoch[8/20], Step [620/938], Reconstruction Loss: 4877.0835, KL Divergence: 1616.0400\n",
            "Epoch[8/20], Step [630/938], Reconstruction Loss: 5056.1416, KL Divergence: 1588.9913\n",
            "Epoch[8/20], Step [640/938], Reconstruction Loss: 4819.4321, KL Divergence: 1628.4492\n",
            "Epoch[8/20], Step [650/938], Reconstruction Loss: 5362.5605, KL Divergence: 1617.2241\n",
            "Epoch[8/20], Step [660/938], Reconstruction Loss: 5146.4199, KL Divergence: 1558.5059\n",
            "Epoch[8/20], Step [670/938], Reconstruction Loss: 5105.6689, KL Divergence: 1619.7877\n",
            "Epoch[8/20], Step [680/938], Reconstruction Loss: 5363.4678, KL Divergence: 1635.9611\n",
            "Epoch[8/20], Step [690/938], Reconstruction Loss: 4920.0308, KL Divergence: 1724.8772\n",
            "Epoch[8/20], Step [700/938], Reconstruction Loss: 5242.6973, KL Divergence: 1649.3933\n",
            "Epoch[8/20], Step [710/938], Reconstruction Loss: 4953.7651, KL Divergence: 1693.9417\n",
            "Epoch[8/20], Step [720/938], Reconstruction Loss: 5480.6377, KL Divergence: 1634.1187\n",
            "Epoch[8/20], Step [730/938], Reconstruction Loss: 5034.2949, KL Divergence: 1644.7111\n",
            "Epoch[8/20], Step [740/938], Reconstruction Loss: 4960.6597, KL Divergence: 1560.5989\n",
            "Epoch[8/20], Step [750/938], Reconstruction Loss: 5047.1729, KL Divergence: 1537.0131\n",
            "Epoch[8/20], Step [760/938], Reconstruction Loss: 5005.7109, KL Divergence: 1577.9204\n",
            "Epoch[8/20], Step [770/938], Reconstruction Loss: 5299.6870, KL Divergence: 1651.1676\n",
            "Epoch[8/20], Step [780/938], Reconstruction Loss: 5208.1040, KL Divergence: 1612.6593\n",
            "Epoch[8/20], Step [790/938], Reconstruction Loss: 5285.2969, KL Divergence: 1585.6362\n",
            "Epoch[8/20], Step [800/938], Reconstruction Loss: 5010.6055, KL Divergence: 1529.0150\n",
            "Epoch[8/20], Step [810/938], Reconstruction Loss: 5122.9873, KL Divergence: 1582.1335\n",
            "Epoch[8/20], Step [820/938], Reconstruction Loss: 4927.3599, KL Divergence: 1661.1876\n",
            "Epoch[8/20], Step [830/938], Reconstruction Loss: 5176.0249, KL Divergence: 1568.4238\n",
            "Epoch[8/20], Step [840/938], Reconstruction Loss: 5104.6113, KL Divergence: 1657.7343\n",
            "Epoch[8/20], Step [850/938], Reconstruction Loss: 4998.7661, KL Divergence: 1543.2079\n",
            "Epoch[8/20], Step [860/938], Reconstruction Loss: 5388.4805, KL Divergence: 1662.3070\n",
            "Epoch[8/20], Step [870/938], Reconstruction Loss: 5696.3433, KL Divergence: 1667.9504\n",
            "Epoch[8/20], Step [880/938], Reconstruction Loss: 4957.2754, KL Divergence: 1575.3263\n",
            "Epoch[8/20], Step [890/938], Reconstruction Loss: 5266.0913, KL Divergence: 1632.9515\n",
            "Epoch[8/20], Step [900/938], Reconstruction Loss: 5258.9116, KL Divergence: 1650.4388\n",
            "Epoch[8/20], Step [910/938], Reconstruction Loss: 5079.0845, KL Divergence: 1663.7673\n",
            "Epoch[8/20], Step [920/938], Reconstruction Loss: 5233.1899, KL Divergence: 1638.3453\n",
            "Epoch[8/20], Step [930/938], Reconstruction Loss: 5251.5518, KL Divergence: 1620.5500\n",
            "Epoch[9/20], Step [10/938], Reconstruction Loss: 4804.0024, KL Divergence: 1651.0547\n",
            "Epoch[9/20], Step [20/938], Reconstruction Loss: 5336.0767, KL Divergence: 1634.0734\n",
            "Epoch[9/20], Step [30/938], Reconstruction Loss: 5305.2583, KL Divergence: 1771.2682\n",
            "Epoch[9/20], Step [40/938], Reconstruction Loss: 5181.0386, KL Divergence: 1581.2621\n",
            "Epoch[9/20], Step [50/938], Reconstruction Loss: 4976.8062, KL Divergence: 1666.9044\n",
            "Epoch[9/20], Step [60/938], Reconstruction Loss: 5180.4819, KL Divergence: 1605.4802\n",
            "Epoch[9/20], Step [70/938], Reconstruction Loss: 5024.2754, KL Divergence: 1662.4724\n",
            "Epoch[9/20], Step [80/938], Reconstruction Loss: 5234.8027, KL Divergence: 1590.2186\n",
            "Epoch[9/20], Step [90/938], Reconstruction Loss: 5080.8184, KL Divergence: 1649.2377\n",
            "Epoch[9/20], Step [100/938], Reconstruction Loss: 4956.3389, KL Divergence: 1677.1442\n",
            "Epoch[9/20], Step [110/938], Reconstruction Loss: 5181.9717, KL Divergence: 1549.4675\n",
            "Epoch[9/20], Step [120/938], Reconstruction Loss: 5376.4658, KL Divergence: 1688.8741\n",
            "Epoch[9/20], Step [130/938], Reconstruction Loss: 5353.9297, KL Divergence: 1598.0990\n",
            "Epoch[9/20], Step [140/938], Reconstruction Loss: 5215.9487, KL Divergence: 1670.4368\n",
            "Epoch[9/20], Step [150/938], Reconstruction Loss: 5520.6992, KL Divergence: 1584.7334\n",
            "Epoch[9/20], Step [160/938], Reconstruction Loss: 5416.4863, KL Divergence: 1655.5548\n",
            "Epoch[9/20], Step [170/938], Reconstruction Loss: 5300.1577, KL Divergence: 1585.8492\n",
            "Epoch[9/20], Step [180/938], Reconstruction Loss: 5439.0225, KL Divergence: 1703.8638\n",
            "Epoch[9/20], Step [190/938], Reconstruction Loss: 5036.2739, KL Divergence: 1631.4283\n",
            "Epoch[9/20], Step [200/938], Reconstruction Loss: 5117.8462, KL Divergence: 1673.1622\n",
            "Epoch[9/20], Step [210/938], Reconstruction Loss: 5423.0547, KL Divergence: 1616.8817\n",
            "Epoch[9/20], Step [220/938], Reconstruction Loss: 5455.0264, KL Divergence: 1694.0873\n",
            "Epoch[9/20], Step [230/938], Reconstruction Loss: 5122.1455, KL Divergence: 1626.7919\n",
            "Epoch[9/20], Step [240/938], Reconstruction Loss: 4896.8530, KL Divergence: 1510.5208\n",
            "Epoch[9/20], Step [250/938], Reconstruction Loss: 4901.1538, KL Divergence: 1651.1383\n",
            "Epoch[9/20], Step [260/938], Reconstruction Loss: 5396.3750, KL Divergence: 1611.6924\n",
            "Epoch[9/20], Step [270/938], Reconstruction Loss: 5223.5845, KL Divergence: 1700.5321\n",
            "Epoch[9/20], Step [280/938], Reconstruction Loss: 5158.4438, KL Divergence: 1627.5774\n",
            "Epoch[9/20], Step [290/938], Reconstruction Loss: 5298.6934, KL Divergence: 1701.9288\n",
            "Epoch[9/20], Step [300/938], Reconstruction Loss: 5182.7725, KL Divergence: 1616.9409\n",
            "Epoch[9/20], Step [310/938], Reconstruction Loss: 4867.1689, KL Divergence: 1597.0312\n",
            "Epoch[9/20], Step [320/938], Reconstruction Loss: 5238.9209, KL Divergence: 1614.6179\n",
            "Epoch[9/20], Step [330/938], Reconstruction Loss: 5134.4458, KL Divergence: 1658.4629\n",
            "Epoch[9/20], Step [340/938], Reconstruction Loss: 5239.6821, KL Divergence: 1629.3009\n",
            "Epoch[9/20], Step [350/938], Reconstruction Loss: 5066.1880, KL Divergence: 1632.8046\n",
            "Epoch[9/20], Step [360/938], Reconstruction Loss: 5472.9419, KL Divergence: 1671.5984\n",
            "Epoch[9/20], Step [370/938], Reconstruction Loss: 5261.9458, KL Divergence: 1722.7828\n",
            "Epoch[9/20], Step [380/938], Reconstruction Loss: 5157.9751, KL Divergence: 1498.9590\n",
            "Epoch[9/20], Step [390/938], Reconstruction Loss: 5186.7998, KL Divergence: 1696.9962\n",
            "Epoch[9/20], Step [400/938], Reconstruction Loss: 5332.4946, KL Divergence: 1611.6010\n",
            "Epoch[9/20], Step [410/938], Reconstruction Loss: 5336.5737, KL Divergence: 1667.7552\n",
            "Epoch[9/20], Step [420/938], Reconstruction Loss: 5171.8687, KL Divergence: 1602.7809\n",
            "Epoch[9/20], Step [430/938], Reconstruction Loss: 5194.9595, KL Divergence: 1639.2001\n",
            "Epoch[9/20], Step [440/938], Reconstruction Loss: 5300.3164, KL Divergence: 1660.4149\n",
            "Epoch[9/20], Step [450/938], Reconstruction Loss: 5645.0908, KL Divergence: 1654.1665\n",
            "Epoch[9/20], Step [460/938], Reconstruction Loss: 5101.7891, KL Divergence: 1639.5443\n",
            "Epoch[9/20], Step [470/938], Reconstruction Loss: 5259.6719, KL Divergence: 1585.9526\n",
            "Epoch[9/20], Step [480/938], Reconstruction Loss: 5157.1392, KL Divergence: 1681.2554\n",
            "Epoch[9/20], Step [490/938], Reconstruction Loss: 5200.2119, KL Divergence: 1633.1611\n",
            "Epoch[9/20], Step [500/938], Reconstruction Loss: 5074.3232, KL Divergence: 1635.7184\n",
            "Epoch[9/20], Step [510/938], Reconstruction Loss: 5204.7388, KL Divergence: 1603.3140\n",
            "Epoch[9/20], Step [520/938], Reconstruction Loss: 5043.9087, KL Divergence: 1628.2316\n",
            "Epoch[9/20], Step [530/938], Reconstruction Loss: 5174.7690, KL Divergence: 1630.6602\n",
            "Epoch[9/20], Step [540/938], Reconstruction Loss: 5392.7690, KL Divergence: 1638.6885\n",
            "Epoch[9/20], Step [550/938], Reconstruction Loss: 5125.2271, KL Divergence: 1653.2261\n",
            "Epoch[9/20], Step [560/938], Reconstruction Loss: 5351.1294, KL Divergence: 1611.9932\n",
            "Epoch[9/20], Step [570/938], Reconstruction Loss: 5423.8545, KL Divergence: 1611.1458\n",
            "Epoch[9/20], Step [580/938], Reconstruction Loss: 5689.6440, KL Divergence: 1618.2335\n",
            "Epoch[9/20], Step [590/938], Reconstruction Loss: 5135.0605, KL Divergence: 1545.4972\n",
            "Epoch[9/20], Step [600/938], Reconstruction Loss: 5228.5698, KL Divergence: 1627.4950\n",
            "Epoch[9/20], Step [610/938], Reconstruction Loss: 4986.9487, KL Divergence: 1542.0735\n",
            "Epoch[9/20], Step [620/938], Reconstruction Loss: 4861.1758, KL Divergence: 1597.1769\n",
            "Epoch[9/20], Step [630/938], Reconstruction Loss: 5298.4927, KL Divergence: 1704.2047\n",
            "Epoch[9/20], Step [640/938], Reconstruction Loss: 5305.8193, KL Divergence: 1672.5660\n",
            "Epoch[9/20], Step [650/938], Reconstruction Loss: 5109.5156, KL Divergence: 1623.8849\n",
            "Epoch[9/20], Step [660/938], Reconstruction Loss: 5326.3984, KL Divergence: 1652.9990\n",
            "Epoch[9/20], Step [670/938], Reconstruction Loss: 5239.1665, KL Divergence: 1658.9310\n",
            "Epoch[9/20], Step [680/938], Reconstruction Loss: 5180.0327, KL Divergence: 1594.8014\n",
            "Epoch[9/20], Step [690/938], Reconstruction Loss: 5097.0195, KL Divergence: 1621.4890\n",
            "Epoch[9/20], Step [700/938], Reconstruction Loss: 4772.1040, KL Divergence: 1572.3291\n",
            "Epoch[9/20], Step [710/938], Reconstruction Loss: 5228.4424, KL Divergence: 1664.3560\n",
            "Epoch[9/20], Step [720/938], Reconstruction Loss: 5037.9790, KL Divergence: 1657.2372\n",
            "Epoch[9/20], Step [730/938], Reconstruction Loss: 5226.3374, KL Divergence: 1608.1005\n",
            "Epoch[9/20], Step [740/938], Reconstruction Loss: 5169.0654, KL Divergence: 1666.9032\n",
            "Epoch[9/20], Step [750/938], Reconstruction Loss: 5060.8159, KL Divergence: 1649.7379\n",
            "Epoch[9/20], Step [760/938], Reconstruction Loss: 5193.8252, KL Divergence: 1588.4620\n",
            "Epoch[9/20], Step [770/938], Reconstruction Loss: 5272.1440, KL Divergence: 1675.4902\n",
            "Epoch[9/20], Step [780/938], Reconstruction Loss: 5111.3730, KL Divergence: 1587.5203\n",
            "Epoch[9/20], Step [790/938], Reconstruction Loss: 5144.5132, KL Divergence: 1678.1262\n",
            "Epoch[9/20], Step [800/938], Reconstruction Loss: 5294.3408, KL Divergence: 1611.8562\n",
            "Epoch[9/20], Step [810/938], Reconstruction Loss: 5208.6841, KL Divergence: 1672.5829\n",
            "Epoch[9/20], Step [820/938], Reconstruction Loss: 5332.6680, KL Divergence: 1634.7559\n",
            "Epoch[9/20], Step [830/938], Reconstruction Loss: 5314.2754, KL Divergence: 1616.3229\n",
            "Epoch[9/20], Step [840/938], Reconstruction Loss: 4958.0117, KL Divergence: 1614.9503\n",
            "Epoch[9/20], Step [850/938], Reconstruction Loss: 5422.1157, KL Divergence: 1640.3196\n",
            "Epoch[9/20], Step [860/938], Reconstruction Loss: 5002.4312, KL Divergence: 1739.7301\n",
            "Epoch[9/20], Step [870/938], Reconstruction Loss: 5208.5366, KL Divergence: 1622.2035\n",
            "Epoch[9/20], Step [880/938], Reconstruction Loss: 5180.9277, KL Divergence: 1664.2574\n",
            "Epoch[9/20], Step [890/938], Reconstruction Loss: 5141.0869, KL Divergence: 1669.3964\n",
            "Epoch[9/20], Step [900/938], Reconstruction Loss: 5361.7271, KL Divergence: 1608.7972\n",
            "Epoch[9/20], Step [910/938], Reconstruction Loss: 4936.4360, KL Divergence: 1666.0360\n",
            "Epoch[9/20], Step [920/938], Reconstruction Loss: 4923.9639, KL Divergence: 1592.3508\n",
            "Epoch[9/20], Step [930/938], Reconstruction Loss: 5159.1523, KL Divergence: 1665.5004\n",
            "Epoch[10/20], Step [10/938], Reconstruction Loss: 5041.2085, KL Divergence: 1607.0547\n",
            "Epoch[10/20], Step [20/938], Reconstruction Loss: 5038.2852, KL Divergence: 1591.8568\n",
            "Epoch[10/20], Step [30/938], Reconstruction Loss: 5256.1885, KL Divergence: 1644.6740\n",
            "Epoch[10/20], Step [40/938], Reconstruction Loss: 5433.9731, KL Divergence: 1703.2511\n",
            "Epoch[10/20], Step [50/938], Reconstruction Loss: 5075.2939, KL Divergence: 1563.0000\n",
            "Epoch[10/20], Step [60/938], Reconstruction Loss: 5056.5869, KL Divergence: 1680.2023\n",
            "Epoch[10/20], Step [70/938], Reconstruction Loss: 5316.0591, KL Divergence: 1655.5767\n",
            "Epoch[10/20], Step [80/938], Reconstruction Loss: 5138.3184, KL Divergence: 1691.5701\n",
            "Epoch[10/20], Step [90/938], Reconstruction Loss: 4970.1958, KL Divergence: 1587.7920\n",
            "Epoch[10/20], Step [100/938], Reconstruction Loss: 5058.6021, KL Divergence: 1626.6508\n",
            "Epoch[10/20], Step [110/938], Reconstruction Loss: 4992.4639, KL Divergence: 1604.8884\n",
            "Epoch[10/20], Step [120/938], Reconstruction Loss: 5335.6851, KL Divergence: 1685.7522\n",
            "Epoch[10/20], Step [130/938], Reconstruction Loss: 4888.6128, KL Divergence: 1634.4524\n",
            "Epoch[10/20], Step [140/938], Reconstruction Loss: 5122.7344, KL Divergence: 1618.7882\n",
            "Epoch[10/20], Step [150/938], Reconstruction Loss: 5433.9336, KL Divergence: 1668.0465\n",
            "Epoch[10/20], Step [160/938], Reconstruction Loss: 4992.9985, KL Divergence: 1596.9933\n",
            "Epoch[10/20], Step [170/938], Reconstruction Loss: 5455.1470, KL Divergence: 1662.0455\n",
            "Epoch[10/20], Step [180/938], Reconstruction Loss: 4938.2935, KL Divergence: 1635.2675\n",
            "Epoch[10/20], Step [190/938], Reconstruction Loss: 5284.3545, KL Divergence: 1619.7864\n",
            "Epoch[10/20], Step [200/938], Reconstruction Loss: 4820.6348, KL Divergence: 1619.1621\n",
            "Epoch[10/20], Step [210/938], Reconstruction Loss: 5224.8252, KL Divergence: 1643.5813\n",
            "Epoch[10/20], Step [220/938], Reconstruction Loss: 5271.3691, KL Divergence: 1647.2108\n",
            "Epoch[10/20], Step [230/938], Reconstruction Loss: 5133.2368, KL Divergence: 1635.0057\n",
            "Epoch[10/20], Step [240/938], Reconstruction Loss: 5146.4233, KL Divergence: 1704.0492\n",
            "Epoch[10/20], Step [250/938], Reconstruction Loss: 5302.8223, KL Divergence: 1563.3083\n",
            "Epoch[10/20], Step [260/938], Reconstruction Loss: 4923.7690, KL Divergence: 1575.4869\n",
            "Epoch[10/20], Step [270/938], Reconstruction Loss: 4807.2720, KL Divergence: 1553.3278\n",
            "Epoch[10/20], Step [280/938], Reconstruction Loss: 5217.0981, KL Divergence: 1693.6555\n",
            "Epoch[10/20], Step [290/938], Reconstruction Loss: 5305.3696, KL Divergence: 1651.3729\n",
            "Epoch[10/20], Step [300/938], Reconstruction Loss: 5209.5317, KL Divergence: 1662.7937\n",
            "Epoch[10/20], Step [310/938], Reconstruction Loss: 4972.3159, KL Divergence: 1601.5131\n",
            "Epoch[10/20], Step [320/938], Reconstruction Loss: 4817.9731, KL Divergence: 1562.8759\n",
            "Epoch[10/20], Step [330/938], Reconstruction Loss: 5239.2446, KL Divergence: 1670.0520\n",
            "Epoch[10/20], Step [340/938], Reconstruction Loss: 5022.7705, KL Divergence: 1621.0735\n",
            "Epoch[10/20], Step [350/938], Reconstruction Loss: 4978.7480, KL Divergence: 1623.2141\n",
            "Epoch[10/20], Step [360/938], Reconstruction Loss: 4855.8828, KL Divergence: 1598.3619\n",
            "Epoch[10/20], Step [370/938], Reconstruction Loss: 5309.1978, KL Divergence: 1655.1316\n",
            "Epoch[10/20], Step [380/938], Reconstruction Loss: 5183.6260, KL Divergence: 1661.2745\n",
            "Epoch[10/20], Step [390/938], Reconstruction Loss: 4975.7007, KL Divergence: 1611.3445\n",
            "Epoch[10/20], Step [400/938], Reconstruction Loss: 5183.3555, KL Divergence: 1603.2847\n",
            "Epoch[10/20], Step [410/938], Reconstruction Loss: 4861.3672, KL Divergence: 1595.7836\n",
            "Epoch[10/20], Step [420/938], Reconstruction Loss: 5413.8530, KL Divergence: 1607.6791\n",
            "Epoch[10/20], Step [430/938], Reconstruction Loss: 5124.8833, KL Divergence: 1706.9398\n",
            "Epoch[10/20], Step [440/938], Reconstruction Loss: 5074.8228, KL Divergence: 1681.7462\n",
            "Epoch[10/20], Step [450/938], Reconstruction Loss: 5052.4961, KL Divergence: 1572.8385\n",
            "Epoch[10/20], Step [460/938], Reconstruction Loss: 4812.5537, KL Divergence: 1563.1427\n",
            "Epoch[10/20], Step [470/938], Reconstruction Loss: 5179.2324, KL Divergence: 1615.6870\n",
            "Epoch[10/20], Step [480/938], Reconstruction Loss: 5253.7969, KL Divergence: 1657.8289\n",
            "Epoch[10/20], Step [490/938], Reconstruction Loss: 5077.2549, KL Divergence: 1661.2965\n",
            "Epoch[10/20], Step [500/938], Reconstruction Loss: 5376.4043, KL Divergence: 1639.8702\n",
            "Epoch[10/20], Step [510/938], Reconstruction Loss: 5141.1167, KL Divergence: 1688.4708\n",
            "Epoch[10/20], Step [520/938], Reconstruction Loss: 5109.7661, KL Divergence: 1609.2821\n",
            "Epoch[10/20], Step [530/938], Reconstruction Loss: 5009.1436, KL Divergence: 1670.9705\n",
            "Epoch[10/20], Step [540/938], Reconstruction Loss: 5166.2471, KL Divergence: 1608.1034\n",
            "Epoch[10/20], Step [550/938], Reconstruction Loss: 5328.2217, KL Divergence: 1681.7483\n",
            "Epoch[10/20], Step [560/938], Reconstruction Loss: 5141.1992, KL Divergence: 1629.4352\n",
            "Epoch[10/20], Step [570/938], Reconstruction Loss: 4904.6597, KL Divergence: 1620.7858\n",
            "Epoch[10/20], Step [580/938], Reconstruction Loss: 5127.4297, KL Divergence: 1658.2152\n",
            "Epoch[10/20], Step [590/938], Reconstruction Loss: 5307.6909, KL Divergence: 1617.7543\n",
            "Epoch[10/20], Step [600/938], Reconstruction Loss: 5253.5957, KL Divergence: 1577.8721\n",
            "Epoch[10/20], Step [610/938], Reconstruction Loss: 5415.7749, KL Divergence: 1649.1079\n",
            "Epoch[10/20], Step [620/938], Reconstruction Loss: 5177.2549, KL Divergence: 1625.8287\n",
            "Epoch[10/20], Step [630/938], Reconstruction Loss: 5227.2695, KL Divergence: 1633.3303\n",
            "Epoch[10/20], Step [640/938], Reconstruction Loss: 5271.7607, KL Divergence: 1649.2323\n",
            "Epoch[10/20], Step [650/938], Reconstruction Loss: 4925.7744, KL Divergence: 1633.1117\n",
            "Epoch[10/20], Step [660/938], Reconstruction Loss: 4692.2388, KL Divergence: 1550.8586\n",
            "Epoch[10/20], Step [670/938], Reconstruction Loss: 5123.4688, KL Divergence: 1656.5254\n",
            "Epoch[10/20], Step [680/938], Reconstruction Loss: 5336.2393, KL Divergence: 1640.2606\n",
            "Epoch[10/20], Step [690/938], Reconstruction Loss: 5383.6592, KL Divergence: 1647.5918\n",
            "Epoch[10/20], Step [700/938], Reconstruction Loss: 4973.1011, KL Divergence: 1627.6353\n",
            "Epoch[10/20], Step [710/938], Reconstruction Loss: 5336.2856, KL Divergence: 1574.8611\n",
            "Epoch[10/20], Step [720/938], Reconstruction Loss: 5561.7734, KL Divergence: 1714.5566\n",
            "Epoch[10/20], Step [730/938], Reconstruction Loss: 5048.1943, KL Divergence: 1674.8936\n",
            "Epoch[10/20], Step [740/938], Reconstruction Loss: 4966.9585, KL Divergence: 1568.2661\n",
            "Epoch[10/20], Step [750/938], Reconstruction Loss: 4714.9390, KL Divergence: 1586.8242\n",
            "Epoch[10/20], Step [760/938], Reconstruction Loss: 5021.9658, KL Divergence: 1618.2152\n",
            "Epoch[10/20], Step [770/938], Reconstruction Loss: 5051.8687, KL Divergence: 1633.1335\n",
            "Epoch[10/20], Step [780/938], Reconstruction Loss: 5084.7778, KL Divergence: 1581.2561\n",
            "Epoch[10/20], Step [790/938], Reconstruction Loss: 5135.4639, KL Divergence: 1642.0715\n",
            "Epoch[10/20], Step [800/938], Reconstruction Loss: 4918.7329, KL Divergence: 1578.0029\n",
            "Epoch[10/20], Step [810/938], Reconstruction Loss: 5298.9800, KL Divergence: 1644.3368\n",
            "Epoch[10/20], Step [820/938], Reconstruction Loss: 5256.7827, KL Divergence: 1662.3644\n",
            "Epoch[10/20], Step [830/938], Reconstruction Loss: 4906.7764, KL Divergence: 1619.9080\n",
            "Epoch[10/20], Step [840/938], Reconstruction Loss: 4890.6475, KL Divergence: 1568.4545\n",
            "Epoch[10/20], Step [850/938], Reconstruction Loss: 4852.6533, KL Divergence: 1546.0253\n",
            "Epoch[10/20], Step [860/938], Reconstruction Loss: 4955.3530, KL Divergence: 1682.3173\n",
            "Epoch[10/20], Step [870/938], Reconstruction Loss: 5239.7266, KL Divergence: 1632.8220\n",
            "Epoch[10/20], Step [880/938], Reconstruction Loss: 5290.3076, KL Divergence: 1677.6573\n",
            "Epoch[10/20], Step [890/938], Reconstruction Loss: 5158.6069, KL Divergence: 1604.0922\n",
            "Epoch[10/20], Step [900/938], Reconstruction Loss: 5245.1411, KL Divergence: 1671.4875\n",
            "Epoch[10/20], Step [910/938], Reconstruction Loss: 5210.3306, KL Divergence: 1695.8691\n",
            "Epoch[10/20], Step [920/938], Reconstruction Loss: 5381.3857, KL Divergence: 1659.3751\n",
            "Epoch[10/20], Step [930/938], Reconstruction Loss: 5469.1265, KL Divergence: 1623.3062\n",
            "Epoch[11/20], Step [10/938], Reconstruction Loss: 5494.5386, KL Divergence: 1695.8019\n",
            "Epoch[11/20], Step [20/938], Reconstruction Loss: 5334.4619, KL Divergence: 1666.1416\n",
            "Epoch[11/20], Step [30/938], Reconstruction Loss: 5078.6494, KL Divergence: 1541.4286\n",
            "Epoch[11/20], Step [40/938], Reconstruction Loss: 5207.1152, KL Divergence: 1654.6096\n",
            "Epoch[11/20], Step [50/938], Reconstruction Loss: 4910.9941, KL Divergence: 1588.0149\n",
            "Epoch[11/20], Step [60/938], Reconstruction Loss: 4974.4590, KL Divergence: 1572.8983\n",
            "Epoch[11/20], Step [70/938], Reconstruction Loss: 5337.3970, KL Divergence: 1609.5878\n",
            "Epoch[11/20], Step [80/938], Reconstruction Loss: 5130.0708, KL Divergence: 1647.9333\n",
            "Epoch[11/20], Step [90/938], Reconstruction Loss: 5353.8418, KL Divergence: 1687.8745\n",
            "Epoch[11/20], Step [100/938], Reconstruction Loss: 5032.7041, KL Divergence: 1667.1976\n",
            "Epoch[11/20], Step [110/938], Reconstruction Loss: 5113.4590, KL Divergence: 1646.6581\n",
            "Epoch[11/20], Step [120/938], Reconstruction Loss: 5444.6934, KL Divergence: 1699.0245\n",
            "Epoch[11/20], Step [130/938], Reconstruction Loss: 4895.7031, KL Divergence: 1661.5392\n",
            "Epoch[11/20], Step [140/938], Reconstruction Loss: 5158.6890, KL Divergence: 1584.3778\n",
            "Epoch[11/20], Step [150/938], Reconstruction Loss: 5326.0337, KL Divergence: 1654.0258\n",
            "Epoch[11/20], Step [160/938], Reconstruction Loss: 5097.1040, KL Divergence: 1661.3013\n",
            "Epoch[11/20], Step [170/938], Reconstruction Loss: 5114.1514, KL Divergence: 1643.1058\n",
            "Epoch[11/20], Step [180/938], Reconstruction Loss: 5236.7007, KL Divergence: 1665.5361\n",
            "Epoch[11/20], Step [190/938], Reconstruction Loss: 5141.9883, KL Divergence: 1602.2198\n",
            "Epoch[11/20], Step [200/938], Reconstruction Loss: 4910.2920, KL Divergence: 1663.5863\n",
            "Epoch[11/20], Step [210/938], Reconstruction Loss: 4776.2021, KL Divergence: 1546.5082\n",
            "Epoch[11/20], Step [220/938], Reconstruction Loss: 5299.9414, KL Divergence: 1607.3833\n",
            "Epoch[11/20], Step [230/938], Reconstruction Loss: 4960.5371, KL Divergence: 1629.4569\n",
            "Epoch[11/20], Step [240/938], Reconstruction Loss: 5242.5024, KL Divergence: 1612.2440\n",
            "Epoch[11/20], Step [250/938], Reconstruction Loss: 5194.0923, KL Divergence: 1645.6277\n",
            "Epoch[11/20], Step [260/938], Reconstruction Loss: 5435.7202, KL Divergence: 1638.1425\n",
            "Epoch[11/20], Step [270/938], Reconstruction Loss: 5192.6685, KL Divergence: 1557.5055\n",
            "Epoch[11/20], Step [280/938], Reconstruction Loss: 5682.8706, KL Divergence: 1690.0479\n",
            "Epoch[11/20], Step [290/938], Reconstruction Loss: 5100.7046, KL Divergence: 1662.3376\n",
            "Epoch[11/20], Step [300/938], Reconstruction Loss: 5365.1055, KL Divergence: 1645.4302\n",
            "Epoch[11/20], Step [310/938], Reconstruction Loss: 5026.8311, KL Divergence: 1712.0153\n",
            "Epoch[11/20], Step [320/938], Reconstruction Loss: 5415.6587, KL Divergence: 1648.3516\n",
            "Epoch[11/20], Step [330/938], Reconstruction Loss: 4681.6328, KL Divergence: 1629.3107\n",
            "Epoch[11/20], Step [340/938], Reconstruction Loss: 5021.3896, KL Divergence: 1627.2412\n",
            "Epoch[11/20], Step [350/938], Reconstruction Loss: 5118.6592, KL Divergence: 1658.5326\n",
            "Epoch[11/20], Step [360/938], Reconstruction Loss: 5213.0332, KL Divergence: 1638.4037\n",
            "Epoch[11/20], Step [370/938], Reconstruction Loss: 5178.1855, KL Divergence: 1657.2668\n",
            "Epoch[11/20], Step [380/938], Reconstruction Loss: 5101.5869, KL Divergence: 1604.6528\n",
            "Epoch[11/20], Step [390/938], Reconstruction Loss: 5483.0288, KL Divergence: 1653.4055\n",
            "Epoch[11/20], Step [400/938], Reconstruction Loss: 5141.7520, KL Divergence: 1560.3468\n",
            "Epoch[11/20], Step [410/938], Reconstruction Loss: 5241.9224, KL Divergence: 1695.9811\n",
            "Epoch[11/20], Step [420/938], Reconstruction Loss: 4839.4956, KL Divergence: 1580.5559\n",
            "Epoch[11/20], Step [430/938], Reconstruction Loss: 5183.3125, KL Divergence: 1666.8286\n",
            "Epoch[11/20], Step [440/938], Reconstruction Loss: 5269.2627, KL Divergence: 1604.8182\n",
            "Epoch[11/20], Step [450/938], Reconstruction Loss: 5340.1240, KL Divergence: 1650.8599\n",
            "Epoch[11/20], Step [460/938], Reconstruction Loss: 5092.7163, KL Divergence: 1690.3602\n",
            "Epoch[11/20], Step [470/938], Reconstruction Loss: 4915.1274, KL Divergence: 1568.6698\n",
            "Epoch[11/20], Step [480/938], Reconstruction Loss: 4970.6440, KL Divergence: 1622.7302\n",
            "Epoch[11/20], Step [490/938], Reconstruction Loss: 5479.7422, KL Divergence: 1674.1848\n",
            "Epoch[11/20], Step [500/938], Reconstruction Loss: 4982.0205, KL Divergence: 1662.5100\n",
            "Epoch[11/20], Step [510/938], Reconstruction Loss: 5089.1714, KL Divergence: 1625.1600\n",
            "Epoch[11/20], Step [520/938], Reconstruction Loss: 4883.6118, KL Divergence: 1644.3148\n",
            "Epoch[11/20], Step [530/938], Reconstruction Loss: 5119.7515, KL Divergence: 1572.4275\n",
            "Epoch[11/20], Step [540/938], Reconstruction Loss: 5107.4072, KL Divergence: 1661.5598\n",
            "Epoch[11/20], Step [550/938], Reconstruction Loss: 5178.6323, KL Divergence: 1612.4442\n",
            "Epoch[11/20], Step [560/938], Reconstruction Loss: 5091.1548, KL Divergence: 1659.7094\n",
            "Epoch[11/20], Step [570/938], Reconstruction Loss: 4968.8188, KL Divergence: 1627.1935\n",
            "Epoch[11/20], Step [580/938], Reconstruction Loss: 5304.4966, KL Divergence: 1602.9171\n",
            "Epoch[11/20], Step [590/938], Reconstruction Loss: 5135.6636, KL Divergence: 1626.0518\n",
            "Epoch[11/20], Step [600/938], Reconstruction Loss: 4864.9731, KL Divergence: 1602.2040\n",
            "Epoch[11/20], Step [610/938], Reconstruction Loss: 4747.1733, KL Divergence: 1547.8550\n",
            "Epoch[11/20], Step [620/938], Reconstruction Loss: 4964.6851, KL Divergence: 1617.8782\n",
            "Epoch[11/20], Step [630/938], Reconstruction Loss: 4916.6523, KL Divergence: 1664.6324\n",
            "Epoch[11/20], Step [640/938], Reconstruction Loss: 5191.1206, KL Divergence: 1609.0125\n",
            "Epoch[11/20], Step [650/938], Reconstruction Loss: 5399.2837, KL Divergence: 1658.0376\n",
            "Epoch[11/20], Step [660/938], Reconstruction Loss: 4977.5068, KL Divergence: 1569.6132\n",
            "Epoch[11/20], Step [670/938], Reconstruction Loss: 4995.0308, KL Divergence: 1627.9153\n",
            "Epoch[11/20], Step [680/938], Reconstruction Loss: 5021.6689, KL Divergence: 1643.2657\n",
            "Epoch[11/20], Step [690/938], Reconstruction Loss: 5158.2720, KL Divergence: 1630.4275\n",
            "Epoch[11/20], Step [700/938], Reconstruction Loss: 5177.0825, KL Divergence: 1694.3104\n",
            "Epoch[11/20], Step [710/938], Reconstruction Loss: 5027.0649, KL Divergence: 1610.9812\n",
            "Epoch[11/20], Step [720/938], Reconstruction Loss: 5109.8457, KL Divergence: 1615.4230\n",
            "Epoch[11/20], Step [730/938], Reconstruction Loss: 5249.6997, KL Divergence: 1594.0048\n",
            "Epoch[11/20], Step [740/938], Reconstruction Loss: 4804.2598, KL Divergence: 1592.5729\n",
            "Epoch[11/20], Step [750/938], Reconstruction Loss: 5137.1821, KL Divergence: 1653.3496\n",
            "Epoch[11/20], Step [760/938], Reconstruction Loss: 5260.0605, KL Divergence: 1660.7708\n",
            "Epoch[11/20], Step [770/938], Reconstruction Loss: 5072.5547, KL Divergence: 1682.6106\n",
            "Epoch[11/20], Step [780/938], Reconstruction Loss: 5194.0376, KL Divergence: 1574.3086\n",
            "Epoch[11/20], Step [790/938], Reconstruction Loss: 5152.5464, KL Divergence: 1637.4683\n",
            "Epoch[11/20], Step [800/938], Reconstruction Loss: 5133.9897, KL Divergence: 1680.9797\n",
            "Epoch[11/20], Step [810/938], Reconstruction Loss: 5121.8491, KL Divergence: 1646.8932\n",
            "Epoch[11/20], Step [820/938], Reconstruction Loss: 5201.7236, KL Divergence: 1657.3467\n",
            "Epoch[11/20], Step [830/938], Reconstruction Loss: 5109.5142, KL Divergence: 1630.3220\n",
            "Epoch[11/20], Step [840/938], Reconstruction Loss: 5320.2500, KL Divergence: 1667.1678\n",
            "Epoch[11/20], Step [850/938], Reconstruction Loss: 5102.2500, KL Divergence: 1634.2877\n",
            "Epoch[11/20], Step [860/938], Reconstruction Loss: 5046.4312, KL Divergence: 1674.3315\n",
            "Epoch[11/20], Step [870/938], Reconstruction Loss: 4762.7769, KL Divergence: 1551.4001\n",
            "Epoch[11/20], Step [880/938], Reconstruction Loss: 5397.1738, KL Divergence: 1715.0004\n",
            "Epoch[11/20], Step [890/938], Reconstruction Loss: 4796.7441, KL Divergence: 1572.0105\n",
            "Epoch[11/20], Step [900/938], Reconstruction Loss: 5256.4546, KL Divergence: 1675.1674\n",
            "Epoch[11/20], Step [910/938], Reconstruction Loss: 5019.3789, KL Divergence: 1672.0410\n",
            "Epoch[11/20], Step [920/938], Reconstruction Loss: 4570.8901, KL Divergence: 1539.8584\n",
            "Epoch[11/20], Step [930/938], Reconstruction Loss: 5086.0825, KL Divergence: 1690.8508\n",
            "Epoch[12/20], Step [10/938], Reconstruction Loss: 5388.5537, KL Divergence: 1709.8478\n",
            "Epoch[12/20], Step [20/938], Reconstruction Loss: 5261.0913, KL Divergence: 1568.7533\n",
            "Epoch[12/20], Step [30/938], Reconstruction Loss: 5265.5542, KL Divergence: 1659.4015\n",
            "Epoch[12/20], Step [40/938], Reconstruction Loss: 5022.5142, KL Divergence: 1599.4286\n",
            "Epoch[12/20], Step [50/938], Reconstruction Loss: 4964.4292, KL Divergence: 1595.0510\n",
            "Epoch[12/20], Step [60/938], Reconstruction Loss: 5090.6504, KL Divergence: 1614.8521\n",
            "Epoch[12/20], Step [70/938], Reconstruction Loss: 5010.0386, KL Divergence: 1559.2092\n",
            "Epoch[12/20], Step [80/938], Reconstruction Loss: 4997.9648, KL Divergence: 1573.7971\n",
            "Epoch[12/20], Step [90/938], Reconstruction Loss: 5026.0571, KL Divergence: 1624.0670\n",
            "Epoch[12/20], Step [100/938], Reconstruction Loss: 5170.3823, KL Divergence: 1666.6812\n",
            "Epoch[12/20], Step [110/938], Reconstruction Loss: 5191.4536, KL Divergence: 1631.7970\n",
            "Epoch[12/20], Step [120/938], Reconstruction Loss: 5214.5576, KL Divergence: 1694.4928\n",
            "Epoch[12/20], Step [130/938], Reconstruction Loss: 5175.5942, KL Divergence: 1725.3186\n",
            "Epoch[12/20], Step [140/938], Reconstruction Loss: 5356.1924, KL Divergence: 1600.2216\n",
            "Epoch[12/20], Step [150/938], Reconstruction Loss: 4953.4673, KL Divergence: 1669.2043\n",
            "Epoch[12/20], Step [160/938], Reconstruction Loss: 4966.1611, KL Divergence: 1541.7755\n",
            "Epoch[12/20], Step [170/938], Reconstruction Loss: 5024.7349, KL Divergence: 1673.5907\n",
            "Epoch[12/20], Step [180/938], Reconstruction Loss: 5179.7681, KL Divergence: 1637.6311\n",
            "Epoch[12/20], Step [190/938], Reconstruction Loss: 5128.1699, KL Divergence: 1687.5649\n",
            "Epoch[12/20], Step [200/938], Reconstruction Loss: 4939.9326, KL Divergence: 1623.6627\n",
            "Epoch[12/20], Step [210/938], Reconstruction Loss: 5267.1338, KL Divergence: 1747.3544\n",
            "Epoch[12/20], Step [220/938], Reconstruction Loss: 5317.6548, KL Divergence: 1593.8597\n",
            "Epoch[12/20], Step [230/938], Reconstruction Loss: 5461.6553, KL Divergence: 1669.2076\n",
            "Epoch[12/20], Step [240/938], Reconstruction Loss: 5168.9819, KL Divergence: 1619.3784\n",
            "Epoch[12/20], Step [250/938], Reconstruction Loss: 5128.7881, KL Divergence: 1605.5524\n",
            "Epoch[12/20], Step [260/938], Reconstruction Loss: 5212.3882, KL Divergence: 1636.7456\n",
            "Epoch[12/20], Step [270/938], Reconstruction Loss: 5070.4062, KL Divergence: 1628.9719\n",
            "Epoch[12/20], Step [280/938], Reconstruction Loss: 5153.1958, KL Divergence: 1607.5593\n",
            "Epoch[12/20], Step [290/938], Reconstruction Loss: 5260.5098, KL Divergence: 1697.3148\n",
            "Epoch[12/20], Step [300/938], Reconstruction Loss: 5232.4639, KL Divergence: 1676.4144\n",
            "Epoch[12/20], Step [310/938], Reconstruction Loss: 5177.6787, KL Divergence: 1670.7776\n",
            "Epoch[12/20], Step [320/938], Reconstruction Loss: 5086.7847, KL Divergence: 1695.4270\n",
            "Epoch[12/20], Step [330/938], Reconstruction Loss: 5155.1665, KL Divergence: 1662.8009\n",
            "Epoch[12/20], Step [340/938], Reconstruction Loss: 5014.7559, KL Divergence: 1690.0273\n",
            "Epoch[12/20], Step [350/938], Reconstruction Loss: 4998.5068, KL Divergence: 1561.9070\n",
            "Epoch[12/20], Step [360/938], Reconstruction Loss: 4872.1465, KL Divergence: 1618.4222\n",
            "Epoch[12/20], Step [370/938], Reconstruction Loss: 4966.6240, KL Divergence: 1601.0331\n",
            "Epoch[12/20], Step [380/938], Reconstruction Loss: 5100.3428, KL Divergence: 1643.0677\n",
            "Epoch[12/20], Step [390/938], Reconstruction Loss: 5213.0459, KL Divergence: 1632.7797\n",
            "Epoch[12/20], Step [400/938], Reconstruction Loss: 5579.7583, KL Divergence: 1745.6772\n",
            "Epoch[12/20], Step [410/938], Reconstruction Loss: 4724.9976, KL Divergence: 1612.7570\n",
            "Epoch[12/20], Step [420/938], Reconstruction Loss: 4846.2871, KL Divergence: 1572.8125\n",
            "Epoch[12/20], Step [430/938], Reconstruction Loss: 5110.7534, KL Divergence: 1643.3701\n",
            "Epoch[12/20], Step [440/938], Reconstruction Loss: 4868.1943, KL Divergence: 1625.3347\n",
            "Epoch[12/20], Step [450/938], Reconstruction Loss: 5017.4160, KL Divergence: 1649.0195\n",
            "Epoch[12/20], Step [460/938], Reconstruction Loss: 5390.6738, KL Divergence: 1716.0775\n",
            "Epoch[12/20], Step [470/938], Reconstruction Loss: 5206.5088, KL Divergence: 1667.4133\n",
            "Epoch[12/20], Step [480/938], Reconstruction Loss: 5244.5415, KL Divergence: 1662.5771\n",
            "Epoch[12/20], Step [490/938], Reconstruction Loss: 4975.5669, KL Divergence: 1618.7483\n",
            "Epoch[12/20], Step [500/938], Reconstruction Loss: 5043.1538, KL Divergence: 1618.0922\n",
            "Epoch[12/20], Step [510/938], Reconstruction Loss: 5262.4512, KL Divergence: 1660.5583\n",
            "Epoch[12/20], Step [520/938], Reconstruction Loss: 5188.4751, KL Divergence: 1714.1517\n",
            "Epoch[12/20], Step [530/938], Reconstruction Loss: 5006.6641, KL Divergence: 1600.6617\n",
            "Epoch[12/20], Step [540/938], Reconstruction Loss: 4836.5605, KL Divergence: 1574.5938\n",
            "Epoch[12/20], Step [550/938], Reconstruction Loss: 5050.9204, KL Divergence: 1698.2421\n",
            "Epoch[12/20], Step [560/938], Reconstruction Loss: 5094.9639, KL Divergence: 1641.8983\n",
            "Epoch[12/20], Step [570/938], Reconstruction Loss: 5406.8472, KL Divergence: 1715.2838\n",
            "Epoch[12/20], Step [580/938], Reconstruction Loss: 5230.2178, KL Divergence: 1697.3590\n",
            "Epoch[12/20], Step [590/938], Reconstruction Loss: 4939.7368, KL Divergence: 1575.0487\n",
            "Epoch[12/20], Step [600/938], Reconstruction Loss: 5408.1836, KL Divergence: 1695.7609\n",
            "Epoch[12/20], Step [610/938], Reconstruction Loss: 5177.2070, KL Divergence: 1623.0505\n",
            "Epoch[12/20], Step [620/938], Reconstruction Loss: 5131.6362, KL Divergence: 1582.6945\n",
            "Epoch[12/20], Step [630/938], Reconstruction Loss: 5394.3091, KL Divergence: 1678.5382\n",
            "Epoch[12/20], Step [640/938], Reconstruction Loss: 4844.9756, KL Divergence: 1642.2098\n",
            "Epoch[12/20], Step [650/938], Reconstruction Loss: 4815.0215, KL Divergence: 1628.2220\n",
            "Epoch[12/20], Step [660/938], Reconstruction Loss: 5339.7749, KL Divergence: 1600.9985\n",
            "Epoch[12/20], Step [670/938], Reconstruction Loss: 5089.3398, KL Divergence: 1669.6542\n",
            "Epoch[12/20], Step [680/938], Reconstruction Loss: 5296.5229, KL Divergence: 1639.8655\n",
            "Epoch[12/20], Step [690/938], Reconstruction Loss: 4906.3184, KL Divergence: 1595.0164\n",
            "Epoch[12/20], Step [700/938], Reconstruction Loss: 5145.5063, KL Divergence: 1653.0120\n",
            "Epoch[12/20], Step [710/938], Reconstruction Loss: 5102.7876, KL Divergence: 1613.3308\n",
            "Epoch[12/20], Step [720/938], Reconstruction Loss: 5067.0366, KL Divergence: 1628.5829\n",
            "Epoch[12/20], Step [730/938], Reconstruction Loss: 5068.8291, KL Divergence: 1679.4044\n",
            "Epoch[12/20], Step [740/938], Reconstruction Loss: 4897.8228, KL Divergence: 1591.5026\n",
            "Epoch[12/20], Step [750/938], Reconstruction Loss: 4866.0361, KL Divergence: 1553.8392\n",
            "Epoch[12/20], Step [760/938], Reconstruction Loss: 4902.7808, KL Divergence: 1529.7495\n",
            "Epoch[12/20], Step [770/938], Reconstruction Loss: 5303.4097, KL Divergence: 1653.1198\n",
            "Epoch[12/20], Step [780/938], Reconstruction Loss: 5159.9473, KL Divergence: 1693.8787\n",
            "Epoch[12/20], Step [790/938], Reconstruction Loss: 5198.8242, KL Divergence: 1543.5328\n",
            "Epoch[12/20], Step [800/938], Reconstruction Loss: 5194.1147, KL Divergence: 1645.5332\n",
            "Epoch[12/20], Step [810/938], Reconstruction Loss: 4855.4424, KL Divergence: 1491.7892\n",
            "Epoch[12/20], Step [820/938], Reconstruction Loss: 4958.2749, KL Divergence: 1640.7313\n",
            "Epoch[12/20], Step [830/938], Reconstruction Loss: 5094.5352, KL Divergence: 1571.6228\n",
            "Epoch[12/20], Step [840/938], Reconstruction Loss: 5274.8833, KL Divergence: 1663.7640\n",
            "Epoch[12/20], Step [850/938], Reconstruction Loss: 5048.4917, KL Divergence: 1640.4231\n",
            "Epoch[12/20], Step [860/938], Reconstruction Loss: 5170.1562, KL Divergence: 1623.9225\n",
            "Epoch[12/20], Step [870/938], Reconstruction Loss: 4897.5938, KL Divergence: 1616.2568\n",
            "Epoch[12/20], Step [880/938], Reconstruction Loss: 4971.2207, KL Divergence: 1527.6162\n",
            "Epoch[12/20], Step [890/938], Reconstruction Loss: 5270.4956, KL Divergence: 1678.5171\n",
            "Epoch[12/20], Step [900/938], Reconstruction Loss: 5165.5527, KL Divergence: 1615.9646\n",
            "Epoch[12/20], Step [910/938], Reconstruction Loss: 4994.7378, KL Divergence: 1628.4642\n",
            "Epoch[12/20], Step [920/938], Reconstruction Loss: 5084.8960, KL Divergence: 1616.6167\n",
            "Epoch[12/20], Step [930/938], Reconstruction Loss: 5034.1113, KL Divergence: 1620.1549\n",
            "Epoch[13/20], Step [10/938], Reconstruction Loss: 4944.9932, KL Divergence: 1619.0955\n",
            "Epoch[13/20], Step [20/938], Reconstruction Loss: 4962.3364, KL Divergence: 1598.5822\n",
            "Epoch[13/20], Step [30/938], Reconstruction Loss: 5174.3511, KL Divergence: 1792.4911\n",
            "Epoch[13/20], Step [40/938], Reconstruction Loss: 5142.4038, KL Divergence: 1641.3406\n",
            "Epoch[13/20], Step [50/938], Reconstruction Loss: 5023.3706, KL Divergence: 1682.9452\n",
            "Epoch[13/20], Step [60/938], Reconstruction Loss: 5480.7827, KL Divergence: 1688.0580\n",
            "Epoch[13/20], Step [70/938], Reconstruction Loss: 5201.8042, KL Divergence: 1672.3345\n",
            "Epoch[13/20], Step [80/938], Reconstruction Loss: 5040.9653, KL Divergence: 1641.4613\n",
            "Epoch[13/20], Step [90/938], Reconstruction Loss: 5181.0918, KL Divergence: 1651.0393\n",
            "Epoch[13/20], Step [100/938], Reconstruction Loss: 5013.4258, KL Divergence: 1609.1897\n",
            "Epoch[13/20], Step [110/938], Reconstruction Loss: 4885.3711, KL Divergence: 1630.8630\n",
            "Epoch[13/20], Step [120/938], Reconstruction Loss: 4966.6680, KL Divergence: 1625.0143\n",
            "Epoch[13/20], Step [130/938], Reconstruction Loss: 5156.8975, KL Divergence: 1649.1593\n",
            "Epoch[13/20], Step [140/938], Reconstruction Loss: 5332.4556, KL Divergence: 1642.4081\n",
            "Epoch[13/20], Step [150/938], Reconstruction Loss: 5147.8672, KL Divergence: 1674.8219\n",
            "Epoch[13/20], Step [160/938], Reconstruction Loss: 4935.5347, KL Divergence: 1602.2729\n",
            "Epoch[13/20], Step [170/938], Reconstruction Loss: 5281.4907, KL Divergence: 1715.5688\n",
            "Epoch[13/20], Step [180/938], Reconstruction Loss: 5317.2695, KL Divergence: 1693.0616\n",
            "Epoch[13/20], Step [190/938], Reconstruction Loss: 5540.4165, KL Divergence: 1727.0693\n",
            "Epoch[13/20], Step [200/938], Reconstruction Loss: 5169.9717, KL Divergence: 1657.4159\n",
            "Epoch[13/20], Step [210/938], Reconstruction Loss: 5075.7720, KL Divergence: 1622.7273\n",
            "Epoch[13/20], Step [220/938], Reconstruction Loss: 5298.9131, KL Divergence: 1661.7493\n",
            "Epoch[13/20], Step [230/938], Reconstruction Loss: 4749.9102, KL Divergence: 1670.6267\n",
            "Epoch[13/20], Step [240/938], Reconstruction Loss: 5305.7188, KL Divergence: 1590.2109\n",
            "Epoch[13/20], Step [250/938], Reconstruction Loss: 5331.2378, KL Divergence: 1653.9044\n",
            "Epoch[13/20], Step [260/938], Reconstruction Loss: 4952.7856, KL Divergence: 1613.1692\n",
            "Epoch[13/20], Step [270/938], Reconstruction Loss: 4948.0083, KL Divergence: 1617.3741\n",
            "Epoch[13/20], Step [280/938], Reconstruction Loss: 4828.9863, KL Divergence: 1581.3036\n",
            "Epoch[13/20], Step [290/938], Reconstruction Loss: 4993.4502, KL Divergence: 1588.0922\n",
            "Epoch[13/20], Step [300/938], Reconstruction Loss: 4845.1602, KL Divergence: 1583.9531\n",
            "Epoch[13/20], Step [310/938], Reconstruction Loss: 4902.0708, KL Divergence: 1648.9260\n",
            "Epoch[13/20], Step [320/938], Reconstruction Loss: 5161.5034, KL Divergence: 1619.3373\n",
            "Epoch[13/20], Step [330/938], Reconstruction Loss: 5139.8071, KL Divergence: 1659.2367\n",
            "Epoch[13/20], Step [340/938], Reconstruction Loss: 4826.7207, KL Divergence: 1569.6929\n",
            "Epoch[13/20], Step [350/938], Reconstruction Loss: 5464.6670, KL Divergence: 1754.6251\n",
            "Epoch[13/20], Step [360/938], Reconstruction Loss: 5064.1211, KL Divergence: 1601.0809\n",
            "Epoch[13/20], Step [370/938], Reconstruction Loss: 5023.5024, KL Divergence: 1616.6108\n",
            "Epoch[13/20], Step [380/938], Reconstruction Loss: 5032.5615, KL Divergence: 1587.0729\n",
            "Epoch[13/20], Step [390/938], Reconstruction Loss: 5171.3818, KL Divergence: 1618.5956\n",
            "Epoch[13/20], Step [400/938], Reconstruction Loss: 5151.2876, KL Divergence: 1672.1548\n",
            "Epoch[13/20], Step [410/938], Reconstruction Loss: 5115.5459, KL Divergence: 1650.1501\n",
            "Epoch[13/20], Step [420/938], Reconstruction Loss: 5128.7480, KL Divergence: 1662.6643\n",
            "Epoch[13/20], Step [430/938], Reconstruction Loss: 5855.3408, KL Divergence: 1743.5251\n",
            "Epoch[13/20], Step [440/938], Reconstruction Loss: 4829.8101, KL Divergence: 1603.8331\n",
            "Epoch[13/20], Step [450/938], Reconstruction Loss: 5183.1890, KL Divergence: 1604.5741\n",
            "Epoch[13/20], Step [460/938], Reconstruction Loss: 5081.4375, KL Divergence: 1644.1584\n",
            "Epoch[13/20], Step [470/938], Reconstruction Loss: 5136.6338, KL Divergence: 1615.5171\n",
            "Epoch[13/20], Step [480/938], Reconstruction Loss: 5238.8315, KL Divergence: 1683.7302\n",
            "Epoch[13/20], Step [490/938], Reconstruction Loss: 5619.5996, KL Divergence: 1650.0753\n",
            "Epoch[13/20], Step [500/938], Reconstruction Loss: 4907.1787, KL Divergence: 1616.3804\n",
            "Epoch[13/20], Step [510/938], Reconstruction Loss: 5343.4907, KL Divergence: 1610.1274\n",
            "Epoch[13/20], Step [520/938], Reconstruction Loss: 5058.2510, KL Divergence: 1646.0370\n",
            "Epoch[13/20], Step [530/938], Reconstruction Loss: 5321.4839, KL Divergence: 1700.6091\n",
            "Epoch[13/20], Step [540/938], Reconstruction Loss: 5234.2822, KL Divergence: 1612.2585\n",
            "Epoch[13/20], Step [550/938], Reconstruction Loss: 5264.3911, KL Divergence: 1689.9026\n",
            "Epoch[13/20], Step [560/938], Reconstruction Loss: 4970.0366, KL Divergence: 1604.8076\n",
            "Epoch[13/20], Step [570/938], Reconstruction Loss: 5337.2539, KL Divergence: 1661.1860\n",
            "Epoch[13/20], Step [580/938], Reconstruction Loss: 4857.8564, KL Divergence: 1575.0339\n",
            "Epoch[13/20], Step [590/938], Reconstruction Loss: 5171.7788, KL Divergence: 1684.7562\n",
            "Epoch[13/20], Step [600/938], Reconstruction Loss: 5140.9326, KL Divergence: 1650.3024\n",
            "Epoch[13/20], Step [610/938], Reconstruction Loss: 4683.8223, KL Divergence: 1597.9749\n",
            "Epoch[13/20], Step [620/938], Reconstruction Loss: 5229.7822, KL Divergence: 1695.3878\n",
            "Epoch[13/20], Step [630/938], Reconstruction Loss: 5202.8413, KL Divergence: 1645.6381\n",
            "Epoch[13/20], Step [640/938], Reconstruction Loss: 5267.6392, KL Divergence: 1608.9319\n",
            "Epoch[13/20], Step [650/938], Reconstruction Loss: 5075.7969, KL Divergence: 1648.6895\n",
            "Epoch[13/20], Step [660/938], Reconstruction Loss: 5059.0166, KL Divergence: 1714.1053\n",
            "Epoch[13/20], Step [670/938], Reconstruction Loss: 4857.3120, KL Divergence: 1654.4060\n",
            "Epoch[13/20], Step [680/938], Reconstruction Loss: 5353.6104, KL Divergence: 1657.3485\n",
            "Epoch[13/20], Step [690/938], Reconstruction Loss: 5146.6777, KL Divergence: 1621.3600\n",
            "Epoch[13/20], Step [700/938], Reconstruction Loss: 5106.7998, KL Divergence: 1623.7661\n",
            "Epoch[13/20], Step [710/938], Reconstruction Loss: 5090.8721, KL Divergence: 1600.0382\n",
            "Epoch[13/20], Step [720/938], Reconstruction Loss: 4988.5562, KL Divergence: 1566.2852\n",
            "Epoch[13/20], Step [730/938], Reconstruction Loss: 5190.9634, KL Divergence: 1739.3125\n",
            "Epoch[13/20], Step [740/938], Reconstruction Loss: 5167.6055, KL Divergence: 1625.1333\n",
            "Epoch[13/20], Step [750/938], Reconstruction Loss: 4979.8379, KL Divergence: 1636.0803\n",
            "Epoch[13/20], Step [760/938], Reconstruction Loss: 5572.4326, KL Divergence: 1675.5400\n",
            "Epoch[13/20], Step [770/938], Reconstruction Loss: 5133.6167, KL Divergence: 1554.6777\n",
            "Epoch[13/20], Step [780/938], Reconstruction Loss: 5094.6748, KL Divergence: 1631.4944\n",
            "Epoch[13/20], Step [790/938], Reconstruction Loss: 5092.0796, KL Divergence: 1646.7223\n",
            "Epoch[13/20], Step [800/938], Reconstruction Loss: 5290.0967, KL Divergence: 1683.4208\n",
            "Epoch[13/20], Step [810/938], Reconstruction Loss: 4794.9121, KL Divergence: 1630.2266\n",
            "Epoch[13/20], Step [820/938], Reconstruction Loss: 5447.5981, KL Divergence: 1690.8567\n",
            "Epoch[13/20], Step [830/938], Reconstruction Loss: 5119.5400, KL Divergence: 1621.6067\n",
            "Epoch[13/20], Step [840/938], Reconstruction Loss: 4752.8340, KL Divergence: 1630.3096\n",
            "Epoch[13/20], Step [850/938], Reconstruction Loss: 5334.5728, KL Divergence: 1621.5461\n",
            "Epoch[13/20], Step [860/938], Reconstruction Loss: 4924.6201, KL Divergence: 1676.6113\n",
            "Epoch[13/20], Step [870/938], Reconstruction Loss: 5118.0146, KL Divergence: 1585.8483\n",
            "Epoch[13/20], Step [880/938], Reconstruction Loss: 5244.0835, KL Divergence: 1662.7987\n",
            "Epoch[13/20], Step [890/938], Reconstruction Loss: 5119.4443, KL Divergence: 1591.1157\n",
            "Epoch[13/20], Step [900/938], Reconstruction Loss: 4998.6284, KL Divergence: 1586.3645\n",
            "Epoch[13/20], Step [910/938], Reconstruction Loss: 5121.4463, KL Divergence: 1678.6434\n",
            "Epoch[13/20], Step [920/938], Reconstruction Loss: 4726.4521, KL Divergence: 1522.7673\n",
            "Epoch[13/20], Step [930/938], Reconstruction Loss: 5204.3071, KL Divergence: 1628.2347\n",
            "Epoch[14/20], Step [10/938], Reconstruction Loss: 5181.8584, KL Divergence: 1732.1925\n",
            "Epoch[14/20], Step [20/938], Reconstruction Loss: 4936.7900, KL Divergence: 1654.2706\n",
            "Epoch[14/20], Step [30/938], Reconstruction Loss: 5173.1396, KL Divergence: 1656.2246\n",
            "Epoch[14/20], Step [40/938], Reconstruction Loss: 5167.1128, KL Divergence: 1671.7719\n",
            "Epoch[14/20], Step [50/938], Reconstruction Loss: 4934.0996, KL Divergence: 1604.8210\n",
            "Epoch[14/20], Step [60/938], Reconstruction Loss: 5152.7803, KL Divergence: 1660.9917\n",
            "Epoch[14/20], Step [70/938], Reconstruction Loss: 4882.7070, KL Divergence: 1611.7760\n",
            "Epoch[14/20], Step [80/938], Reconstruction Loss: 4876.6934, KL Divergence: 1678.3025\n",
            "Epoch[14/20], Step [90/938], Reconstruction Loss: 4704.7153, KL Divergence: 1551.3944\n",
            "Epoch[14/20], Step [100/938], Reconstruction Loss: 4856.8242, KL Divergence: 1667.4486\n",
            "Epoch[14/20], Step [110/938], Reconstruction Loss: 5046.9009, KL Divergence: 1664.0056\n",
            "Epoch[14/20], Step [120/938], Reconstruction Loss: 5106.0757, KL Divergence: 1621.6453\n",
            "Epoch[14/20], Step [130/938], Reconstruction Loss: 5288.3716, KL Divergence: 1664.5183\n",
            "Epoch[14/20], Step [140/938], Reconstruction Loss: 5338.5640, KL Divergence: 1659.0599\n",
            "Epoch[14/20], Step [150/938], Reconstruction Loss: 5267.4385, KL Divergence: 1725.5890\n",
            "Epoch[14/20], Step [160/938], Reconstruction Loss: 4930.9883, KL Divergence: 1596.4408\n",
            "Epoch[14/20], Step [170/938], Reconstruction Loss: 4731.3447, KL Divergence: 1578.9218\n",
            "Epoch[14/20], Step [180/938], Reconstruction Loss: 5225.3252, KL Divergence: 1647.4080\n",
            "Epoch[14/20], Step [190/938], Reconstruction Loss: 5086.8672, KL Divergence: 1610.3198\n",
            "Epoch[14/20], Step [200/938], Reconstruction Loss: 5125.6357, KL Divergence: 1653.8223\n",
            "Epoch[14/20], Step [210/938], Reconstruction Loss: 5390.2847, KL Divergence: 1685.4818\n",
            "Epoch[14/20], Step [220/938], Reconstruction Loss: 5111.8472, KL Divergence: 1673.4393\n",
            "Epoch[14/20], Step [230/938], Reconstruction Loss: 5073.1011, KL Divergence: 1587.6265\n",
            "Epoch[14/20], Step [240/938], Reconstruction Loss: 5126.7974, KL Divergence: 1611.5220\n",
            "Epoch[14/20], Step [250/938], Reconstruction Loss: 5106.4697, KL Divergence: 1616.2482\n",
            "Epoch[14/20], Step [260/938], Reconstruction Loss: 5028.1270, KL Divergence: 1650.4348\n",
            "Epoch[14/20], Step [270/938], Reconstruction Loss: 5112.0039, KL Divergence: 1629.2706\n",
            "Epoch[14/20], Step [280/938], Reconstruction Loss: 5382.2363, KL Divergence: 1638.6049\n",
            "Epoch[14/20], Step [290/938], Reconstruction Loss: 5274.9624, KL Divergence: 1674.3413\n",
            "Epoch[14/20], Step [300/938], Reconstruction Loss: 5026.2061, KL Divergence: 1585.6406\n",
            "Epoch[14/20], Step [310/938], Reconstruction Loss: 4985.6108, KL Divergence: 1645.2361\n",
            "Epoch[14/20], Step [320/938], Reconstruction Loss: 4904.1455, KL Divergence: 1605.3141\n",
            "Epoch[14/20], Step [330/938], Reconstruction Loss: 4945.8726, KL Divergence: 1576.3481\n",
            "Epoch[14/20], Step [340/938], Reconstruction Loss: 5073.4785, KL Divergence: 1632.9705\n",
            "Epoch[14/20], Step [350/938], Reconstruction Loss: 5062.2119, KL Divergence: 1626.5200\n",
            "Epoch[14/20], Step [360/938], Reconstruction Loss: 5139.6553, KL Divergence: 1675.7073\n",
            "Epoch[14/20], Step [370/938], Reconstruction Loss: 4618.9302, KL Divergence: 1568.4783\n",
            "Epoch[14/20], Step [380/938], Reconstruction Loss: 4854.3765, KL Divergence: 1638.4341\n",
            "Epoch[14/20], Step [390/938], Reconstruction Loss: 5053.0752, KL Divergence: 1660.5477\n",
            "Epoch[14/20], Step [400/938], Reconstruction Loss: 5094.5493, KL Divergence: 1608.5608\n",
            "Epoch[14/20], Step [410/938], Reconstruction Loss: 4938.0542, KL Divergence: 1652.9670\n",
            "Epoch[14/20], Step [420/938], Reconstruction Loss: 5168.1465, KL Divergence: 1690.0745\n",
            "Epoch[14/20], Step [430/938], Reconstruction Loss: 5330.7021, KL Divergence: 1609.7225\n",
            "Epoch[14/20], Step [440/938], Reconstruction Loss: 5171.8037, KL Divergence: 1616.7307\n",
            "Epoch[14/20], Step [450/938], Reconstruction Loss: 5168.9941, KL Divergence: 1654.6226\n",
            "Epoch[14/20], Step [460/938], Reconstruction Loss: 5233.8745, KL Divergence: 1696.1863\n",
            "Epoch[14/20], Step [470/938], Reconstruction Loss: 5058.9907, KL Divergence: 1623.0227\n",
            "Epoch[14/20], Step [480/938], Reconstruction Loss: 5151.9326, KL Divergence: 1679.0571\n",
            "Epoch[14/20], Step [490/938], Reconstruction Loss: 5059.0908, KL Divergence: 1652.0192\n",
            "Epoch[14/20], Step [500/938], Reconstruction Loss: 4634.4927, KL Divergence: 1561.7452\n",
            "Epoch[14/20], Step [510/938], Reconstruction Loss: 5039.6543, KL Divergence: 1675.1740\n",
            "Epoch[14/20], Step [520/938], Reconstruction Loss: 4850.8989, KL Divergence: 1630.3290\n",
            "Epoch[14/20], Step [530/938], Reconstruction Loss: 4918.2334, KL Divergence: 1622.3473\n",
            "Epoch[14/20], Step [540/938], Reconstruction Loss: 5261.0884, KL Divergence: 1703.7410\n",
            "Epoch[14/20], Step [550/938], Reconstruction Loss: 4708.3267, KL Divergence: 1606.4784\n",
            "Epoch[14/20], Step [560/938], Reconstruction Loss: 4880.3950, KL Divergence: 1582.3243\n",
            "Epoch[14/20], Step [570/938], Reconstruction Loss: 4906.9634, KL Divergence: 1607.4421\n",
            "Epoch[14/20], Step [580/938], Reconstruction Loss: 5342.3467, KL Divergence: 1681.3170\n",
            "Epoch[14/20], Step [590/938], Reconstruction Loss: 4896.9858, KL Divergence: 1646.7963\n",
            "Epoch[14/20], Step [600/938], Reconstruction Loss: 5162.7495, KL Divergence: 1629.9171\n",
            "Epoch[14/20], Step [610/938], Reconstruction Loss: 5080.2954, KL Divergence: 1604.8921\n",
            "Epoch[14/20], Step [620/938], Reconstruction Loss: 4874.7476, KL Divergence: 1570.4521\n",
            "Epoch[14/20], Step [630/938], Reconstruction Loss: 4666.9761, KL Divergence: 1621.2845\n",
            "Epoch[14/20], Step [640/938], Reconstruction Loss: 5309.0981, KL Divergence: 1683.7758\n",
            "Epoch[14/20], Step [650/938], Reconstruction Loss: 5141.9604, KL Divergence: 1694.3171\n",
            "Epoch[14/20], Step [660/938], Reconstruction Loss: 5582.1265, KL Divergence: 1606.8175\n",
            "Epoch[14/20], Step [670/938], Reconstruction Loss: 4747.5508, KL Divergence: 1608.2469\n",
            "Epoch[14/20], Step [680/938], Reconstruction Loss: 5377.5200, KL Divergence: 1649.0730\n",
            "Epoch[14/20], Step [690/938], Reconstruction Loss: 5074.8491, KL Divergence: 1630.4452\n",
            "Epoch[14/20], Step [700/938], Reconstruction Loss: 5156.4521, KL Divergence: 1622.2386\n",
            "Epoch[14/20], Step [710/938], Reconstruction Loss: 5332.3857, KL Divergence: 1612.3926\n",
            "Epoch[14/20], Step [720/938], Reconstruction Loss: 5013.1475, KL Divergence: 1696.5525\n",
            "Epoch[14/20], Step [730/938], Reconstruction Loss: 5030.1260, KL Divergence: 1545.8419\n",
            "Epoch[14/20], Step [740/938], Reconstruction Loss: 4999.6919, KL Divergence: 1650.2927\n",
            "Epoch[14/20], Step [750/938], Reconstruction Loss: 4859.1660, KL Divergence: 1610.0844\n",
            "Epoch[14/20], Step [760/938], Reconstruction Loss: 4922.4897, KL Divergence: 1561.8961\n",
            "Epoch[14/20], Step [770/938], Reconstruction Loss: 5263.2041, KL Divergence: 1729.3856\n",
            "Epoch[14/20], Step [780/938], Reconstruction Loss: 5064.6211, KL Divergence: 1618.7749\n",
            "Epoch[14/20], Step [790/938], Reconstruction Loss: 5172.5786, KL Divergence: 1653.1007\n",
            "Epoch[14/20], Step [800/938], Reconstruction Loss: 5116.6455, KL Divergence: 1643.4283\n",
            "Epoch[14/20], Step [810/938], Reconstruction Loss: 4840.3486, KL Divergence: 1611.5480\n",
            "Epoch[14/20], Step [820/938], Reconstruction Loss: 4914.4307, KL Divergence: 1592.6331\n",
            "Epoch[14/20], Step [830/938], Reconstruction Loss: 4931.6450, KL Divergence: 1596.6704\n",
            "Epoch[14/20], Step [840/938], Reconstruction Loss: 5339.4229, KL Divergence: 1656.1038\n",
            "Epoch[14/20], Step [850/938], Reconstruction Loss: 4802.0532, KL Divergence: 1562.3167\n",
            "Epoch[14/20], Step [860/938], Reconstruction Loss: 4783.9741, KL Divergence: 1586.9249\n",
            "Epoch[14/20], Step [870/938], Reconstruction Loss: 5025.8032, KL Divergence: 1661.1041\n",
            "Epoch[14/20], Step [880/938], Reconstruction Loss: 4874.6626, KL Divergence: 1640.8741\n",
            "Epoch[14/20], Step [890/938], Reconstruction Loss: 5327.9927, KL Divergence: 1599.8499\n",
            "Epoch[14/20], Step [900/938], Reconstruction Loss: 4837.6489, KL Divergence: 1580.3911\n",
            "Epoch[14/20], Step [910/938], Reconstruction Loss: 5007.4897, KL Divergence: 1638.4128\n",
            "Epoch[14/20], Step [920/938], Reconstruction Loss: 4981.0986, KL Divergence: 1587.7384\n",
            "Epoch[14/20], Step [930/938], Reconstruction Loss: 4942.9951, KL Divergence: 1666.3116\n",
            "Epoch[15/20], Step [10/938], Reconstruction Loss: 4919.9229, KL Divergence: 1625.6887\n",
            "Epoch[15/20], Step [20/938], Reconstruction Loss: 5433.9780, KL Divergence: 1654.5255\n",
            "Epoch[15/20], Step [30/938], Reconstruction Loss: 5025.8604, KL Divergence: 1620.6760\n",
            "Epoch[15/20], Step [40/938], Reconstruction Loss: 5030.7485, KL Divergence: 1623.0370\n",
            "Epoch[15/20], Step [50/938], Reconstruction Loss: 5190.2339, KL Divergence: 1701.0214\n",
            "Epoch[15/20], Step [60/938], Reconstruction Loss: 5210.5923, KL Divergence: 1639.7499\n",
            "Epoch[15/20], Step [70/938], Reconstruction Loss: 5391.1201, KL Divergence: 1686.6599\n",
            "Epoch[15/20], Step [80/938], Reconstruction Loss: 5215.4927, KL Divergence: 1633.1732\n",
            "Epoch[15/20], Step [90/938], Reconstruction Loss: 4958.3223, KL Divergence: 1664.7755\n",
            "Epoch[15/20], Step [100/938], Reconstruction Loss: 4909.7002, KL Divergence: 1631.8591\n",
            "Epoch[15/20], Step [110/938], Reconstruction Loss: 4894.5693, KL Divergence: 1574.8970\n",
            "Epoch[15/20], Step [120/938], Reconstruction Loss: 5398.2910, KL Divergence: 1675.7360\n",
            "Epoch[15/20], Step [130/938], Reconstruction Loss: 5165.6729, KL Divergence: 1712.6310\n",
            "Epoch[15/20], Step [140/938], Reconstruction Loss: 5263.0371, KL Divergence: 1596.2946\n",
            "Epoch[15/20], Step [150/938], Reconstruction Loss: 4998.7026, KL Divergence: 1652.8361\n",
            "Epoch[15/20], Step [160/938], Reconstruction Loss: 4962.7637, KL Divergence: 1561.5961\n",
            "Epoch[15/20], Step [170/938], Reconstruction Loss: 4934.9839, KL Divergence: 1612.6370\n",
            "Epoch[15/20], Step [180/938], Reconstruction Loss: 4928.3906, KL Divergence: 1590.1427\n",
            "Epoch[15/20], Step [190/938], Reconstruction Loss: 4994.0991, KL Divergence: 1667.4237\n",
            "Epoch[15/20], Step [200/938], Reconstruction Loss: 5311.8564, KL Divergence: 1598.4457\n",
            "Epoch[15/20], Step [210/938], Reconstruction Loss: 5034.9971, KL Divergence: 1687.9353\n",
            "Epoch[15/20], Step [220/938], Reconstruction Loss: 5010.4126, KL Divergence: 1601.3483\n",
            "Epoch[15/20], Step [230/938], Reconstruction Loss: 5215.8096, KL Divergence: 1680.9626\n",
            "Epoch[15/20], Step [240/938], Reconstruction Loss: 5065.2163, KL Divergence: 1572.6548\n",
            "Epoch[15/20], Step [250/938], Reconstruction Loss: 4983.1714, KL Divergence: 1712.3125\n",
            "Epoch[15/20], Step [260/938], Reconstruction Loss: 5312.8110, KL Divergence: 1643.0300\n",
            "Epoch[15/20], Step [270/938], Reconstruction Loss: 5304.7617, KL Divergence: 1656.5812\n",
            "Epoch[15/20], Step [280/938], Reconstruction Loss: 4921.7393, KL Divergence: 1660.8446\n",
            "Epoch[15/20], Step [290/938], Reconstruction Loss: 4886.8652, KL Divergence: 1603.8196\n",
            "Epoch[15/20], Step [300/938], Reconstruction Loss: 4957.6372, KL Divergence: 1639.9104\n",
            "Epoch[15/20], Step [310/938], Reconstruction Loss: 5067.0146, KL Divergence: 1637.0912\n",
            "Epoch[15/20], Step [320/938], Reconstruction Loss: 5138.0039, KL Divergence: 1629.4790\n",
            "Epoch[15/20], Step [330/938], Reconstruction Loss: 4765.0464, KL Divergence: 1646.8745\n",
            "Epoch[15/20], Step [340/938], Reconstruction Loss: 5195.5420, KL Divergence: 1674.2014\n",
            "Epoch[15/20], Step [350/938], Reconstruction Loss: 4820.7788, KL Divergence: 1638.0155\n",
            "Epoch[15/20], Step [360/938], Reconstruction Loss: 4697.7085, KL Divergence: 1599.8380\n",
            "Epoch[15/20], Step [370/938], Reconstruction Loss: 5136.0928, KL Divergence: 1622.1029\n",
            "Epoch[15/20], Step [380/938], Reconstruction Loss: 4895.1519, KL Divergence: 1667.7314\n",
            "Epoch[15/20], Step [390/938], Reconstruction Loss: 4939.4419, KL Divergence: 1597.5903\n",
            "Epoch[15/20], Step [400/938], Reconstruction Loss: 4852.0806, KL Divergence: 1576.1974\n",
            "Epoch[15/20], Step [410/938], Reconstruction Loss: 5318.5449, KL Divergence: 1723.2557\n",
            "Epoch[15/20], Step [420/938], Reconstruction Loss: 4931.8457, KL Divergence: 1623.4194\n",
            "Epoch[15/20], Step [430/938], Reconstruction Loss: 5130.4629, KL Divergence: 1648.3964\n",
            "Epoch[15/20], Step [440/938], Reconstruction Loss: 5276.6670, KL Divergence: 1645.6246\n",
            "Epoch[15/20], Step [450/938], Reconstruction Loss: 5299.8120, KL Divergence: 1736.1587\n",
            "Epoch[15/20], Step [460/938], Reconstruction Loss: 5191.7114, KL Divergence: 1585.6282\n",
            "Epoch[15/20], Step [470/938], Reconstruction Loss: 5341.7183, KL Divergence: 1695.4816\n",
            "Epoch[15/20], Step [480/938], Reconstruction Loss: 4851.9751, KL Divergence: 1665.1783\n",
            "Epoch[15/20], Step [490/938], Reconstruction Loss: 4950.6646, KL Divergence: 1573.0063\n",
            "Epoch[15/20], Step [500/938], Reconstruction Loss: 4764.3447, KL Divergence: 1633.6168\n",
            "Epoch[15/20], Step [510/938], Reconstruction Loss: 4823.8237, KL Divergence: 1560.7062\n",
            "Epoch[15/20], Step [520/938], Reconstruction Loss: 5129.5781, KL Divergence: 1649.2738\n",
            "Epoch[15/20], Step [530/938], Reconstruction Loss: 4981.9692, KL Divergence: 1626.2474\n",
            "Epoch[15/20], Step [540/938], Reconstruction Loss: 4994.5400, KL Divergence: 1608.6748\n",
            "Epoch[15/20], Step [550/938], Reconstruction Loss: 5181.2798, KL Divergence: 1692.4658\n",
            "Epoch[15/20], Step [560/938], Reconstruction Loss: 4852.0112, KL Divergence: 1604.9241\n",
            "Epoch[15/20], Step [570/938], Reconstruction Loss: 5203.5747, KL Divergence: 1621.4905\n",
            "Epoch[15/20], Step [580/938], Reconstruction Loss: 4905.1475, KL Divergence: 1595.6372\n",
            "Epoch[15/20], Step [590/938], Reconstruction Loss: 5253.4331, KL Divergence: 1658.5548\n",
            "Epoch[15/20], Step [600/938], Reconstruction Loss: 5311.6699, KL Divergence: 1679.0394\n",
            "Epoch[15/20], Step [610/938], Reconstruction Loss: 4843.5205, KL Divergence: 1601.6190\n",
            "Epoch[15/20], Step [620/938], Reconstruction Loss: 5023.5381, KL Divergence: 1565.8124\n",
            "Epoch[15/20], Step [630/938], Reconstruction Loss: 5321.7930, KL Divergence: 1680.2528\n",
            "Epoch[15/20], Step [640/938], Reconstruction Loss: 4754.2148, KL Divergence: 1591.5389\n",
            "Epoch[15/20], Step [650/938], Reconstruction Loss: 5071.8315, KL Divergence: 1631.1779\n",
            "Epoch[15/20], Step [660/938], Reconstruction Loss: 5382.6621, KL Divergence: 1668.9534\n",
            "Epoch[15/20], Step [670/938], Reconstruction Loss: 5259.1558, KL Divergence: 1605.0251\n",
            "Epoch[15/20], Step [680/938], Reconstruction Loss: 4989.3389, KL Divergence: 1621.9554\n",
            "Epoch[15/20], Step [690/938], Reconstruction Loss: 5169.4468, KL Divergence: 1674.6189\n",
            "Epoch[15/20], Step [700/938], Reconstruction Loss: 4884.3770, KL Divergence: 1609.2010\n",
            "Epoch[15/20], Step [710/938], Reconstruction Loss: 5077.9272, KL Divergence: 1677.9924\n",
            "Epoch[15/20], Step [720/938], Reconstruction Loss: 4870.0176, KL Divergence: 1597.0104\n",
            "Epoch[15/20], Step [730/938], Reconstruction Loss: 5475.3169, KL Divergence: 1607.1281\n",
            "Epoch[15/20], Step [740/938], Reconstruction Loss: 5165.3481, KL Divergence: 1708.9709\n",
            "Epoch[15/20], Step [750/938], Reconstruction Loss: 5106.3472, KL Divergence: 1625.9829\n",
            "Epoch[15/20], Step [760/938], Reconstruction Loss: 5107.9731, KL Divergence: 1625.9756\n",
            "Epoch[15/20], Step [770/938], Reconstruction Loss: 5259.9941, KL Divergence: 1600.9343\n",
            "Epoch[15/20], Step [780/938], Reconstruction Loss: 4746.6699, KL Divergence: 1596.0284\n",
            "Epoch[15/20], Step [790/938], Reconstruction Loss: 5242.8345, KL Divergence: 1643.5101\n",
            "Epoch[15/20], Step [800/938], Reconstruction Loss: 5263.2793, KL Divergence: 1680.9761\n",
            "Epoch[15/20], Step [810/938], Reconstruction Loss: 5466.6782, KL Divergence: 1659.5563\n",
            "Epoch[15/20], Step [820/938], Reconstruction Loss: 4966.5840, KL Divergence: 1595.2650\n",
            "Epoch[15/20], Step [830/938], Reconstruction Loss: 5132.1914, KL Divergence: 1621.1603\n",
            "Epoch[15/20], Step [840/938], Reconstruction Loss: 4908.9771, KL Divergence: 1624.0013\n",
            "Epoch[15/20], Step [850/938], Reconstruction Loss: 5502.4946, KL Divergence: 1628.1499\n",
            "Epoch[15/20], Step [860/938], Reconstruction Loss: 5211.3369, KL Divergence: 1633.9281\n",
            "Epoch[15/20], Step [870/938], Reconstruction Loss: 5000.6694, KL Divergence: 1623.9771\n",
            "Epoch[15/20], Step [880/938], Reconstruction Loss: 5069.9512, KL Divergence: 1667.8960\n",
            "Epoch[15/20], Step [890/938], Reconstruction Loss: 5004.1157, KL Divergence: 1651.6908\n",
            "Epoch[15/20], Step [900/938], Reconstruction Loss: 5059.6025, KL Divergence: 1648.1068\n",
            "Epoch[15/20], Step [910/938], Reconstruction Loss: 5120.4712, KL Divergence: 1631.3473\n",
            "Epoch[15/20], Step [920/938], Reconstruction Loss: 5118.0591, KL Divergence: 1600.9292\n",
            "Epoch[15/20], Step [930/938], Reconstruction Loss: 4834.3911, KL Divergence: 1570.8689\n",
            "Epoch[16/20], Step [10/938], Reconstruction Loss: 5100.5449, KL Divergence: 1660.5363\n",
            "Epoch[16/20], Step [20/938], Reconstruction Loss: 5276.2607, KL Divergence: 1642.4800\n",
            "Epoch[16/20], Step [30/938], Reconstruction Loss: 5113.2646, KL Divergence: 1711.6112\n",
            "Epoch[16/20], Step [40/938], Reconstruction Loss: 4595.1953, KL Divergence: 1537.8889\n",
            "Epoch[16/20], Step [50/938], Reconstruction Loss: 5115.1987, KL Divergence: 1641.5271\n",
            "Epoch[16/20], Step [60/938], Reconstruction Loss: 5144.9048, KL Divergence: 1619.9502\n",
            "Epoch[16/20], Step [70/938], Reconstruction Loss: 5311.5767, KL Divergence: 1657.9720\n",
            "Epoch[16/20], Step [80/938], Reconstruction Loss: 4818.0576, KL Divergence: 1613.4899\n",
            "Epoch[16/20], Step [90/938], Reconstruction Loss: 4941.2427, KL Divergence: 1610.3474\n",
            "Epoch[16/20], Step [100/938], Reconstruction Loss: 5047.0435, KL Divergence: 1639.8569\n",
            "Epoch[16/20], Step [110/938], Reconstruction Loss: 5041.0801, KL Divergence: 1617.6194\n",
            "Epoch[16/20], Step [120/938], Reconstruction Loss: 5181.5674, KL Divergence: 1662.8837\n",
            "Epoch[16/20], Step [130/938], Reconstruction Loss: 4608.0552, KL Divergence: 1606.7191\n",
            "Epoch[16/20], Step [140/938], Reconstruction Loss: 4985.1880, KL Divergence: 1606.4362\n",
            "Epoch[16/20], Step [150/938], Reconstruction Loss: 4789.9053, KL Divergence: 1643.6180\n",
            "Epoch[16/20], Step [160/938], Reconstruction Loss: 4696.2466, KL Divergence: 1586.2026\n",
            "Epoch[16/20], Step [170/938], Reconstruction Loss: 4995.9478, KL Divergence: 1649.9298\n",
            "Epoch[16/20], Step [180/938], Reconstruction Loss: 5144.1089, KL Divergence: 1658.2990\n",
            "Epoch[16/20], Step [190/938], Reconstruction Loss: 4863.4053, KL Divergence: 1612.1681\n",
            "Epoch[16/20], Step [200/938], Reconstruction Loss: 5059.0225, KL Divergence: 1605.1207\n",
            "Epoch[16/20], Step [210/938], Reconstruction Loss: 5107.6050, KL Divergence: 1666.5953\n",
            "Epoch[16/20], Step [220/938], Reconstruction Loss: 5091.1675, KL Divergence: 1585.8867\n",
            "Epoch[16/20], Step [230/938], Reconstruction Loss: 4816.0669, KL Divergence: 1641.8408\n",
            "Epoch[16/20], Step [240/938], Reconstruction Loss: 4945.9800, KL Divergence: 1559.8740\n",
            "Epoch[16/20], Step [250/938], Reconstruction Loss: 4877.0571, KL Divergence: 1609.4126\n",
            "Epoch[16/20], Step [260/938], Reconstruction Loss: 5225.1372, KL Divergence: 1665.0610\n",
            "Epoch[16/20], Step [270/938], Reconstruction Loss: 4897.0029, KL Divergence: 1674.9634\n",
            "Epoch[16/20], Step [280/938], Reconstruction Loss: 5121.5742, KL Divergence: 1670.3529\n",
            "Epoch[16/20], Step [290/938], Reconstruction Loss: 5290.0337, KL Divergence: 1692.0941\n",
            "Epoch[16/20], Step [300/938], Reconstruction Loss: 4889.1021, KL Divergence: 1584.3605\n",
            "Epoch[16/20], Step [310/938], Reconstruction Loss: 5154.3726, KL Divergence: 1624.6801\n",
            "Epoch[16/20], Step [320/938], Reconstruction Loss: 4827.7891, KL Divergence: 1625.3652\n",
            "Epoch[16/20], Step [330/938], Reconstruction Loss: 5221.5186, KL Divergence: 1684.6047\n",
            "Epoch[16/20], Step [340/938], Reconstruction Loss: 4941.9644, KL Divergence: 1585.1073\n",
            "Epoch[16/20], Step [350/938], Reconstruction Loss: 5310.1968, KL Divergence: 1742.4337\n",
            "Epoch[16/20], Step [360/938], Reconstruction Loss: 4953.1138, KL Divergence: 1639.0591\n",
            "Epoch[16/20], Step [370/938], Reconstruction Loss: 5021.5117, KL Divergence: 1657.8346\n",
            "Epoch[16/20], Step [380/938], Reconstruction Loss: 4958.0527, KL Divergence: 1622.0006\n",
            "Epoch[16/20], Step [390/938], Reconstruction Loss: 5109.6729, KL Divergence: 1648.2654\n",
            "Epoch[16/20], Step [400/938], Reconstruction Loss: 4913.0381, KL Divergence: 1630.6572\n",
            "Epoch[16/20], Step [410/938], Reconstruction Loss: 4901.5947, KL Divergence: 1617.6617\n",
            "Epoch[16/20], Step [420/938], Reconstruction Loss: 4743.1523, KL Divergence: 1643.5905\n",
            "Epoch[16/20], Step [430/938], Reconstruction Loss: 4970.9902, KL Divergence: 1636.5259\n",
            "Epoch[16/20], Step [440/938], Reconstruction Loss: 4807.7651, KL Divergence: 1537.1519\n",
            "Epoch[16/20], Step [450/938], Reconstruction Loss: 4777.7617, KL Divergence: 1636.4646\n",
            "Epoch[16/20], Step [460/938], Reconstruction Loss: 4828.4976, KL Divergence: 1585.5889\n",
            "Epoch[16/20], Step [470/938], Reconstruction Loss: 5323.6631, KL Divergence: 1699.5282\n",
            "Epoch[16/20], Step [480/938], Reconstruction Loss: 5432.5576, KL Divergence: 1648.7574\n",
            "Epoch[16/20], Step [490/938], Reconstruction Loss: 5027.3579, KL Divergence: 1652.5724\n",
            "Epoch[16/20], Step [500/938], Reconstruction Loss: 5089.6133, KL Divergence: 1653.6819\n",
            "Epoch[16/20], Step [510/938], Reconstruction Loss: 4848.2661, KL Divergence: 1572.0284\n",
            "Epoch[16/20], Step [520/938], Reconstruction Loss: 5163.0063, KL Divergence: 1669.6594\n",
            "Epoch[16/20], Step [530/938], Reconstruction Loss: 4982.4805, KL Divergence: 1578.6434\n",
            "Epoch[16/20], Step [540/938], Reconstruction Loss: 4997.3970, KL Divergence: 1619.7415\n",
            "Epoch[16/20], Step [550/938], Reconstruction Loss: 5336.2310, KL Divergence: 1656.9810\n",
            "Epoch[16/20], Step [560/938], Reconstruction Loss: 5086.3901, KL Divergence: 1626.0413\n",
            "Epoch[16/20], Step [570/938], Reconstruction Loss: 4991.5337, KL Divergence: 1641.6213\n",
            "Epoch[16/20], Step [580/938], Reconstruction Loss: 5124.0820, KL Divergence: 1613.9976\n",
            "Epoch[16/20], Step [590/938], Reconstruction Loss: 4927.6626, KL Divergence: 1580.8250\n",
            "Epoch[16/20], Step [600/938], Reconstruction Loss: 5027.9731, KL Divergence: 1660.5862\n",
            "Epoch[16/20], Step [610/938], Reconstruction Loss: 5079.9746, KL Divergence: 1550.0867\n",
            "Epoch[16/20], Step [620/938], Reconstruction Loss: 5155.5469, KL Divergence: 1673.0291\n",
            "Epoch[16/20], Step [630/938], Reconstruction Loss: 4715.9873, KL Divergence: 1626.7631\n",
            "Epoch[16/20], Step [640/938], Reconstruction Loss: 5139.8784, KL Divergence: 1563.4237\n",
            "Epoch[16/20], Step [650/938], Reconstruction Loss: 4848.0376, KL Divergence: 1682.8843\n",
            "Epoch[16/20], Step [660/938], Reconstruction Loss: 4989.1001, KL Divergence: 1638.5541\n",
            "Epoch[16/20], Step [670/938], Reconstruction Loss: 5087.6943, KL Divergence: 1581.0844\n",
            "Epoch[16/20], Step [680/938], Reconstruction Loss: 4770.8110, KL Divergence: 1612.1693\n",
            "Epoch[16/20], Step [690/938], Reconstruction Loss: 4823.4185, KL Divergence: 1597.2617\n",
            "Epoch[16/20], Step [700/938], Reconstruction Loss: 5149.2266, KL Divergence: 1702.6830\n",
            "Epoch[16/20], Step [710/938], Reconstruction Loss: 5066.7944, KL Divergence: 1616.8622\n",
            "Epoch[16/20], Step [720/938], Reconstruction Loss: 5040.5601, KL Divergence: 1691.8282\n",
            "Epoch[16/20], Step [730/938], Reconstruction Loss: 5014.1758, KL Divergence: 1636.4357\n",
            "Epoch[16/20], Step [740/938], Reconstruction Loss: 4984.5317, KL Divergence: 1626.7443\n",
            "Epoch[16/20], Step [750/938], Reconstruction Loss: 4943.5913, KL Divergence: 1516.9357\n",
            "Epoch[16/20], Step [760/938], Reconstruction Loss: 5007.8750, KL Divergence: 1696.2736\n",
            "Epoch[16/20], Step [770/938], Reconstruction Loss: 5216.5806, KL Divergence: 1669.1871\n",
            "Epoch[16/20], Step [780/938], Reconstruction Loss: 5075.2803, KL Divergence: 1648.8805\n",
            "Epoch[16/20], Step [790/938], Reconstruction Loss: 4985.3921, KL Divergence: 1642.7590\n",
            "Epoch[16/20], Step [800/938], Reconstruction Loss: 5168.2485, KL Divergence: 1659.6564\n",
            "Epoch[16/20], Step [810/938], Reconstruction Loss: 5135.4878, KL Divergence: 1636.9448\n",
            "Epoch[16/20], Step [820/938], Reconstruction Loss: 5294.1289, KL Divergence: 1725.2760\n",
            "Epoch[16/20], Step [830/938], Reconstruction Loss: 4667.3892, KL Divergence: 1612.3705\n",
            "Epoch[16/20], Step [840/938], Reconstruction Loss: 4871.7041, KL Divergence: 1598.0845\n",
            "Epoch[16/20], Step [850/938], Reconstruction Loss: 5011.1284, KL Divergence: 1669.1439\n",
            "Epoch[16/20], Step [860/938], Reconstruction Loss: 4863.2476, KL Divergence: 1621.4962\n",
            "Epoch[16/20], Step [870/938], Reconstruction Loss: 4993.6641, KL Divergence: 1603.3857\n",
            "Epoch[16/20], Step [880/938], Reconstruction Loss: 5031.3584, KL Divergence: 1674.5867\n",
            "Epoch[16/20], Step [890/938], Reconstruction Loss: 5188.2271, KL Divergence: 1638.9598\n",
            "Epoch[16/20], Step [900/938], Reconstruction Loss: 5100.3037, KL Divergence: 1715.7723\n",
            "Epoch[16/20], Step [910/938], Reconstruction Loss: 5067.0835, KL Divergence: 1609.4161\n",
            "Epoch[16/20], Step [920/938], Reconstruction Loss: 5066.4409, KL Divergence: 1635.3025\n",
            "Epoch[16/20], Step [930/938], Reconstruction Loss: 4994.4136, KL Divergence: 1659.2666\n",
            "Epoch[17/20], Step [10/938], Reconstruction Loss: 4880.0137, KL Divergence: 1660.7615\n",
            "Epoch[17/20], Step [20/938], Reconstruction Loss: 5020.6509, KL Divergence: 1629.6034\n",
            "Epoch[17/20], Step [30/938], Reconstruction Loss: 4898.9595, KL Divergence: 1674.8820\n",
            "Epoch[17/20], Step [40/938], Reconstruction Loss: 5004.7427, KL Divergence: 1533.2096\n",
            "Epoch[17/20], Step [50/938], Reconstruction Loss: 5433.4971, KL Divergence: 1670.3092\n",
            "Epoch[17/20], Step [60/938], Reconstruction Loss: 5015.0498, KL Divergence: 1607.5690\n",
            "Epoch[17/20], Step [70/938], Reconstruction Loss: 5200.1494, KL Divergence: 1689.6471\n",
            "Epoch[17/20], Step [80/938], Reconstruction Loss: 5183.1040, KL Divergence: 1669.0822\n",
            "Epoch[17/20], Step [90/938], Reconstruction Loss: 4988.5552, KL Divergence: 1618.2319\n",
            "Epoch[17/20], Step [100/938], Reconstruction Loss: 5145.4380, KL Divergence: 1744.8619\n",
            "Epoch[17/20], Step [110/938], Reconstruction Loss: 5087.5112, KL Divergence: 1706.9083\n",
            "Epoch[17/20], Step [120/938], Reconstruction Loss: 5146.9951, KL Divergence: 1674.0526\n",
            "Epoch[17/20], Step [130/938], Reconstruction Loss: 5119.4717, KL Divergence: 1593.4796\n",
            "Epoch[17/20], Step [140/938], Reconstruction Loss: 4771.5830, KL Divergence: 1571.5852\n",
            "Epoch[17/20], Step [150/938], Reconstruction Loss: 5017.8677, KL Divergence: 1641.7539\n",
            "Epoch[17/20], Step [160/938], Reconstruction Loss: 5017.9590, KL Divergence: 1649.7058\n",
            "Epoch[17/20], Step [170/938], Reconstruction Loss: 4958.8267, KL Divergence: 1625.6830\n",
            "Epoch[17/20], Step [180/938], Reconstruction Loss: 4953.1328, KL Divergence: 1630.4376\n",
            "Epoch[17/20], Step [190/938], Reconstruction Loss: 4949.2368, KL Divergence: 1632.2195\n",
            "Epoch[17/20], Step [200/938], Reconstruction Loss: 5340.2808, KL Divergence: 1684.2837\n",
            "Epoch[17/20], Step [210/938], Reconstruction Loss: 4863.6294, KL Divergence: 1603.4226\n",
            "Epoch[17/20], Step [220/938], Reconstruction Loss: 4905.2769, KL Divergence: 1601.3262\n",
            "Epoch[17/20], Step [230/938], Reconstruction Loss: 4801.4360, KL Divergence: 1599.7051\n",
            "Epoch[17/20], Step [240/938], Reconstruction Loss: 4919.6714, KL Divergence: 1585.4025\n",
            "Epoch[17/20], Step [250/938], Reconstruction Loss: 4883.0425, KL Divergence: 1604.1234\n",
            "Epoch[17/20], Step [260/938], Reconstruction Loss: 4739.9062, KL Divergence: 1542.0310\n",
            "Epoch[17/20], Step [270/938], Reconstruction Loss: 5344.5229, KL Divergence: 1716.5006\n",
            "Epoch[17/20], Step [280/938], Reconstruction Loss: 5174.2471, KL Divergence: 1631.5406\n",
            "Epoch[17/20], Step [290/938], Reconstruction Loss: 5020.8877, KL Divergence: 1654.1915\n",
            "Epoch[17/20], Step [300/938], Reconstruction Loss: 4670.3770, KL Divergence: 1575.8875\n",
            "Epoch[17/20], Step [310/938], Reconstruction Loss: 5112.4482, KL Divergence: 1602.7209\n",
            "Epoch[17/20], Step [320/938], Reconstruction Loss: 4857.3594, KL Divergence: 1631.7908\n",
            "Epoch[17/20], Step [330/938], Reconstruction Loss: 5502.2012, KL Divergence: 1663.5483\n",
            "Epoch[17/20], Step [340/938], Reconstruction Loss: 5172.8184, KL Divergence: 1674.7174\n",
            "Epoch[17/20], Step [350/938], Reconstruction Loss: 5130.5913, KL Divergence: 1618.4396\n",
            "Epoch[17/20], Step [360/938], Reconstruction Loss: 5059.8354, KL Divergence: 1594.2346\n",
            "Epoch[17/20], Step [370/938], Reconstruction Loss: 4781.0488, KL Divergence: 1573.9862\n",
            "Epoch[17/20], Step [380/938], Reconstruction Loss: 4632.6758, KL Divergence: 1600.8967\n",
            "Epoch[17/20], Step [390/938], Reconstruction Loss: 4877.2842, KL Divergence: 1578.4440\n",
            "Epoch[17/20], Step [400/938], Reconstruction Loss: 4845.4668, KL Divergence: 1582.1202\n",
            "Epoch[17/20], Step [410/938], Reconstruction Loss: 5094.8350, KL Divergence: 1672.8885\n",
            "Epoch[17/20], Step [420/938], Reconstruction Loss: 4997.3770, KL Divergence: 1620.8068\n",
            "Epoch[17/20], Step [430/938], Reconstruction Loss: 4918.5415, KL Divergence: 1668.9630\n",
            "Epoch[17/20], Step [440/938], Reconstruction Loss: 4952.9058, KL Divergence: 1643.5764\n",
            "Epoch[17/20], Step [450/938], Reconstruction Loss: 5395.2393, KL Divergence: 1701.7045\n",
            "Epoch[17/20], Step [460/938], Reconstruction Loss: 4982.0205, KL Divergence: 1630.6483\n",
            "Epoch[17/20], Step [470/938], Reconstruction Loss: 5320.3667, KL Divergence: 1643.5071\n",
            "Epoch[17/20], Step [480/938], Reconstruction Loss: 5231.0674, KL Divergence: 1650.3184\n",
            "Epoch[17/20], Step [490/938], Reconstruction Loss: 4992.8906, KL Divergence: 1647.8348\n",
            "Epoch[17/20], Step [500/938], Reconstruction Loss: 5220.5762, KL Divergence: 1687.3497\n",
            "Epoch[17/20], Step [510/938], Reconstruction Loss: 5066.7803, KL Divergence: 1634.3665\n",
            "Epoch[17/20], Step [520/938], Reconstruction Loss: 4893.2881, KL Divergence: 1652.9641\n",
            "Epoch[17/20], Step [530/938], Reconstruction Loss: 4784.8286, KL Divergence: 1586.4531\n",
            "Epoch[17/20], Step [540/938], Reconstruction Loss: 4802.9551, KL Divergence: 1565.8304\n",
            "Epoch[17/20], Step [550/938], Reconstruction Loss: 5271.2471, KL Divergence: 1742.9624\n",
            "Epoch[17/20], Step [560/938], Reconstruction Loss: 4712.9355, KL Divergence: 1537.6223\n",
            "Epoch[17/20], Step [570/938], Reconstruction Loss: 5063.9526, KL Divergence: 1674.6981\n",
            "Epoch[17/20], Step [580/938], Reconstruction Loss: 5412.3247, KL Divergence: 1624.3018\n",
            "Epoch[17/20], Step [590/938], Reconstruction Loss: 5234.1851, KL Divergence: 1680.8770\n",
            "Epoch[17/20], Step [600/938], Reconstruction Loss: 4877.9688, KL Divergence: 1600.4314\n",
            "Epoch[17/20], Step [610/938], Reconstruction Loss: 4927.7671, KL Divergence: 1666.9961\n",
            "Epoch[17/20], Step [620/938], Reconstruction Loss: 5117.5801, KL Divergence: 1621.7285\n",
            "Epoch[17/20], Step [630/938], Reconstruction Loss: 5087.1348, KL Divergence: 1708.4203\n",
            "Epoch[17/20], Step [640/938], Reconstruction Loss: 4891.4087, KL Divergence: 1578.0488\n",
            "Epoch[17/20], Step [650/938], Reconstruction Loss: 5037.2310, KL Divergence: 1636.2838\n",
            "Epoch[17/20], Step [660/938], Reconstruction Loss: 5125.9014, KL Divergence: 1664.4647\n",
            "Epoch[17/20], Step [670/938], Reconstruction Loss: 5287.4907, KL Divergence: 1629.3959\n",
            "Epoch[17/20], Step [680/938], Reconstruction Loss: 4790.9268, KL Divergence: 1641.2555\n",
            "Epoch[17/20], Step [690/938], Reconstruction Loss: 5008.0981, KL Divergence: 1573.6769\n",
            "Epoch[17/20], Step [700/938], Reconstruction Loss: 5271.2539, KL Divergence: 1659.4442\n",
            "Epoch[17/20], Step [710/938], Reconstruction Loss: 4937.1836, KL Divergence: 1619.2467\n",
            "Epoch[17/20], Step [720/938], Reconstruction Loss: 5078.2031, KL Divergence: 1647.0452\n",
            "Epoch[17/20], Step [730/938], Reconstruction Loss: 5138.8711, KL Divergence: 1633.8119\n",
            "Epoch[17/20], Step [740/938], Reconstruction Loss: 5053.7417, KL Divergence: 1670.4342\n",
            "Epoch[17/20], Step [750/938], Reconstruction Loss: 5097.8491, KL Divergence: 1656.4795\n",
            "Epoch[17/20], Step [760/938], Reconstruction Loss: 5037.4219, KL Divergence: 1625.3461\n",
            "Epoch[17/20], Step [770/938], Reconstruction Loss: 4890.0967, KL Divergence: 1676.5708\n",
            "Epoch[17/20], Step [780/938], Reconstruction Loss: 5189.8613, KL Divergence: 1617.7126\n",
            "Epoch[17/20], Step [790/938], Reconstruction Loss: 4889.1885, KL Divergence: 1636.3093\n",
            "Epoch[17/20], Step [800/938], Reconstruction Loss: 5004.9644, KL Divergence: 1682.7758\n",
            "Epoch[17/20], Step [810/938], Reconstruction Loss: 4912.7905, KL Divergence: 1568.3058\n",
            "Epoch[17/20], Step [820/938], Reconstruction Loss: 5135.8564, KL Divergence: 1648.7864\n",
            "Epoch[17/20], Step [830/938], Reconstruction Loss: 5437.9692, KL Divergence: 1625.9951\n",
            "Epoch[17/20], Step [840/938], Reconstruction Loss: 5435.7148, KL Divergence: 1694.2117\n",
            "Epoch[17/20], Step [850/938], Reconstruction Loss: 5143.3477, KL Divergence: 1649.9952\n",
            "Epoch[17/20], Step [860/938], Reconstruction Loss: 4951.3140, KL Divergence: 1610.8959\n",
            "Epoch[17/20], Step [870/938], Reconstruction Loss: 4919.1353, KL Divergence: 1594.7396\n",
            "Epoch[17/20], Step [880/938], Reconstruction Loss: 5064.0903, KL Divergence: 1607.7418\n",
            "Epoch[17/20], Step [890/938], Reconstruction Loss: 4850.3701, KL Divergence: 1626.4789\n",
            "Epoch[17/20], Step [900/938], Reconstruction Loss: 4754.0269, KL Divergence: 1571.3470\n",
            "Epoch[17/20], Step [910/938], Reconstruction Loss: 4982.0264, KL Divergence: 1666.2258\n",
            "Epoch[17/20], Step [920/938], Reconstruction Loss: 5318.0879, KL Divergence: 1657.7953\n",
            "Epoch[17/20], Step [930/938], Reconstruction Loss: 5251.4292, KL Divergence: 1628.2710\n",
            "Epoch[18/20], Step [10/938], Reconstruction Loss: 4808.2754, KL Divergence: 1682.2411\n",
            "Epoch[18/20], Step [20/938], Reconstruction Loss: 4687.2612, KL Divergence: 1604.3474\n",
            "Epoch[18/20], Step [30/938], Reconstruction Loss: 5098.8354, KL Divergence: 1682.6173\n",
            "Epoch[18/20], Step [40/938], Reconstruction Loss: 5025.1782, KL Divergence: 1600.6177\n",
            "Epoch[18/20], Step [50/938], Reconstruction Loss: 4808.0845, KL Divergence: 1682.2609\n",
            "Epoch[18/20], Step [60/938], Reconstruction Loss: 4814.7388, KL Divergence: 1577.9323\n",
            "Epoch[18/20], Step [70/938], Reconstruction Loss: 5019.0366, KL Divergence: 1668.6697\n",
            "Epoch[18/20], Step [80/938], Reconstruction Loss: 5149.4194, KL Divergence: 1606.1008\n",
            "Epoch[18/20], Step [90/938], Reconstruction Loss: 4630.8896, KL Divergence: 1621.2778\n",
            "Epoch[18/20], Step [100/938], Reconstruction Loss: 5452.6948, KL Divergence: 1687.7338\n",
            "Epoch[18/20], Step [110/938], Reconstruction Loss: 5071.1533, KL Divergence: 1703.5355\n",
            "Epoch[18/20], Step [120/938], Reconstruction Loss: 5187.9756, KL Divergence: 1671.7235\n",
            "Epoch[18/20], Step [130/938], Reconstruction Loss: 5081.9111, KL Divergence: 1617.9119\n",
            "Epoch[18/20], Step [140/938], Reconstruction Loss: 4602.2568, KL Divergence: 1524.8064\n",
            "Epoch[18/20], Step [150/938], Reconstruction Loss: 4659.8735, KL Divergence: 1680.9293\n",
            "Epoch[18/20], Step [160/938], Reconstruction Loss: 5273.6499, KL Divergence: 1597.8942\n",
            "Epoch[18/20], Step [170/938], Reconstruction Loss: 4885.0894, KL Divergence: 1658.3961\n",
            "Epoch[18/20], Step [180/938], Reconstruction Loss: 5169.5679, KL Divergence: 1651.7957\n",
            "Epoch[18/20], Step [190/938], Reconstruction Loss: 4936.3335, KL Divergence: 1658.5148\n",
            "Epoch[18/20], Step [200/938], Reconstruction Loss: 5035.0972, KL Divergence: 1685.5486\n",
            "Epoch[18/20], Step [210/938], Reconstruction Loss: 4877.4990, KL Divergence: 1590.0298\n",
            "Epoch[18/20], Step [220/938], Reconstruction Loss: 4800.5952, KL Divergence: 1601.0719\n",
            "Epoch[18/20], Step [230/938], Reconstruction Loss: 5324.7446, KL Divergence: 1695.6591\n",
            "Epoch[18/20], Step [240/938], Reconstruction Loss: 4995.2793, KL Divergence: 1596.8513\n",
            "Epoch[18/20], Step [250/938], Reconstruction Loss: 5173.3091, KL Divergence: 1668.5441\n",
            "Epoch[18/20], Step [260/938], Reconstruction Loss: 5414.2310, KL Divergence: 1670.9021\n",
            "Epoch[18/20], Step [270/938], Reconstruction Loss: 5168.8779, KL Divergence: 1637.2399\n",
            "Epoch[18/20], Step [280/938], Reconstruction Loss: 4931.2876, KL Divergence: 1651.1118\n",
            "Epoch[18/20], Step [290/938], Reconstruction Loss: 4896.6587, KL Divergence: 1639.5111\n",
            "Epoch[18/20], Step [300/938], Reconstruction Loss: 4901.5693, KL Divergence: 1603.9542\n",
            "Epoch[18/20], Step [310/938], Reconstruction Loss: 5298.1997, KL Divergence: 1707.6945\n",
            "Epoch[18/20], Step [320/938], Reconstruction Loss: 4929.4658, KL Divergence: 1643.3447\n",
            "Epoch[18/20], Step [330/938], Reconstruction Loss: 5071.8667, KL Divergence: 1652.1637\n",
            "Epoch[18/20], Step [340/938], Reconstruction Loss: 4603.2861, KL Divergence: 1514.9230\n",
            "Epoch[18/20], Step [350/938], Reconstruction Loss: 5086.0986, KL Divergence: 1719.0210\n",
            "Epoch[18/20], Step [360/938], Reconstruction Loss: 5059.9175, KL Divergence: 1702.7683\n",
            "Epoch[18/20], Step [370/938], Reconstruction Loss: 4972.2017, KL Divergence: 1699.5879\n",
            "Epoch[18/20], Step [380/938], Reconstruction Loss: 4792.1313, KL Divergence: 1548.5125\n",
            "Epoch[18/20], Step [390/938], Reconstruction Loss: 5197.7261, KL Divergence: 1699.0264\n",
            "Epoch[18/20], Step [400/938], Reconstruction Loss: 5345.2144, KL Divergence: 1681.5021\n",
            "Epoch[18/20], Step [410/938], Reconstruction Loss: 4762.9097, KL Divergence: 1627.8550\n",
            "Epoch[18/20], Step [420/938], Reconstruction Loss: 4993.7290, KL Divergence: 1623.0464\n",
            "Epoch[18/20], Step [430/938], Reconstruction Loss: 4718.2104, KL Divergence: 1634.4990\n",
            "Epoch[18/20], Step [440/938], Reconstruction Loss: 4992.2881, KL Divergence: 1595.1888\n",
            "Epoch[18/20], Step [450/938], Reconstruction Loss: 4943.9414, KL Divergence: 1681.7151\n",
            "Epoch[18/20], Step [460/938], Reconstruction Loss: 5103.6777, KL Divergence: 1639.5916\n",
            "Epoch[18/20], Step [470/938], Reconstruction Loss: 5368.1313, KL Divergence: 1654.9664\n",
            "Epoch[18/20], Step [480/938], Reconstruction Loss: 5056.0854, KL Divergence: 1645.3025\n",
            "Epoch[18/20], Step [490/938], Reconstruction Loss: 5016.9629, KL Divergence: 1627.1595\n",
            "Epoch[18/20], Step [500/938], Reconstruction Loss: 5331.8584, KL Divergence: 1687.8383\n",
            "Epoch[18/20], Step [510/938], Reconstruction Loss: 4830.7539, KL Divergence: 1641.0671\n",
            "Epoch[18/20], Step [520/938], Reconstruction Loss: 4838.9365, KL Divergence: 1621.9374\n",
            "Epoch[18/20], Step [530/938], Reconstruction Loss: 4976.3789, KL Divergence: 1620.6986\n",
            "Epoch[18/20], Step [540/938], Reconstruction Loss: 5133.5742, KL Divergence: 1622.4613\n",
            "Epoch[18/20], Step [550/938], Reconstruction Loss: 5296.7710, KL Divergence: 1649.3630\n",
            "Epoch[18/20], Step [560/938], Reconstruction Loss: 5124.4814, KL Divergence: 1715.7604\n",
            "Epoch[18/20], Step [570/938], Reconstruction Loss: 5019.4961, KL Divergence: 1635.4213\n",
            "Epoch[18/20], Step [580/938], Reconstruction Loss: 5188.2627, KL Divergence: 1700.7526\n",
            "Epoch[18/20], Step [590/938], Reconstruction Loss: 5125.2603, KL Divergence: 1602.8145\n",
            "Epoch[18/20], Step [600/938], Reconstruction Loss: 5305.9302, KL Divergence: 1644.0934\n",
            "Epoch[18/20], Step [610/938], Reconstruction Loss: 4994.9434, KL Divergence: 1639.7716\n",
            "Epoch[18/20], Step [620/938], Reconstruction Loss: 5063.5581, KL Divergence: 1665.9771\n",
            "Epoch[18/20], Step [630/938], Reconstruction Loss: 4724.8491, KL Divergence: 1627.7156\n",
            "Epoch[18/20], Step [640/938], Reconstruction Loss: 5028.9390, KL Divergence: 1709.4342\n",
            "Epoch[18/20], Step [650/938], Reconstruction Loss: 4822.5391, KL Divergence: 1590.6631\n",
            "Epoch[18/20], Step [660/938], Reconstruction Loss: 4772.8496, KL Divergence: 1590.6062\n",
            "Epoch[18/20], Step [670/938], Reconstruction Loss: 4923.1636, KL Divergence: 1635.4517\n",
            "Epoch[18/20], Step [680/938], Reconstruction Loss: 4986.9663, KL Divergence: 1618.6833\n",
            "Epoch[18/20], Step [690/938], Reconstruction Loss: 5116.7988, KL Divergence: 1650.1906\n",
            "Epoch[18/20], Step [700/938], Reconstruction Loss: 4897.0981, KL Divergence: 1620.5134\n",
            "Epoch[18/20], Step [710/938], Reconstruction Loss: 4988.0024, KL Divergence: 1632.6351\n",
            "Epoch[18/20], Step [720/938], Reconstruction Loss: 5301.9619, KL Divergence: 1621.8019\n",
            "Epoch[18/20], Step [730/938], Reconstruction Loss: 4726.1587, KL Divergence: 1544.4224\n",
            "Epoch[18/20], Step [740/938], Reconstruction Loss: 5212.4473, KL Divergence: 1640.2990\n",
            "Epoch[18/20], Step [750/938], Reconstruction Loss: 5184.3101, KL Divergence: 1649.6600\n",
            "Epoch[18/20], Step [760/938], Reconstruction Loss: 4898.9800, KL Divergence: 1690.5089\n",
            "Epoch[18/20], Step [770/938], Reconstruction Loss: 4757.4346, KL Divergence: 1658.5784\n",
            "Epoch[18/20], Step [780/938], Reconstruction Loss: 5174.0581, KL Divergence: 1648.8143\n",
            "Epoch[18/20], Step [790/938], Reconstruction Loss: 5312.1050, KL Divergence: 1651.2922\n",
            "Epoch[18/20], Step [800/938], Reconstruction Loss: 4907.1929, KL Divergence: 1664.3395\n",
            "Epoch[18/20], Step [810/938], Reconstruction Loss: 4849.9595, KL Divergence: 1588.8035\n",
            "Epoch[18/20], Step [820/938], Reconstruction Loss: 4994.9189, KL Divergence: 1578.5421\n",
            "Epoch[18/20], Step [830/938], Reconstruction Loss: 5281.2705, KL Divergence: 1639.8872\n",
            "Epoch[18/20], Step [840/938], Reconstruction Loss: 5224.0283, KL Divergence: 1642.3318\n",
            "Epoch[18/20], Step [850/938], Reconstruction Loss: 4752.1113, KL Divergence: 1615.6797\n",
            "Epoch[18/20], Step [860/938], Reconstruction Loss: 5002.9766, KL Divergence: 1663.4629\n",
            "Epoch[18/20], Step [870/938], Reconstruction Loss: 5019.3799, KL Divergence: 1611.0443\n",
            "Epoch[18/20], Step [880/938], Reconstruction Loss: 4925.7402, KL Divergence: 1658.4636\n",
            "Epoch[18/20], Step [890/938], Reconstruction Loss: 5098.2822, KL Divergence: 1586.8505\n",
            "Epoch[18/20], Step [900/938], Reconstruction Loss: 4918.2769, KL Divergence: 1607.5608\n",
            "Epoch[18/20], Step [910/938], Reconstruction Loss: 5125.8804, KL Divergence: 1643.9397\n",
            "Epoch[18/20], Step [920/938], Reconstruction Loss: 4895.8066, KL Divergence: 1616.1660\n",
            "Epoch[18/20], Step [930/938], Reconstruction Loss: 5207.6504, KL Divergence: 1648.1960\n",
            "Epoch[19/20], Step [10/938], Reconstruction Loss: 4840.8091, KL Divergence: 1721.5378\n",
            "Epoch[19/20], Step [20/938], Reconstruction Loss: 5112.1626, KL Divergence: 1647.8976\n",
            "Epoch[19/20], Step [30/938], Reconstruction Loss: 5158.6548, KL Divergence: 1633.2920\n",
            "Epoch[19/20], Step [40/938], Reconstruction Loss: 4837.5288, KL Divergence: 1591.1188\n",
            "Epoch[19/20], Step [50/938], Reconstruction Loss: 5020.1768, KL Divergence: 1682.3700\n",
            "Epoch[19/20], Step [60/938], Reconstruction Loss: 4765.3403, KL Divergence: 1581.2123\n",
            "Epoch[19/20], Step [70/938], Reconstruction Loss: 4984.8843, KL Divergence: 1616.9686\n",
            "Epoch[19/20], Step [80/938], Reconstruction Loss: 4914.5332, KL Divergence: 1581.8354\n",
            "Epoch[19/20], Step [90/938], Reconstruction Loss: 4888.6191, KL Divergence: 1613.9182\n",
            "Epoch[19/20], Step [100/938], Reconstruction Loss: 5116.5503, KL Divergence: 1599.6725\n",
            "Epoch[19/20], Step [110/938], Reconstruction Loss: 4867.4277, KL Divergence: 1687.9093\n",
            "Epoch[19/20], Step [120/938], Reconstruction Loss: 4899.3438, KL Divergence: 1625.6658\n",
            "Epoch[19/20], Step [130/938], Reconstruction Loss: 5136.3384, KL Divergence: 1702.9797\n",
            "Epoch[19/20], Step [140/938], Reconstruction Loss: 5218.5645, KL Divergence: 1635.5316\n",
            "Epoch[19/20], Step [150/938], Reconstruction Loss: 4889.3354, KL Divergence: 1625.5099\n",
            "Epoch[19/20], Step [160/938], Reconstruction Loss: 5407.6035, KL Divergence: 1685.3698\n",
            "Epoch[19/20], Step [170/938], Reconstruction Loss: 5007.5020, KL Divergence: 1645.5839\n",
            "Epoch[19/20], Step [180/938], Reconstruction Loss: 5009.1079, KL Divergence: 1701.3312\n",
            "Epoch[19/20], Step [190/938], Reconstruction Loss: 5024.5996, KL Divergence: 1560.4878\n",
            "Epoch[19/20], Step [200/938], Reconstruction Loss: 4842.7798, KL Divergence: 1703.6003\n",
            "Epoch[19/20], Step [210/938], Reconstruction Loss: 4974.6577, KL Divergence: 1573.4999\n",
            "Epoch[19/20], Step [220/938], Reconstruction Loss: 5176.9092, KL Divergence: 1656.0626\n",
            "Epoch[19/20], Step [230/938], Reconstruction Loss: 4690.7231, KL Divergence: 1614.7375\n",
            "Epoch[19/20], Step [240/938], Reconstruction Loss: 4656.1265, KL Divergence: 1587.4468\n",
            "Epoch[19/20], Step [250/938], Reconstruction Loss: 5126.7090, KL Divergence: 1655.8340\n",
            "Epoch[19/20], Step [260/938], Reconstruction Loss: 5002.4658, KL Divergence: 1663.2245\n",
            "Epoch[19/20], Step [270/938], Reconstruction Loss: 5077.6455, KL Divergence: 1629.7584\n",
            "Epoch[19/20], Step [280/938], Reconstruction Loss: 4861.3579, KL Divergence: 1586.8888\n",
            "Epoch[19/20], Step [290/938], Reconstruction Loss: 5293.9487, KL Divergence: 1635.9203\n",
            "Epoch[19/20], Step [300/938], Reconstruction Loss: 5020.5693, KL Divergence: 1686.9056\n",
            "Epoch[19/20], Step [310/938], Reconstruction Loss: 5187.5645, KL Divergence: 1700.2850\n",
            "Epoch[19/20], Step [320/938], Reconstruction Loss: 5066.3169, KL Divergence: 1639.3186\n",
            "Epoch[19/20], Step [330/938], Reconstruction Loss: 4961.0972, KL Divergence: 1656.8195\n",
            "Epoch[19/20], Step [340/938], Reconstruction Loss: 5025.6309, KL Divergence: 1645.4923\n",
            "Epoch[19/20], Step [350/938], Reconstruction Loss: 5047.7607, KL Divergence: 1623.7009\n",
            "Epoch[19/20], Step [360/938], Reconstruction Loss: 5230.0205, KL Divergence: 1641.2301\n",
            "Epoch[19/20], Step [370/938], Reconstruction Loss: 5267.2173, KL Divergence: 1631.0088\n",
            "Epoch[19/20], Step [380/938], Reconstruction Loss: 4894.0127, KL Divergence: 1636.4915\n",
            "Epoch[19/20], Step [390/938], Reconstruction Loss: 4794.5850, KL Divergence: 1570.1522\n",
            "Epoch[19/20], Step [400/938], Reconstruction Loss: 5115.8701, KL Divergence: 1707.6777\n",
            "Epoch[19/20], Step [410/938], Reconstruction Loss: 4796.3145, KL Divergence: 1632.8839\n",
            "Epoch[19/20], Step [420/938], Reconstruction Loss: 4929.3584, KL Divergence: 1639.0620\n",
            "Epoch[19/20], Step [430/938], Reconstruction Loss: 5091.1489, KL Divergence: 1620.5366\n",
            "Epoch[19/20], Step [440/938], Reconstruction Loss: 4844.9829, KL Divergence: 1644.3940\n",
            "Epoch[19/20], Step [450/938], Reconstruction Loss: 5393.1875, KL Divergence: 1691.5364\n",
            "Epoch[19/20], Step [460/938], Reconstruction Loss: 5003.5483, KL Divergence: 1626.9702\n",
            "Epoch[19/20], Step [470/938], Reconstruction Loss: 4971.2764, KL Divergence: 1651.8112\n",
            "Epoch[19/20], Step [480/938], Reconstruction Loss: 4776.3608, KL Divergence: 1603.1219\n",
            "Epoch[19/20], Step [490/938], Reconstruction Loss: 4840.9482, KL Divergence: 1613.0065\n",
            "Epoch[19/20], Step [500/938], Reconstruction Loss: 5142.0396, KL Divergence: 1675.5017\n",
            "Epoch[19/20], Step [510/938], Reconstruction Loss: 4784.4360, KL Divergence: 1605.8885\n",
            "Epoch[19/20], Step [520/938], Reconstruction Loss: 4909.5820, KL Divergence: 1618.2216\n",
            "Epoch[19/20], Step [530/938], Reconstruction Loss: 5021.1426, KL Divergence: 1648.0516\n",
            "Epoch[19/20], Step [540/938], Reconstruction Loss: 5184.2314, KL Divergence: 1643.2268\n",
            "Epoch[19/20], Step [550/938], Reconstruction Loss: 4872.7778, KL Divergence: 1646.0758\n",
            "Epoch[19/20], Step [560/938], Reconstruction Loss: 4890.6572, KL Divergence: 1625.7621\n",
            "Epoch[19/20], Step [570/938], Reconstruction Loss: 4893.9712, KL Divergence: 1637.5300\n",
            "Epoch[19/20], Step [580/938], Reconstruction Loss: 5229.6216, KL Divergence: 1625.5015\n",
            "Epoch[19/20], Step [590/938], Reconstruction Loss: 4862.8496, KL Divergence: 1596.3270\n",
            "Epoch[19/20], Step [600/938], Reconstruction Loss: 4797.8711, KL Divergence: 1582.2887\n",
            "Epoch[19/20], Step [610/938], Reconstruction Loss: 4757.7212, KL Divergence: 1630.0013\n",
            "Epoch[19/20], Step [620/938], Reconstruction Loss: 5180.6162, KL Divergence: 1639.0146\n",
            "Epoch[19/20], Step [630/938], Reconstruction Loss: 5039.5039, KL Divergence: 1625.6187\n",
            "Epoch[19/20], Step [640/938], Reconstruction Loss: 4823.3887, KL Divergence: 1570.5558\n",
            "Epoch[19/20], Step [650/938], Reconstruction Loss: 5132.3091, KL Divergence: 1641.5490\n",
            "Epoch[19/20], Step [660/938], Reconstruction Loss: 4947.8081, KL Divergence: 1622.7664\n",
            "Epoch[19/20], Step [670/938], Reconstruction Loss: 4945.6455, KL Divergence: 1656.2423\n",
            "Epoch[19/20], Step [680/938], Reconstruction Loss: 4973.3311, KL Divergence: 1632.4039\n",
            "Epoch[19/20], Step [690/938], Reconstruction Loss: 5258.0396, KL Divergence: 1720.0298\n",
            "Epoch[19/20], Step [700/938], Reconstruction Loss: 4933.9463, KL Divergence: 1655.5881\n",
            "Epoch[19/20], Step [710/938], Reconstruction Loss: 4905.1182, KL Divergence: 1646.3230\n",
            "Epoch[19/20], Step [720/938], Reconstruction Loss: 5190.1455, KL Divergence: 1701.1735\n",
            "Epoch[19/20], Step [730/938], Reconstruction Loss: 5222.7793, KL Divergence: 1679.8201\n",
            "Epoch[19/20], Step [740/938], Reconstruction Loss: 4997.9204, KL Divergence: 1698.2854\n",
            "Epoch[19/20], Step [750/938], Reconstruction Loss: 5057.7881, KL Divergence: 1647.4672\n",
            "Epoch[19/20], Step [760/938], Reconstruction Loss: 5128.7837, KL Divergence: 1680.8698\n",
            "Epoch[19/20], Step [770/938], Reconstruction Loss: 5212.0776, KL Divergence: 1658.4022\n",
            "Epoch[19/20], Step [780/938], Reconstruction Loss: 4921.7246, KL Divergence: 1575.6709\n",
            "Epoch[19/20], Step [790/938], Reconstruction Loss: 4901.1948, KL Divergence: 1694.5056\n",
            "Epoch[19/20], Step [800/938], Reconstruction Loss: 5216.1533, KL Divergence: 1599.2643\n",
            "Epoch[19/20], Step [810/938], Reconstruction Loss: 4845.1553, KL Divergence: 1672.4504\n",
            "Epoch[19/20], Step [820/938], Reconstruction Loss: 5219.6743, KL Divergence: 1636.8738\n",
            "Epoch[19/20], Step [830/938], Reconstruction Loss: 4722.9541, KL Divergence: 1624.5127\n",
            "Epoch[19/20], Step [840/938], Reconstruction Loss: 4758.7793, KL Divergence: 1619.4369\n",
            "Epoch[19/20], Step [850/938], Reconstruction Loss: 5216.2471, KL Divergence: 1749.0789\n",
            "Epoch[19/20], Step [860/938], Reconstruction Loss: 5275.3149, KL Divergence: 1648.5417\n",
            "Epoch[19/20], Step [870/938], Reconstruction Loss: 4609.3296, KL Divergence: 1613.8444\n",
            "Epoch[19/20], Step [880/938], Reconstruction Loss: 5127.1211, KL Divergence: 1623.8992\n",
            "Epoch[19/20], Step [890/938], Reconstruction Loss: 4704.2451, KL Divergence: 1524.5834\n",
            "Epoch[19/20], Step [900/938], Reconstruction Loss: 4943.6895, KL Divergence: 1623.4478\n",
            "Epoch[19/20], Step [910/938], Reconstruction Loss: 5056.4873, KL Divergence: 1672.6858\n",
            "Epoch[19/20], Step [920/938], Reconstruction Loss: 5268.8433, KL Divergence: 1590.8522\n",
            "Epoch[19/20], Step [930/938], Reconstruction Loss: 4979.8652, KL Divergence: 1692.2540\n",
            "Epoch[20/20], Step [10/938], Reconstruction Loss: 4936.9248, KL Divergence: 1649.0802\n",
            "Epoch[20/20], Step [20/938], Reconstruction Loss: 5056.6328, KL Divergence: 1642.5239\n",
            "Epoch[20/20], Step [30/938], Reconstruction Loss: 5175.7803, KL Divergence: 1694.2518\n",
            "Epoch[20/20], Step [40/938], Reconstruction Loss: 5021.5142, KL Divergence: 1652.1204\n",
            "Epoch[20/20], Step [50/938], Reconstruction Loss: 4969.9854, KL Divergence: 1740.1499\n",
            "Epoch[20/20], Step [60/938], Reconstruction Loss: 5385.7168, KL Divergence: 1632.6929\n",
            "Epoch[20/20], Step [70/938], Reconstruction Loss: 4969.2349, KL Divergence: 1652.5867\n",
            "Epoch[20/20], Step [80/938], Reconstruction Loss: 4935.4282, KL Divergence: 1626.6859\n",
            "Epoch[20/20], Step [90/938], Reconstruction Loss: 4855.9146, KL Divergence: 1622.4976\n",
            "Epoch[20/20], Step [100/938], Reconstruction Loss: 4945.6572, KL Divergence: 1673.3652\n",
            "Epoch[20/20], Step [110/938], Reconstruction Loss: 5093.1592, KL Divergence: 1602.7754\n",
            "Epoch[20/20], Step [120/938], Reconstruction Loss: 5017.8438, KL Divergence: 1651.0067\n",
            "Epoch[20/20], Step [130/938], Reconstruction Loss: 4887.2988, KL Divergence: 1621.6730\n",
            "Epoch[20/20], Step [140/938], Reconstruction Loss: 4890.2109, KL Divergence: 1631.6152\n",
            "Epoch[20/20], Step [150/938], Reconstruction Loss: 4841.7002, KL Divergence: 1609.7856\n",
            "Epoch[20/20], Step [160/938], Reconstruction Loss: 5057.3784, KL Divergence: 1597.4137\n",
            "Epoch[20/20], Step [170/938], Reconstruction Loss: 5146.8198, KL Divergence: 1712.6635\n",
            "Epoch[20/20], Step [180/938], Reconstruction Loss: 5123.2993, KL Divergence: 1682.8518\n",
            "Epoch[20/20], Step [190/938], Reconstruction Loss: 5267.3140, KL Divergence: 1644.4623\n",
            "Epoch[20/20], Step [200/938], Reconstruction Loss: 4985.6411, KL Divergence: 1653.9542\n",
            "Epoch[20/20], Step [210/938], Reconstruction Loss: 4983.6138, KL Divergence: 1659.1544\n",
            "Epoch[20/20], Step [220/938], Reconstruction Loss: 4777.0249, KL Divergence: 1614.0803\n",
            "Epoch[20/20], Step [230/938], Reconstruction Loss: 4958.2065, KL Divergence: 1586.9703\n",
            "Epoch[20/20], Step [240/938], Reconstruction Loss: 4834.7026, KL Divergence: 1558.3875\n",
            "Epoch[20/20], Step [250/938], Reconstruction Loss: 4806.6455, KL Divergence: 1618.2236\n",
            "Epoch[20/20], Step [260/938], Reconstruction Loss: 5264.7251, KL Divergence: 1687.7288\n",
            "Epoch[20/20], Step [270/938], Reconstruction Loss: 5017.2104, KL Divergence: 1589.4419\n",
            "Epoch[20/20], Step [280/938], Reconstruction Loss: 5116.6299, KL Divergence: 1694.7284\n",
            "Epoch[20/20], Step [290/938], Reconstruction Loss: 4660.9834, KL Divergence: 1595.8308\n",
            "Epoch[20/20], Step [300/938], Reconstruction Loss: 5219.6338, KL Divergence: 1696.1051\n",
            "Epoch[20/20], Step [310/938], Reconstruction Loss: 4887.3491, KL Divergence: 1611.6904\n",
            "Epoch[20/20], Step [320/938], Reconstruction Loss: 5095.8022, KL Divergence: 1595.3999\n",
            "Epoch[20/20], Step [330/938], Reconstruction Loss: 5118.8335, KL Divergence: 1692.4316\n",
            "Epoch[20/20], Step [340/938], Reconstruction Loss: 5017.7280, KL Divergence: 1611.7161\n",
            "Epoch[20/20], Step [350/938], Reconstruction Loss: 5071.9600, KL Divergence: 1654.7473\n",
            "Epoch[20/20], Step [360/938], Reconstruction Loss: 4632.4209, KL Divergence: 1564.2960\n",
            "Epoch[20/20], Step [370/938], Reconstruction Loss: 5050.4766, KL Divergence: 1725.8528\n",
            "Epoch[20/20], Step [380/938], Reconstruction Loss: 4928.4067, KL Divergence: 1596.0706\n",
            "Epoch[20/20], Step [390/938], Reconstruction Loss: 4863.2690, KL Divergence: 1645.6409\n",
            "Epoch[20/20], Step [400/938], Reconstruction Loss: 4822.5200, KL Divergence: 1554.9171\n",
            "Epoch[20/20], Step [410/938], Reconstruction Loss: 5034.8877, KL Divergence: 1644.4325\n",
            "Epoch[20/20], Step [420/938], Reconstruction Loss: 4855.0923, KL Divergence: 1551.6068\n",
            "Epoch[20/20], Step [430/938], Reconstruction Loss: 5017.4619, KL Divergence: 1721.6560\n",
            "Epoch[20/20], Step [440/938], Reconstruction Loss: 5170.7603, KL Divergence: 1614.5120\n",
            "Epoch[20/20], Step [450/938], Reconstruction Loss: 4865.3809, KL Divergence: 1665.8898\n",
            "Epoch[20/20], Step [460/938], Reconstruction Loss: 5178.7910, KL Divergence: 1586.5237\n",
            "Epoch[20/20], Step [470/938], Reconstruction Loss: 4981.6621, KL Divergence: 1619.0546\n",
            "Epoch[20/20], Step [480/938], Reconstruction Loss: 4787.2666, KL Divergence: 1592.4072\n",
            "Epoch[20/20], Step [490/938], Reconstruction Loss: 5118.4365, KL Divergence: 1598.9695\n",
            "Epoch[20/20], Step [500/938], Reconstruction Loss: 4825.9697, KL Divergence: 1605.1182\n",
            "Epoch[20/20], Step [510/938], Reconstruction Loss: 5057.1133, KL Divergence: 1660.5209\n",
            "Epoch[20/20], Step [520/938], Reconstruction Loss: 5107.5264, KL Divergence: 1615.6367\n",
            "Epoch[20/20], Step [530/938], Reconstruction Loss: 4845.2822, KL Divergence: 1657.5073\n",
            "Epoch[20/20], Step [540/938], Reconstruction Loss: 5428.9536, KL Divergence: 1668.6423\n",
            "Epoch[20/20], Step [550/938], Reconstruction Loss: 4933.7065, KL Divergence: 1660.6647\n",
            "Epoch[20/20], Step [560/938], Reconstruction Loss: 5162.3457, KL Divergence: 1654.5454\n",
            "Epoch[20/20], Step [570/938], Reconstruction Loss: 5283.6460, KL Divergence: 1672.3115\n",
            "Epoch[20/20], Step [580/938], Reconstruction Loss: 5198.7632, KL Divergence: 1684.3845\n",
            "Epoch[20/20], Step [590/938], Reconstruction Loss: 5005.0532, KL Divergence: 1578.1764\n",
            "Epoch[20/20], Step [600/938], Reconstruction Loss: 5222.2725, KL Divergence: 1685.5599\n",
            "Epoch[20/20], Step [610/938], Reconstruction Loss: 5082.7227, KL Divergence: 1650.6704\n",
            "Epoch[20/20], Step [620/938], Reconstruction Loss: 5176.2354, KL Divergence: 1732.3354\n",
            "Epoch[20/20], Step [630/938], Reconstruction Loss: 5227.7017, KL Divergence: 1679.2290\n",
            "Epoch[20/20], Step [640/938], Reconstruction Loss: 5288.4209, KL Divergence: 1685.8823\n",
            "Epoch[20/20], Step [650/938], Reconstruction Loss: 4801.0288, KL Divergence: 1632.9592\n",
            "Epoch[20/20], Step [660/938], Reconstruction Loss: 4993.5630, KL Divergence: 1679.2584\n",
            "Epoch[20/20], Step [670/938], Reconstruction Loss: 4730.6841, KL Divergence: 1582.3866\n",
            "Epoch[20/20], Step [680/938], Reconstruction Loss: 4690.5078, KL Divergence: 1543.6019\n",
            "Epoch[20/20], Step [690/938], Reconstruction Loss: 4782.8950, KL Divergence: 1601.5752\n",
            "Epoch[20/20], Step [700/938], Reconstruction Loss: 4646.6904, KL Divergence: 1601.1243\n",
            "Epoch[20/20], Step [710/938], Reconstruction Loss: 4622.2388, KL Divergence: 1581.8450\n",
            "Epoch[20/20], Step [720/938], Reconstruction Loss: 4921.2080, KL Divergence: 1569.7080\n",
            "Epoch[20/20], Step [730/938], Reconstruction Loss: 4805.3735, KL Divergence: 1616.3337\n",
            "Epoch[20/20], Step [740/938], Reconstruction Loss: 5026.9956, KL Divergence: 1626.9290\n",
            "Epoch[20/20], Step [750/938], Reconstruction Loss: 4839.5439, KL Divergence: 1611.2692\n",
            "Epoch[20/20], Step [760/938], Reconstruction Loss: 5227.3105, KL Divergence: 1678.2963\n",
            "Epoch[20/20], Step [770/938], Reconstruction Loss: 4983.4346, KL Divergence: 1670.6703\n",
            "Epoch[20/20], Step [780/938], Reconstruction Loss: 5120.7354, KL Divergence: 1711.0939\n",
            "Epoch[20/20], Step [790/938], Reconstruction Loss: 4890.6040, KL Divergence: 1630.6804\n",
            "Epoch[20/20], Step [800/938], Reconstruction Loss: 4789.0273, KL Divergence: 1590.8607\n",
            "Epoch[20/20], Step [810/938], Reconstruction Loss: 4680.0425, KL Divergence: 1585.9618\n",
            "Epoch[20/20], Step [820/938], Reconstruction Loss: 5007.0298, KL Divergence: 1629.6777\n",
            "Epoch[20/20], Step [830/938], Reconstruction Loss: 4950.9009, KL Divergence: 1625.5316\n",
            "Epoch[20/20], Step [840/938], Reconstruction Loss: 5092.6899, KL Divergence: 1609.2632\n",
            "Epoch[20/20], Step [850/938], Reconstruction Loss: 4896.0596, KL Divergence: 1604.6104\n",
            "Epoch[20/20], Step [860/938], Reconstruction Loss: 5263.3154, KL Divergence: 1738.2719\n",
            "Epoch[20/20], Step [870/938], Reconstruction Loss: 5222.7202, KL Divergence: 1643.3529\n",
            "Epoch[20/20], Step [880/938], Reconstruction Loss: 4782.6294, KL Divergence: 1625.6888\n",
            "Epoch[20/20], Step [890/938], Reconstruction Loss: 5395.5654, KL Divergence: 1619.4927\n",
            "Epoch[20/20], Step [900/938], Reconstruction Loss: 4906.6133, KL Divergence: 1683.1537\n",
            "Epoch[20/20], Step [910/938], Reconstruction Loss: 4814.1836, KL Divergence: 1600.1749\n",
            "Epoch[20/20], Step [920/938], Reconstruction Loss: 5221.3594, KL Divergence: 1696.3458\n",
            "Epoch[20/20], Step [930/938], Reconstruction Loss: 5023.0547, KL Divergence: 1616.1958\n",
            "Training Takes 512.7792067527771 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8TQYINpuTy2f",
        "colab_type": "code",
        "outputId": "93e5418b-95ac-4c8c-fcf0-416833f552e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "# Generate an image from random noise\n",
        "eval_model = model.eval()     \n",
        "with torch.no_grad():\n",
        "    # Random noise\n",
        "    z = torch.randn(1, z_dim).to(device)\n",
        "    # Decode\n",
        "    out = eval_model.decode(z).view(-1, 1, 28, 28)\n",
        "\n",
        "sampled_img = torch.squeeze(torch.squeeze(out,dim=0),dim=0)\n",
        "plt.imshow(sampled_img)\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SL5Bhn-hUMxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}